diff --git a/.gitignore b/.gitignore
index 1807df7..0f36d8b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -134,8 +134,4 @@ dmypy.json

 /.idea
 /.vscode
-outputs/
-
-/data
-
-mujoco-2.1.3-cp37-cp37m-linux_x86_64.whl
+outputs/
\ No newline at end of file
diff --git a/.pre-commit-config.yaml b/.pre-commit-config.yaml
deleted file mode 100644
index 0cf26c6..0000000
--- a/.pre-commit-config.yaml
+++ /dev/null
@@ -1,30 +0,0 @@
-repos:
-  - repo: https://github.com/pre-commit/pre-commit-hooks
-    rev: v3.4.0
-    hooks:
-      - id: check-docstring-first
-      - id: check-merge-conflict
-      - id: trailing-whitespace
-      - id: end-of-file-fixer
-      - id: check-yaml
-      - id: check-ast
-      - id: check-added-large-files
-
-  - repo: local
-    hooks:
-      - id: pylint
-        name: pylint
-        entry: python -m pylint src
-        language: system
-        types: [python]
-        always_run: true
-
-  - repo: local
-    hooks:
-      - id: tests
-        name: tests
-        entry: python -m unittest
-        language: system
-        types: [python]
-        pass_filenames: false
-        always_run: true
diff --git a/.pylintrc b/.pylintrc
deleted file mode 100644
index d59ace4..0000000
--- a/.pylintrc
+++ /dev/null
@@ -1,595 +0,0 @@
-[MASTER]
-
-# A comma-separated list of package or module names from where C extensions may
-# be loaded. Extensions are loading into the active Python interpreter and may
-# run arbitrary code.
-extension-pkg-allow-list=
-
-# A comma-separated list of package or module names from where C extensions may
-# be loaded. Extensions are loading into the active Python interpreter and may
-# run arbitrary code. (This is an alternative name to extension-pkg-allow-list
-# for backward compatibility.)
-extension-pkg-whitelist=
-
-# Return non-zero exit code if any of these messages/categories are detected,
-# even if score is above --fail-under value. Syntax same as enable. Messages
-# specified are enabled, while categories only check already-enabled messages.
-fail-on=
-
-# Specify a score threshold to be exceeded before program exits with error.
-fail-under=10.0
-
-# Files or directories to be skipped. They should be base names, not paths.
-ignore=CVS
-
-# Add files or directories matching the regex patterns to the ignore-list. The
-# regex matches against paths and can be in Posix or Windows format.
-ignore-paths=
-
-# Files or directories matching the regex patterns are skipped. The regex
-# matches against base names, not paths. The default value ignores emacs file
-# locks
-ignore-patterns=^\.#
-
-# Python code to execute, usually for sys.path manipulation such as
-# pygtk.require().
-#init-hook=
-
-# Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the
-# number of processors available to use.
-jobs=1
-
-# Control the amount of potential inferred values when inferring a single
-# object. This can help the performance when dealing with large functions or
-# complex, nested conditions.
-limit-inference-results=100
-
-# List of plugins (as comma separated values of python module names) to load,
-# usually to register additional checkers.
-load-plugins=
-
-# Pickle collected data for later comparisons.
-persistent=yes
-
-# Minimum Python version to use for version dependent checks. Will default to
-# the version used to run pylint.
-py-version=3.7
-
-# Discover python modules and packages in the file system subtree.
-recursive=no
-
-# When enabled, pylint would attempt to guess common misconfiguration and emit
-# user-friendly hints instead of false-positive error messages.
-suggestion-mode=yes
-
-# Allow loading of arbitrary C extensions. Extensions are imported into the
-# active Python interpreter and may run arbitrary code.
-unsafe-load-any-extension=no
-
-
-[MESSAGES CONTROL]
-
-# Only show warnings with the listed confidence levels. Leave empty to show
-# all. Valid levels: HIGH, CONTROL_FLOW, INFERENCE, INFERENCE_FAILURE,
-# UNDEFINED.
-confidence=
-
-# Disable the message, report, category or checker with the given id(s). You
-# can either give multiple identifiers separated by comma (,) or put this
-# option multiple times (only on the command line, not in the configuration
-# file where it should appear only once). You can also use "--disable=all" to
-# disable everything first and then re-enable specific checks. For example, if
-# you want to run only the similarities checker, you can use "--disable=all
-# --enable=similarities". If you want to run only the classes checker, but have
-# no Warning level messages displayed, use "--disable=all --enable=classes
-# --disable=W".
-disable=raw-checker-failed,
-        bad-inline-option,
-        locally-disabled,
-        file-ignored,
-        suppressed-message,
-        useless-suppression,
-        deprecated-pragma,
-        use-symbolic-message-instead,
-        abstract-method,
-        missing-module-docstring,
-        too-many-arguments,
-        too-many-locals,
-        invalid-name,
-        too-many-instance-attributes,
-        fixme,
-        too-many-branches,
-        too-many-statements,
-        duplicate-code
-
-# Enable the message, report, category or checker with the given id(s). You can
-# either give multiple identifier separated by comma (,) or put this option
-# multiple time (only on the command line, not in the configuration file where
-# it should appear only once). See also the "--disable" option for examples.
-enable=c-extension-no-member
-
-
-[REPORTS]
-
-# Python expression which should return a score less than or equal to 10. You
-# have access to the variables 'fatal', 'error', 'warning', 'refactor',
-# 'convention', and 'info' which contain the number of messages in each
-# category, as well as 'statement' which is the total number of statements
-# analyzed. This score is used by the global evaluation report (RP0004).
-evaluation=max(0, 0 if fatal else 10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10))
-
-# Template used to display messages. This is a python new-style format string
-# used to format the message information. See doc for all details.
-#msg-template=
-
-# Set the output format. Available formats are text, parseable, colorized, json
-# and msvs (visual studio). You can also give a reporter class, e.g.
-# mypackage.mymodule.MyReporterClass.
-output-format=text
-
-# Tells whether to display a full report or only the messages.
-reports=no
-
-# Activate the evaluation score.
-score=yes
-
-
-[REFACTORING]
-
-# Maximum number of nested blocks for function / method body
-max-nested-blocks=5
-
-# Complete name of functions that never returns. When checking for
-# inconsistent-return-statements if a never returning function is called then
-# it will be considered as an explicit return statement and no message will be
-# printed.
-never-returning-functions=sys.exit,argparse.parse_error
-
-
-[LOGGING]
-
-# The type of string formatting that logging methods do. `old` means using %
-# formatting, `new` is for `{}` formatting.
-logging-format-style=old
-
-# Logging modules to check that the string format arguments are in logging
-# function parameter format.
-logging-modules=logging
-
-
-[SPELLING]
-
-# Limits count of emitted suggestions for spelling mistakes.
-max-spelling-suggestions=4
-
-# Spelling dictionary name. Available dictionaries: none. To make it work,
-# install the 'python-enchant' package.
-spelling-dict=
-
-# List of comma separated words that should be considered directives if they
-# appear and the beginning of a comment and should not be checked.
-spelling-ignore-comment-directives=fmt: on,fmt: off,noqa:,noqa,nosec,isort:skip,mypy:
-
-# List of comma separated words that should not be checked.
-spelling-ignore-words=
-
-# A path to a file that contains the private dictionary; one word per line.
-spelling-private-dict-file=
-
-# Tells whether to store unknown words to the private dictionary (see the
-# --spelling-private-dict-file option) instead of raising a message.
-spelling-store-unknown-words=no
-
-
-[MISCELLANEOUS]
-
-# List of note tags to take in consideration, separated by a comma.
-notes=FIXME,
-      XXX,
-      TODO
-
-# Regular expression of note tags to take in consideration.
-#notes-rgx=
-
-
-[TYPECHECK]
-
-# List of decorators that produce context managers, such as
-# contextlib.contextmanager. Add to this list to register other decorators that
-# produce valid context managers.
-contextmanager-decorators=contextlib.contextmanager
-
-# List of members which are set dynamically and missed by pylint inference
-# system, and so shouldn't trigger E1101 when accessed. Python regular
-# expressions are accepted.
-generated-members=torch.*, numpy.*, cv2.*
-
-# Tells whether missing members accessed in mixin class should be ignored. A
-# class is considered mixin if its name matches the mixin-class-rgx option.
-ignore-mixin-members=yes
-
-# Tells whether to warn about missing members when the owner of the attribute
-# is inferred to be None.
-ignore-none=yes
-
-# This flag controls whether pylint should warn about no-member and similar
-# checks whenever an opaque object is returned when inferring. The inference
-# can return multiple potential results while evaluating a Python object, but
-# some branches might not be evaluated, which results in partial inference. In
-# that case, it might be useful to still emit no-member and other checks for
-# the rest of the inferred objects.
-ignore-on-opaque-inference=yes
-
-# List of class names for which member attributes should not be checked (useful
-# for classes with dynamically set attributes). This supports the use of
-# qualified names.
-ignored-classes=optparse.Values,thread._local,_thread._local
-
-# List of module names for which member attributes should not be checked
-# (useful for modules/projects where namespaces are manipulated during runtime
-# and thus existing member attributes cannot be deduced by static analysis). It
-# supports qualified module names, as well as Unix pattern matching.
-ignored-modules=
-
-# Show a hint with possible names when a member name was not found. The aspect
-# of finding the hint is based on edit distance.
-missing-member-hint=yes
-
-# The minimum edit distance a name should have in order to be considered a
-# similar match for a missing member name.
-missing-member-hint-distance=1
-
-# The total number of similar names that should be taken in consideration when
-# showing a hint for a missing member.
-missing-member-max-choices=1
-
-# Regex pattern to define which classes are considered mixins ignore-mixin-
-# members is set to 'yes'
-mixin-class-rgx=.*[Mm]ixin
-
-# List of decorators that change the signature of a decorated function.
-signature-mutators=
-
-
-[VARIABLES]
-
-# List of additional names supposed to be defined in builtins. Remember that
-# you should avoid defining new builtins when possible.
-additional-builtins=
-
-# Tells whether unused global variables should be treated as a violation.
-allow-global-unused-variables=yes
-
-# List of names allowed to shadow builtins
-allowed-redefined-builtins=
-
-# List of strings which can identify a callback function by name. A callback
-# name must start or end with one of those strings.
-callbacks=cb_,
-          _cb
-
-# A regular expression matching the name of dummy variables (i.e. expected to
-# not be used).
-dummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_
-
-# Argument names that match this expression will be ignored. Default to name
-# with leading underscore.
-ignored-argument-names=_.*|^ignored_|^unused_
-
-# Tells whether we should check for unused import in __init__ files.
-init-import=no
-
-# List of qualified module names which can have objects that can redefine
-# builtins.
-redefining-builtins-modules=six.moves,past.builtins,future.builtins,builtins,io
-
-
-[FORMAT]
-
-# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.
-expected-line-ending-format=
-
-# Regexp for a line that is allowed to be longer than the limit.
-ignore-long-lines=^\s*(# )?<?https?://\S+>?$
-
-# Number of spaces of indent required inside a hanging or continued line.
-indent-after-paren=4
-
-# String used as indentation unit. This is usually "    " (4 spaces) or "\t" (1
-# tab).
-indent-string='    '
-
-# Maximum number of characters on a single line.
-max-line-length=100
-
-# Maximum number of lines in a module.
-max-module-lines=1000
-
-# Allow the body of a class to be on the same line as the declaration if body
-# contains single statement.
-single-line-class-stmt=no
-
-# Allow the body of an if to be on the same line as the test if there is no
-# else.
-single-line-if-stmt=no
-
-
-[SIMILARITIES]
-
-# Comments are removed from the similarity computation
-ignore-comments=yes
-
-# Docstrings are removed from the similarity computation
-ignore-docstrings=yes
-
-# Imports are removed from the similarity computation
-ignore-imports=no
-
-# Signatures are removed from the similarity computation
-ignore-signatures=no
-
-# Minimum lines number of a similarity.
-min-similarity-lines=4
-
-
-[STRING]
-
-# This flag controls whether inconsistent-quotes generates a warning when the
-# character used as a quote delimiter is used inconsistently within a module.
-check-quote-consistency=no
-
-# This flag controls whether the implicit-str-concat should generate a warning
-# on implicit string concatenation in sequences defined over several lines.
-check-str-concat-over-line-jumps=no
-
-
-[BASIC]
-
-# Naming style matching correct argument names.
-argument-naming-style=snake_case
-
-# Regular expression matching correct argument names. Overrides argument-
-# naming-style. If left empty, argument names will be checked with the set
-# naming style.
-#argument-rgx=
-
-# Naming style matching correct attribute names.
-attr-naming-style=snake_case
-
-# Regular expression matching correct attribute names. Overrides attr-naming-
-# style. If left empty, attribute names will be checked with the set naming
-# style.
-#attr-rgx=
-
-# Bad variable names which should always be refused, separated by a comma.
-bad-names=foo,
-          bar,
-          baz,
-          toto,
-          tutu,
-          tata
-
-# Bad variable names regexes, separated by a comma. If names match any regex,
-# they will always be refused
-bad-names-rgxs=
-
-# Naming style matching correct class attribute names.
-class-attribute-naming-style=any
-
-# Regular expression matching correct class attribute names. Overrides class-
-# attribute-naming-style. If left empty, class attribute names will be checked
-# with the set naming style.
-#class-attribute-rgx=
-
-# Naming style matching correct class constant names.
-class-const-naming-style=UPPER_CASE
-
-# Regular expression matching correct class constant names. Overrides class-
-# const-naming-style. If left empty, class constant names will be checked with
-# the set naming style.
-#class-const-rgx=
-
-# Naming style matching correct class names.
-class-naming-style=PascalCase
-
-# Regular expression matching correct class names. Overrides class-naming-
-# style. If left empty, class names will be checked with the set naming style.
-#class-rgx=
-
-# Naming style matching correct constant names.
-const-naming-style=UPPER_CASE
-
-# Regular expression matching correct constant names. Overrides const-naming-
-# style. If left empty, constant names will be checked with the set naming
-# style.
-#const-rgx=
-
-# Minimum line length for functions/classes that require docstrings, shorter
-# ones are exempt.
-docstring-min-length=-1
-
-# Naming style matching correct function names.
-function-naming-style=snake_case
-
-# Regular expression matching correct function names. Overrides function-
-# naming-style. If left empty, function names will be checked with the set
-# naming style.
-#function-rgx=
-
-# Good variable names which should always be accepted, separated by a comma.
-good-names=i,
-           j,
-           k,
-           ex,
-           Run,
-           _
-
-# Good variable names regexes, separated by a comma. If names match any regex,
-# they will always be accepted
-good-names-rgxs=
-
-# Include a hint for the correct naming format with invalid-name.
-include-naming-hint=no
-
-# Naming style matching correct inline iteration names.
-inlinevar-naming-style=any
-
-# Regular expression matching correct inline iteration names. Overrides
-# inlinevar-naming-style. If left empty, inline iteration names will be checked
-# with the set naming style.
-#inlinevar-rgx=
-
-# Naming style matching correct method names.
-method-naming-style=snake_case
-
-# Regular expression matching correct method names. Overrides method-naming-
-# style. If left empty, method names will be checked with the set naming style.
-#method-rgx=
-
-# Naming style matching correct module names.
-module-naming-style=snake_case
-
-# Regular expression matching correct module names. Overrides module-naming-
-# style. If left empty, module names will be checked with the set naming style.
-#module-rgx=
-
-# Colon-delimited sets of names that determine each other's naming style when
-# the name regexes allow several styles.
-name-group=
-
-# Regular expression which should only match function or class names that do
-# not require a docstring.
-no-docstring-rgx=^_
-
-# List of decorators that produce properties, such as abc.abstractproperty. Add
-# to this list to register other decorators that produce valid properties.
-# These decorators are taken in consideration only for invalid-name.
-property-classes=abc.abstractproperty
-
-# Regular expression matching correct type variable names. If left empty, type
-# variable names will be checked with the set naming style.
-#typevar-rgx=
-
-# Naming style matching correct variable names.
-variable-naming-style=snake_case
-
-# Regular expression matching correct variable names. Overrides variable-
-# naming-style. If left empty, variable names will be checked with the set
-# naming style.
-#variable-rgx=
-
-
-[CLASSES]
-
-# Warn about protected attribute access inside special methods
-check-protected-access-in-special-methods=no
-
-# List of method names used to declare (i.e. assign) instance attributes.
-defining-attr-methods=__init__,
-                      __new__,
-                      setUp,
-                      __post_init__
-
-# List of member names, which should be excluded from the protected access
-# warning.
-exclude-protected=_asdict,
-                  _fields,
-                  _replace,
-                  _source,
-                  _make
-
-# List of valid names for the first argument in a class method.
-valid-classmethod-first-arg=cls
-
-# List of valid names for the first argument in a metaclass class method.
-valid-metaclass-classmethod-first-arg=cls
-
-
-[IMPORTS]
-
-# List of modules that can be imported at any level, not just the top level
-# one.
-allow-any-import-level=
-
-# Allow wildcard imports from modules that define __all__.
-allow-wildcard-with-all=no
-
-# Analyse import fallback blocks. This can be used to support both Python 2 and
-# 3 compatible code, which means that the block might have code that exists
-# only in one or another interpreter, leading to false positives when analysed.
-analyse-fallback-blocks=no
-
-# Deprecated modules which should not be used, separated by a comma.
-deprecated-modules=
-
-# Output a graph (.gv or any supported image format) of external dependencies
-# to the given file (report RP0402 must not be disabled).
-ext-import-graph=
-
-# Output a graph (.gv or any supported image format) of all (i.e. internal and
-# external) dependencies to the given file (report RP0402 must not be
-# disabled).
-import-graph=
-
-# Output a graph (.gv or any supported image format) of internal dependencies
-# to the given file (report RP0402 must not be disabled).
-int-import-graph=
-
-# Force import order to recognize a module as part of the standard
-# compatibility libraries.
-known-standard-library=
-
-# Force import order to recognize a module as part of a third party library.
-known-third-party=enchant
-
-# Couples of modules and preferred modules, separated by a comma.
-preferred-modules=
-
-
-[DESIGN]
-
-# List of regular expressions of class ancestor names to ignore when counting
-# public methods (see R0903)
-exclude-too-few-public-methods=
-
-# List of qualified class names to ignore when counting class parents (see
-# R0901)
-ignored-parents=
-
-# Maximum number of arguments for function / method.
-max-args=5
-
-# Maximum number of attributes for a class (see R0902).
-max-attributes=7
-
-# Maximum number of boolean expressions in an if statement (see R0916).
-max-bool-expr=5
-
-# Maximum number of branch for function / method body.
-max-branches=12
-
-# Maximum number of locals for function / method body.
-max-locals=15
-
-# Maximum number of parents for a class (see R0901).
-max-parents=7
-
-# Maximum number of public methods for a class (see R0904).
-max-public-methods=20
-
-# Maximum number of return / yield for function / method body.
-max-returns=6
-
-# Maximum number of statements in function / method body.
-max-statements=50
-
-# Minimum number of public methods for a class (see R0903).
-min-public-methods=2
-
-
-[EXCEPTIONS]
-
-# Exceptions that will emit a warning when being caught. Defaults to
-# "BaseException, Exception".
-overgeneral-exceptions=BaseException,
-                       Exception
diff --git a/README.md b/README.md
index 2459210..5c35b9a 100644
--- a/README.md
+++ b/README.md
@@ -1,105 +1,4 @@
 ### References
-This work is based out of an implementation of Planet from: https://github.com/Kaixhin/PlaNet

+Based on initial code from:

-### Running Some of the Stuff
-#### Install Pre-commit configs
-`python -m pre_commit install`
-
-#### Tests
-`python -m unittest`
-
-#### Code Quality
-`python -m pylint src`
-
-#### How to run Models
-Once every dependencies are installed (following the steps of the next section), you can run the following command to train the a model:
-
-```
-python src/main.py disable_cuda=True \
-                    algorithm="dreamer" \
-                    env="Pendulum-v0" \
-                    action_repeat=2 \
-                    episodes=100 \
-                    collect_interval=50 \
-                    hidden_size=32 \
-                    belief_size=32 \
-                    test_interval=10 \
-                    log_video_freq=10
-```
-
-Notes:
-Use `algorithm="dreamer"` to run the Dreamer algorithm.
-Use `algorithm="planet"` to run the Planet algorithm.
-Use `algorithm="dreamerV2"` to run the DreamerV2 algorithm (not complete yet).
-
-## Data experiment links
-Our event files from our experiments were too large, and therefore could not be uploaded to gradescope with our code. Feel free to download the following links below:
-
-1. Pendulum-v0 with Planet: https://drive.google.com/file/d/1p27FyXWIBRFniYQ58StAevUmRL7ujxSL/view?usp=sharing
-2. Pendulum-v0 with Dreamer:
-3. HumanoidStandup-v2 with Planet:
-4. HumanoidStandup-v2 with Dreamer:
-
-## Installation Procedures from Homeworks
-
-### Install mujoco:
-```
-mkdir ~/.mujoco
-cd ~/.mujoco
-wget https://www.roboti.us/download/mujoco200_linux.zip
-unzip mujoco200_linux.zip
-mv mujoco200_linux mujoco200
-rm mujoco200_linux.zip
-wget -O mjkey.txt https://github.com/milarobotlearningcourse/ift6163_homeworks/blob/master/hw1/mjkey.txt?raw=true
-```
-The above instructions download MuJoCo for Linux. If you are on Mac or Windows, you will need to change the `wget` address to either
-`https://www.roboti.us/download/mujoco200_macos.zip` or `https://www.roboti.us/download/mujoco200_win64.zip`.
-
-Finally, add the following to bottom of your bashrc:
-```
-export LD_LIBRARY_PATH=~/.mujoco/mujoco200/bin/
-```
-
-### Install other dependencies
-
-There are two options:
-
-A. (Recommended) Install with conda:
-
-	1. Install conda, if you don't already have it, by following the instructions at [this link](https://docs.conda.io/projects/conda/en/latest/user-guide/install/)
-
-	```
-
-	This install will modify the `PATH` variable in your bashrc.
-	You need to open a new terminal for that path change to take place (to be able to find 'conda' in the next step).
-
-	2. Create a conda environment that will contain python 3:
-	```
-	conda create -n big-dreamer python=3.7
-	```
-
-	3. activate the environment (do this every time you open a new terminal and want to run code):
-	```
-	source activate big-dreamer
-	```
-
-	4. Install the requirements into this conda environment
-	```
-	pip install --user -r requirements.txt
-	```
-
-	5. Allow your code to be able to see 'src'
-	```
-	cd <path_to_hw1>
-	$ pip install -e .
-	```
-
-
-### Debugging issues with installing `mujoco-py`
-
-If you run into issues with installing `mujoco-py` (especially on MacOS), here are a few common pointers to help:
-  1. If you run into GCC issues, consider switching to GCC7 (`brew install gcc@7`)
-  2. [Try this](https://github.com/hashicorp/terraform/issues/23033#issuecomment-543507812) if you run into developer verification issues (use due diligence when granting permissions to code from unfamiliar sources)
-  3. StackOverflow is your friend, feel free to shamelessly look up your error and get in touch with your classmates or instructors
-  4. If nothing works and you are frustrated beyond repair, consider using the Colab version of the homework!
diff --git a/base_agent.py b/base_agent.py
new file mode 100644
index 0000000..21d1bd0
--- /dev/null
+++ b/base_agent.py
@@ -0,0 +1,17 @@
+class BaseAgent(object):
+    def __init__(self, **kwargs):
+        super(BaseAgent, self).__init__(**kwargs)
+
+    def train(self) -> dict:
+        """Return a dictionary of logging information."""
+        raise NotImplementedError
+
+    def add_to_replay_buffer(self, paths):
+        raise NotImplementedError
+
+    def sample(self, batch_size):
+        raise NotImplementedError
+
+    def save(self, path):
+        # pass
+        raise NotImplementedError
\ No newline at end of file
diff --git a/commands.sh b/commands.sh
deleted file mode 100644
index a6bf50b..0000000
--- a/commands.sh
+++ /dev/null
@@ -1,60 +0,0 @@
-export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/jgsimard/.mujoco/mujoco200/bin
-# salloc --time=1:0:0 --account=rrg-bengioy-ad --gres=gpu:1 -c 12 --mem=124G
-
-
-ENV=Ant-v2 # [-1, 1]
-#ENV=HalfCheetah-v2 # [-1, 1]
-#ENV=Hopper-v2 # [-1, 1] # short
-#ENV=HumanoiDd-v2 # [-0.4, 0.4]
-#ENV=HumanoidStandup-v2 # [-0.4, 0.4]
-#ENV=InvertedDoublePendulum-v2 # [-1, 1] # short
-#ENV=InvertedPendulum-v2 # [-3, 3]
-#ENV=Reacher-v2 # [-1, 1]
-#ENV=Swimmer-v2 # [-1, 1]
-#ENV=Walker2d-v2 # [-1, 1] # short
-#python src/main.py disable_cuda=False \
-#                    algorithm='dreamer' \
-#                    env=${ENV} \
-#                    action_repeat=2 \
-#                    seed_steps=2000 \
-#                    episodes=100 \
-#                    collect_interval=5 \
-#                    hidden_size=32 \
-#                    belief_size=32 \
-#                    test_interval=200 \
-#                    log_video_freq=-1\
-#                    log_freq=20 \
-#                    use_discount=False \
-#                    n_layers=3 \
-#                    pixel_observation=False \
-#                    embedding_size=256 \
-#                    kl_balance=0.8 \
-##                    latent_distribution=Gaussian\
-#                    latent_distribution=Categorical\
-#                    discrete_latent_dimensions=16 \
-#                    discrete_latent_classes=16
-
-
-python src/main.py disable_cuda=False \
-                    algorithm='dreamer' \
-                    env=${ENV} \
-                    action_repeat=2 \
-                    seed_steps=2000 \
-                    episodes=100 \
-                    collect_interval=5 \
-                    hidden_size=100 \
-                    belief_size=100 \
-                    test_interval=200 \
-                    log_video_freq=-1\
-                    log_freq=20 \
-                    use_discount=False \
-                    n_layers=4 \
-                    pixel_observation=False \
-                    embedding_size=256 \
-                    kl_balance=0.8 \
-#                    latent_distribution=Gaussian\
-                    latent_distribution=Categorical\
-                    discrete_latent_dimensions=32 \
-                    discrete_latent_classes=32 \
-                    action_repeat=1 \
-                    environment_steps_per_update = 20
diff --git a/src/conf/__init__.py b/conf/__init__.py
similarity index 100%
rename from src/conf/__init__.py
rename to conf/__init__.py
diff --git a/conf/config.yaml b/conf/config.yaml
new file mode 100644
index 0000000..dbdbb9f
--- /dev/null
+++ b/conf/config.yaml
@@ -0,0 +1,50 @@
+algorithm: 'dreamer'
+exp_name: 'default'
+seed: 0
+disable_cuda: false
+env: 'Pendulum-v0'
+symbolic_env: False
+max_episode_length: 1000
+experience_size: 1000000   # Original implementation has an unlimited buffer size, but 1 million is the max experience collected anyway
+cnn_activation_function: 'relu'
+dense_activation_function: 'elu'
+embedding_size: 1024  # Note that the default encoder for visual observations outputs a 1024D vector; for other embedding sizes an additional fully-connected layer is used
+hidden_size: 200
+belief_size: 200
+state_size: 30
+action_repeat: 2
+action_noise: 0.3
+episodes: 1000
+seed_episodes: 5
+collect_interval: 100
+batch_size: 50
+chunk_size: 50
+worldmodel_LogProbLoss: False
+overshooting_distance: 50
+overshooting_kl_beta: 0.0
+overshooting_reward_scale: 0.0
+global_kl_beta: 0.0  # Global KL weight (0 to disable)')
+free_nats: 3.0
+bit_depth: 5
+model_learning_rate: 1e-3
+actor_learning_rate: 8e-5
+value_learning_rate: 8e-5
+learning_rate_schedule: 0
+adam_epsilon: 1e-7
+# Note that original has a linear learning rate decay, but it seems unlikely that this makes a significant difference
+grad_clip_norm: 100.0
+planning_horizon: 15
+discount: 0.99
+disclam: 0.95
+optimisation_iters: 10
+candidates: 1000
+top_candidates: 100
+test: False
+test_interval: 25
+test_episodes: 10
+checkpoint_interval: 50
+checkpoint_experience: False
+models: ''
+experience_replay: ''
+render: False
+log_freq : 1
\ No newline at end of file
diff --git a/dreamer.py b/dreamer.py
new file mode 100644
index 0000000..c98ec9f
--- /dev/null
+++ b/dreamer.py
@@ -0,0 +1,152 @@
+import torch
+from torch import optim, nn
+from torch.distributions import Normal
+from torch.nn import functional as F
+
+from models import ActorModel, bottle, CriticModel
+from planet import Planet
+from utils import FreezeParameters, device
+
+
+class Dreamer(Planet):
+    def __init__(self, params, env):
+        super(Dreamer, self).__init__(params, env)
+
+        self.actor_model = ActorModel(self.belief_size,
+                                      self.state_size,
+                                      self.hidden_size,
+                                      self.action_size,
+                                      self.dense_activation_function).to(device)
+        self.planner = self.actor_model
+
+        self.critic_model = CriticModel(self.belief_size,
+                                        self.state_size,
+                                        self.hidden_size,
+                                        self.dense_activation_function).to(device)
+
+        self.actor_optimizer = optim.Adam(self.actor_model.parameters(),
+                                          lr=params['actor_learning_rate'],
+                                          eps=params['adam_epsilon'])
+        self.value_optimizer = optim.Adam(self.critic_model.parameters(),
+                                          lr=params['value_learning_rate'],
+                                          eps=params['adam_epsilon'])
+
+        self.discount = params['discount']
+        self.disclam = params['disclam']
+
+    def imagine_ahead(self, prev_state, prev_belief):
+        '''
+        imagine_ahead is the function to draw the imaginary tracjectory using the dynamics model, actor, critic.
+        Input: current state (posterior), current belief (hidden), policy, transition_model  # torch.Size([50, 30]) torch.Size([50, 200])
+        Output: generated trajectory of features includes beliefs, prior_states, prior_means, prior_std_devs
+                torch.Size([49, 50, 200]) torch.Size([49, 50, 30]) torch.Size([49, 50, 30]) torch.Size([49, 50, 30])
+        '''
+        flatten = lambda x: x.view([-1] + list(x.size()[2:]))
+        prev_belief = flatten(prev_belief)
+        prev_state = flatten(prev_state)
+
+        # Create lists for hidden states (cannot use single tensor as buffer because autograd won't work with inplace writes)
+        T = self.planning_horizon
+        beliefs, prior_states, prior_means, prior_std_devs = [torch.empty(0)] * T, [torch.empty(0)] * T, [
+            torch.empty(0)] * T, [torch.empty(0)] * T
+        beliefs[0], prior_states[0] = prev_belief, prev_state
+
+        # Loop over time sequence
+        for t in range(T - 1):
+            _state = prior_states[t]
+            actions = self.planner.get_action(beliefs[t].detach(), _state.detach())
+            # Compute belief (deterministic hidden state)
+            hidden = self.transition_model.act_fn(
+                self.transition_model.fc_embed_state_action(torch.cat([_state, actions], dim=1)))
+            beliefs[t + 1] = self.transition_model.rnn(hidden, beliefs[t])
+            # Compute state prior by applying transition dynamics
+            hidden = self.transition_model.act_fn(self.transition_model.fc_embed_belief_prior(beliefs[t + 1]))
+            prior_means[t + 1], _prior_std_dev = torch.chunk(self.transition_model.fc_state_prior(hidden), 2, dim=1)
+            prior_std_devs[t + 1] = F.softplus(_prior_std_dev) + self.transition_model.min_std_dev
+            prior_states[t + 1] = prior_means[t + 1] + prior_std_devs[t + 1] * torch.randn_like(prior_means[t + 1])
+            # Return new hidden states
+
+        # imagined_traj = [beliefs, prior_states, prior_means, prior_std_devs]
+        return torch.stack(beliefs[1:], dim=0), \
+               torch.stack(prior_states[1:], dim=0), \
+               torch.stack(prior_means[1:], dim=0), \
+               torch.stack(prior_std_devs[1:], dim=0)
+
+    def update_actor_critic(self, posterior_states, beliefs) -> dict:
+        logs = {}
+        ####################
+        # BEHAVIOUR LEARNING
+        ####################
+
+        # Imagine trajectories
+        with torch.no_grad():
+            actor_states = posterior_states.detach()
+            actor_beliefs = beliefs.detach()
+        with FreezeParameters(self.model_modules):
+            imged_beliefs, imged_prior_states, imged_prior_means, imged_prior_std_devs = self.imagine_ahead(
+                actor_states, actor_beliefs)
+
+        # Predict Rewards & Values
+        with FreezeParameters(self.model_modules + self.critic_model.modules):
+            imged_reward = bottle(self.reward_model, (imged_beliefs, imged_prior_states))
+            value_pred = bottle(self.critic_model, (imged_beliefs, imged_prior_states))
+
+        # Compute Values estimates
+        returns = lambda_return(imged_reward, value_pred, bootstrap=value_pred[-1], discount=self.discount,
+                                lambda_=self.disclam)
+
+        # Update Actor weights
+        actor_loss = -torch.mean(returns)
+        logs['actor_loss'] = actor_loss
+
+        self.actor_optimizer.zero_grad()
+        actor_loss.backward()
+        nn.utils.clip_grad_norm_(self.actor_model.parameters(), self.grad_clip_norm, norm_type=2)
+        self.actor_optimizer.step()
+
+        with torch.no_grad():
+            value_beliefs = imged_beliefs.detach()
+            value_prior_states = imged_prior_states.detach()
+            target_return = returns.detach()
+        # detach the input tensor from the transition network.
+        value_dist = Normal(bottle(self.critic_model, (value_beliefs, value_prior_states)), 1)
+        value_loss = -value_dist.log_prob(target_return).mean(dim=(0, 1))
+        logs['value_loss'] = value_loss
+
+        # Update model parameters
+        self.value_optimizer.zero_grad()
+        value_loss.backward()
+        nn.utils.clip_grad_norm_(self.critic_model.parameters(), self.grad_clip_norm, norm_type=2)
+        self.value_optimizer.step()
+
+        return logs
+
+    def train_step(self) -> dict:
+        ####################
+        # DYNAMICS LEARNING
+        ####################
+        logs = super(Dreamer, self).train_step()
+        ####################
+        # BEHAVIOUR LEARNING
+        ####################
+        logs.update(self.update_actor_critic(self.posterior_states, self.beliefs))
+        return logs
+
+
+def lambda_return(imged_reward, value_pred, bootstrap, discount=0.99, lambda_=0.95):
+    # Setting lambda=1 gives a discounted Monte Carlo return.
+    # Setting lambda=0 gives a fixed 1-step return.
+    next_values = torch.cat([value_pred[1:], bootstrap[None]], 0)
+    discount_tensor = discount * torch.ones_like(imged_reward)  # pcont
+    inputs = imged_reward + discount_tensor * next_values * (1 - lambda_)
+    last = bootstrap
+    indices = reversed(range(len(inputs)))
+    outputs = []
+    for index in indices:
+        inp, disc = inputs[index], discount_tensor[index]
+        last = inp + disc * lambda_ * last
+        outputs.append(last)
+    outputs = list(reversed(outputs))
+    outputs = torch.stack(outputs, 0)
+    returns = outputs
+    return returns
diff --git a/env.py b/env.py
new file mode 100644
index 0000000..9c81583
--- /dev/null
+++ b/env.py
@@ -0,0 +1,194 @@
+import cv2
+import numpy as np
+import torch
+
+GYM_ENVS = ['Pendulum-v0', 'MountainCarContinuous-v0', 'Ant-v2', 'HalfCheetah-v2', 'Hopper-v2', 'Humanoid-v2',
+            'HumanoidStandup-v2', 'InvertedDoublePendulum-v2', 'InvertedPendulum-v2', 'Reacher-v2', 'Swimmer-v2',
+            'Walker2d-v2']
+CONTROL_SUITE_ENVS = ['cartpole-balance', 'cartpole-swingup', 'reacher-easy', 'finger-spin', 'cheetah-run',
+                      'ball_in_cup-catch', 'walker-walk', 'reacher-hard', 'walker-run', 'humanoid-stand',
+                      'humanoid-walk', 'fish-swim', 'acrobot-swingup']
+CONTROL_SUITE_ACTION_REPEATS = {'cartpole': 8, 'reacher': 4, 'finger': 2, 'cheetah': 4, 'ball_in_cup': 6, 'walker': 2,
+                                'humanoid': 2, 'fish': 2, 'acrobot': 4}
+
+
+# Preprocesses an observation inplace (from float32 Tensor [0, 255] to [-0.5, 0.5])
+def preprocess_observation_(observation, bit_depth):
+    # Quantise to given bit depth and centre
+    observation.div_(2 ** (8 - bit_depth)).floor_().div_(2 ** bit_depth).sub_(0.5)
+    # Dequantise (to approx. match likelihood of PDF of continuous images vs. PMF of discrete images)
+    observation.add_(torch.rand_like(observation).div_(2 ** bit_depth))
+
+
+# Postprocess an observation for storage (from float32 numpy array [-0.5, 0.5] to uint8 numpy array [0, 255])
+def postprocess_observation(observation, bit_depth):
+    return np.clip(np.floor((observation + 0.5) * 2 ** bit_depth) * 2 ** (8-bit_depth), 0, 2 ** 8 - 1).astype(np.uint8)
+
+
+def _images_to_observation(images, bit_depth):
+    # Resize and put channel first
+    images = torch.tensor(cv2.resize(images, (64, 64), interpolation=cv2.INTER_LINEAR).transpose(2, 0, 1), dtype=torch.float32)
+    # Quantise, centre and dequantise inplace
+    preprocess_observation_(images, bit_depth)
+    # Add batch dimension
+    return images.unsqueeze(dim=0)
+
+
+class ControlSuiteEnv():
+    def __init__(self, env, symbolic, seed, max_episode_length, action_repeat, bit_depth):
+        from dm_control import suite
+        from dm_control.suite.wrappers import pixels
+        domain, task = env.split('-')
+        self.symbolic = symbolic
+        self._env = suite.load(domain_name=domain, task_name=task, task_kwargs={'random': seed})
+        if not symbolic:
+            self._env = pixels.Wrapper(self._env)
+        self.max_episode_length = max_episode_length
+        self.action_repeat = action_repeat
+        if action_repeat != CONTROL_SUITE_ACTION_REPEATS[domain]:
+            print('Using action repeat %d; recommended action repeat for domain is %d' % (
+                action_repeat, CONTROL_SUITE_ACTION_REPEATS[domain]))
+        self.bit_depth = bit_depth
+
+    def reset(self):
+        self.t = 0  # Reset internal timer
+        state = self._env.reset()
+        if self.symbolic:
+            return torch.tensor(np.concatenate(
+                [np.asarray([obs]) if isinstance(obs, float) else obs for obs in state.observation.values()], axis=0),
+                dtype=torch.float32).unsqueeze(dim=0)
+        else:
+            return _images_to_observation(self._env.physics.render(camera_id=0), self.bit_depth)
+
+    def step(self, action):
+        action = action.detach().numpy()
+        reward = 0
+        for k in range(self.action_repeat):
+            state = self._env.step(action)
+            reward += state.reward
+            self.t += 1  # Increment internal timer
+            done = state.last() or self.t == self.max_episode_length
+            if done:
+                break
+        if self.symbolic:
+            observation = torch.tensor(np.concatenate(
+                [np.asarray([obs]) if isinstance(obs, float) else obs for obs in state.observation.values()], axis=0),
+                dtype=torch.float32).unsqueeze(dim=0)
+        else:
+            observation = _images_to_observation(self._env.physics.render(camera_id=0), self.bit_depth)
+        return observation, reward, done
+
+    def render(self):
+        cv2.imshow('screen', self._env.physics.render(camera_id=0)[:, :, ::-1])
+        cv2.waitKey(1)
+
+    def close(self):
+        cv2.destroyAllWindows()
+        self._env.close()
+
+    @property
+    def observation_size(self):
+        return sum([(1 if len(obs.shape) == 0 else obs.shape[0]) for obs in
+                    self._env.observation_spec().values()]) if self.symbolic else (3, 64, 64)
+
+    @property
+    def action_size(self):
+        return self._env.action_spec().shape[0]
+
+    # Sample an action randomly from a uniform distribution over all valid actions
+    def sample_random_action(self):
+        spec = self._env.action_spec()
+        return torch.from_numpy(np.random.uniform(spec.minimum, spec.maximum, spec.shape))
+
+
+class GymEnv():
+    def __init__(self, env, symbolic, seed, max_episode_length, action_repeat, bit_depth):
+        import gym
+        self.symbolic = symbolic
+        self._env = gym.make(env)
+        self._env.seed(seed)
+        self._env.action_space.seed(seed)
+        self.max_episode_length = max_episode_length
+        self.action_repeat = action_repeat
+        self.bit_depth = bit_depth
+
+    def reset(self):
+        self.t = 0  # Reset internal timer
+        state = self._env.reset()
+        if self.symbolic:
+            return torch.tensor(state, dtype=torch.float32).unsqueeze(dim=0)
+        else:
+            return _images_to_observation(self._env.render(mode='rgb_array'), self.bit_depth)
+
+    def step(self, action):
+        action = action.detach().numpy()
+        reward = 0
+        for k in range(self.action_repeat):
+            state, reward_k, done, _ = self._env.step(action)
+            reward += reward_k
+            self.t += 1  # Increment internal timer
+            done = done or self.t == self.max_episode_length
+            if done:
+                break
+        if self.symbolic:
+            observation = torch.tensor(state, dtype=torch.float32).unsqueeze(dim=0)
+        else:
+            observation = _images_to_observation(self._env.render(mode='rgb_array'), self.bit_depth)
+        return observation, reward, done
+
+    def render(self):
+        self._env.render()
+
+    def close(self):
+        self._env.close()
+
+    @property
+    def observation_size(self):
+        return self._env.observation_space.shape[0] if self.symbolic else (3, 64, 64)
+
+    @property
+    def action_size(self):
+        return self._env.action_space.shape[0]
+
+    # Sample an action randomly from a uniform distribution over all valid actions
+    def sample_random_action(self):
+        return torch.from_numpy(self._env.action_space.sample())
+
+
+def Env(env, symbolic, seed, max_episode_length, action_repeat, bit_depth):
+    if env in GYM_ENVS:
+        return GymEnv(env, symbolic, seed, max_episode_length, action_repeat, bit_depth)
+    elif env in CONTROL_SUITE_ENVS:
+        return ControlSuiteEnv(env, symbolic, seed, max_episode_length, action_repeat, bit_depth)
+
+
+# Wrapper for batching environments together
+class EnvBatcher():
+    def __init__(self, env_class, env_args, env_kwargs, n):
+        self.n = n
+        self.envs = [env_class(*env_args, **env_kwargs) for _ in range(n)]
+        self.dones = [True] * n
+
+    # Resets every environment and returns observation
+    def reset(self):
+        observations = [env.reset() for env in self.envs]
+        self.dones = [False] * self.n
+        return torch.cat(observations)
+
+    # Steps/resets every environment and returns (observation, reward, done)
+    def step(self, actions):
+        # Done mask to blank out observations and zero rewards for previously terminated environments
+        done_mask = torch.nonzero(torch.tensor(self.dones))[:, 0]
+        observations, rewards, dones = zip(*[env.step(action) for env, action in zip(self.envs, actions)])
+        dones = [d or prev_d for d, prev_d in
+                 zip(dones, self.dones)]  # Env should remain terminated if previously terminated
+        self.dones = dones
+        observations = torch.cat(observations)
+        rewards = torch.tensor(rewards, dtype=torch.float32)
+        dones = torch.tensor(dones, dtype=torch.uint8)
+        observations[done_mask] = 0
+        rewards[done_mask] = 0
+        return observations, rewards, dones
+
+    def close(self):
+        [env.close() for env in self.envs]
diff --git a/logger.py b/logger.py
new file mode 100644
index 0000000..a64931c
--- /dev/null
+++ b/logger.py
@@ -0,0 +1,74 @@
+import os
+from tensorboardX import SummaryWriter
+import numpy as np
+
+class Logger:
+    def __init__(self, log_dir, n_logged_samples=10, summary_writer=None):
+        self._log_dir = log_dir
+        print('########################')
+        print('logging outputs to ', log_dir)
+        print('########################')
+        self._n_logged_samples = n_logged_samples
+        self._summ_writer = SummaryWriter(log_dir, flush_secs=1, max_queue=1)
+
+    def log_scalar(self, scalar, name, step_):
+        self._summ_writer.add_scalar('{}'.format(name), scalar, step_)
+
+    def log_scalars(self, scalar_dict, group_name, step, phase):
+        """Will log all scalars in the same plot."""
+        self._summ_writer.add_scalars('{}_{}'.format(group_name, phase), scalar_dict, step)
+
+    def log_image(self, image, name, step):
+        assert(len(image.shape) == 3)  # [C, H, W]
+        self._summ_writer.add_image('{}'.format(name), image, step)
+
+    def log_video(self, video_frames, name, step, fps=10):
+        assert len(video_frames.shape) == 5, "Need [N, T, C, H, W] input tensor for video logging!"
+        self._summ_writer.add_video('{}'.format(name), video_frames, step, fps=fps)
+
+    def log_paths_as_videos(self, paths, step, max_videos_to_save=2, fps=10, video_title='video'):
+
+        # reshape the rollouts
+        videos = [np.transpose(p['image_obs'], [0, 3, 1, 2]) for p in paths]
+
+        # max rollout length
+        max_videos_to_save = np.min([max_videos_to_save, len(videos)])
+        max_length = videos[0].shape[0]
+        for i in range(max_videos_to_save):
+            if videos[i].shape[0]>max_length:
+                max_length = videos[i].shape[0]
+
+        # pad rollouts to all be same length
+        for i in range(max_videos_to_save):
+            if videos[i].shape[0]<max_length:
+                padding = np.tile([videos[i][-1]], (max_length-videos[i].shape[0],1,1,1))
+                videos[i] = np.concatenate([videos[i], padding], 0)
+
+        # log videos to tensorboard event file
+        videos = np.stack(videos[:max_videos_to_save], 0)
+        self.log_video(videos, video_title, step, fps=fps)
+
+    def log_figures(self, figure, name, step, phase):
+        """figure: matplotlib.pyplot figure handle"""
+        assert figure.shape[0] > 0, "Figure logging requires input shape [batch x figures]!"
+        self._summ_writer.add_figure('{}_{}'.format(name, phase), figure, step)
+
+    def log_figure(self, figure, name, step, phase):
+        """figure: matplotlib.pyplot figure handle"""
+        self._summ_writer.add_figure('{}_{}'.format(name, phase), figure, step)
+
+    def log_graph(self, array, name, step, phase):
+        """figure: matplotlib.pyplot figure handle"""
+        im = plot_graph(array)
+        self._summ_writer.add_image('{}_{}'.format(name, phase), im, step)
+
+    def dump_scalars(self, log_path=None):
+        log_path = os.path.join(self._log_dir, "scalar_data.json") if log_path is None else log_path
+        self._summ_writer.export_scalars_to_json(log_path)
+
+    def flush(self):
+        self._summ_writer.flush()
+
+
+
+
diff --git a/main.py b/main.py
new file mode 100644
index 0000000..d8614a6
--- /dev/null
+++ b/main.py
@@ -0,0 +1,228 @@
+import random
+import time
+
+import hydra
+import numpy as np
+import torch
+from omegaconf import DictConfig
+from tqdm import tqdm
+from torchvision.utils import make_grid, save_image
+
+from planet import Planet
+from dreamer import Dreamer
+from logger import Logger
+from env import Env, EnvBatcher
+from utils import init_gpu, device
+
+
+@hydra.main(config_path="conf", config_name="config")
+def my_main(cfg: DictConfig):
+    my_app(cfg)
+
+
+def my_app(cfg: DictConfig):
+    # print(OmegaConf.to_yaml(cfg))
+    print("Command Dir:", os.getcwd())
+
+    params = vars(cfg)
+    # params.extend(env_args)
+    for key, value in cfg.items():
+        params[key] = value
+    print("params: ", params)
+
+    # ##################################
+    # ### CREATE DIRECTORY FOR LOGGING
+    # ##################################
+    logdir_prefix = 'project_'  # keep for autograder
+
+    data_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'data')
+
+    if not (os.path.exists(data_path)):
+        os.makedirs(data_path)
+
+    logdir = logdir_prefix + cfg.exp_name + '_' + cfg.env + '_' + time.strftime("%d-%m-%Y_%H-%M-%S")
+    logdir = os.path.join(data_path, logdir)
+    params['logdir'] = logdir
+    if not (os.path.exists(logdir)):
+        os.makedirs(logdir)
+    from omegaconf import open_dict
+    with open_dict(cfg):
+        cfg.logdir = logdir
+
+    print("\nLOGGING TO: ", logdir, "\n")
+
+    #############
+    # INIT Structure
+    #############
+    np.random.seed(params['seed'])
+    torch.manual_seed(params['seed'])
+    random.seed(params['seed'])
+
+    init_gpu(use_gpu=not params['disable_cuda'])
+
+    logger = Logger(params['logdir'])
+
+    # metrics = {'steps': [], 'episodes': [], 'train_rewards': [], 'test_episodes': [], 'test_rewards': [],
+    #            'observation_loss': [], 'reward_loss': [], 'kl_loss': [], 'actor_loss': [], 'value_loss': []}
+
+    #############
+    # ENV
+    #############
+    env = Env(params['env'],
+              params['symbolic_env'],
+              params['seed'],
+              params['max_episode_length'],
+              params['action_repeat'],
+              params['bit_depth'])
+
+    #############
+    # Model
+    #############
+
+    if params['algorithm'] == 'planet':
+        model = Planet(params, env)
+    elif params['algorithm'] == 'dreamer':
+        model = Dreamer(params, env)
+    elif params['algorithm'] == 'dreamerV2':
+        raise NotImplementedError('DreamerV2 is not yet implemented')
+    else:
+        raise NotImplementedError(f'algorithm {params["algorithm"]} is not yet implemented.')
+
+    env_steps, num_episodes = model.randomly_initialize_replay_buffer()
+
+    ###################
+    # RUN TRAINING
+    ###################
+    train_step = 0
+
+    # Training
+    for episode in tqdm(range(num_episodes+1, params['episodes']+1), total=params['episodes'], initial=num_episodes+1):
+        # Model fitting
+        print("training loop")
+        for s in tqdm(range(params['collect_interval'])):
+            logs = model.train_step()
+
+            # if train_step % params['log_freq'] == 0:
+            #     print("Perform Logging")
+            #     # perform the logging
+            #     for key, value in logs.items():
+            #         print('{} : {}'.format(key, value))
+            #         logger.log_scalar(value, key, train_step)
+            #     print('Done logging...\n')
+            #
+            #     logger.flush()
+
+            train_step += 1
+
+        ##########################
+        # Environment interaction
+        ##########################
+        print("Data collection")
+        with torch.no_grad():
+            observation = env.reset()
+            total_reward = 0
+            belief = torch.zeros(1, params['belief_size'], device=device)
+            posterior_state = torch.zeros(1, params['state_size'], device=device)
+            action = torch.zeros(1, env.action_size, device=device)
+
+            pbar = tqdm(range(params['max_episode_length'] // params['action_repeat']))
+            for t in pbar:
+                outputs = model.update_belief_and_act(
+                    env,
+                    belief,
+                    posterior_state,
+                    action,
+                    observation.to(device=device),
+                    explore=True)
+                belief, posterior_state, action, next_observation, reward, done = outputs
+
+                # store the new stuff
+                model.replay_buffer.append(observation, action, reward, done)
+
+                total_reward += reward
+                observation = next_observation
+
+                if params['render']:
+                    env.render()
+                if done:
+                    pbar.close()
+                    break
+
+            env_steps += t * params['action_repeat']
+            num_episodes += 1
+            logs['episodique_total_reward'] = total_reward
+
+        ##########################
+        # Test model
+        ##########################
+
+        # TODO : Test Model
+        if episode % params['test_interval'] == 0:
+            print("Test model")
+            model.eval()
+
+            # Initialise parallelised test environments
+            test_envs = EnvBatcher(Env, (params['env'],
+                                         params['symbolic_env'],
+                                         params['seed'],
+                                         params['max_episode_length'],
+                                         params['action_repeat'],
+                                         params['bit_depth']),
+                                   {},
+                                   params['test_episodes'])
+
+            with torch.no_grad():
+                observation = test_envs.reset()
+                total_rewards = np.zeros((params['test_episodes'],))
+                video_frames = []
+
+                belief = torch.zeros(params['test_episodes'], params['belief_size'],  device=device)
+                posterior_state = torch.zeros(params['test_episodes'], params['state_size'],device=device)
+                action = torch.zeros(params['test_episodes'], env.action_size, device=device)
+
+                pbar = tqdm(range(params['max_episode_length'] // params['action_repeat']))
+                for t in pbar:
+                    outputs = model.update_belief_and_act(
+                        test_envs,
+                        belief,
+                        posterior_state,
+                        action,
+                        observation.to(device=device))
+                    belief, posterior_state, action, next_observation, reward, done = outputs
+
+                    total_rewards += reward.numpy()
+                    # Collect real vs. predicted frames for video
+                    if not params['symbolic_env']:
+                        video_frames.append(make_grid(
+                            torch.cat([observation, model.observation_model(belief, posterior_state).cpu()], dim=3) + 0.5,
+                            nrow=5).numpy())  # Decentre
+                    observation = next_observation
+                    if done.sum().item() == params['test_episodes']:
+                        pbar.close()
+                        break
+
+                logs['Eval_avg_return'] = total_rewards.mean()
+                logs['Eval_std_return'] = total_rewards.std()
+
+        # TODO : Save Model
+
+
+        if train_step % params['log_freq'] == 0:
+            print("Perform Logging")
+            # perform the logging
+            for key, value in logs.items():
+                print('{} : {}'.format(key, value))
+                logger.log_scalar(value, key, env_steps)
+            print('Done logging...\n')
+
+            logger.flush()
+
+    # Close training environment
+    env.close()
+
+
+if __name__ == "__main__":
+    import os
+
+    print("Command Dir:", os.getcwd())
+    my_main()
diff --git a/memory.py b/memory.py
new file mode 100644
index 0000000..607c41e
--- /dev/null
+++ b/memory.py
@@ -0,0 +1,63 @@
+import numpy as np
+import torch
+
+from env import postprocess_observation, preprocess_observation_
+
+
+class ExperienceReplay():
+    def __init__(self, size, symbolic_env, observation_size, action_size, bit_depth, device):
+        self.device = device
+        self.symbolic_env = symbolic_env
+        self.size = size
+        self.observations = np.empty((size, observation_size) if symbolic_env else (size, 3, 64, 64),
+                                     dtype=np.float32 if symbolic_env else np.uint8)
+        self.actions = np.empty((size, action_size), dtype=np.float32)
+        self.rewards = np.empty((size,), dtype=np.float32)
+        self.nonterminals = np.empty((size, 1), dtype=np.float32)
+        self.idx = 0
+        self.full = False  # Tracks if memory has been filled/all slots are valid
+        self.steps, self.episodes = 0, 0  # Tracks how much experience has been used in total
+        self.bit_depth = bit_depth
+
+    def append(self, observation, action, reward, done):
+        if self.symbolic_env:
+            self.observations[self.idx] = observation.numpy()
+        else:
+            self.observations[self.idx] = postprocess_observation(observation.numpy(),
+                                                                  self.bit_depth)  # Decentre and discretise visual observations (to save memory)
+        self.actions[self.idx] = action.numpy()
+        self.rewards[self.idx] = reward
+        self.nonterminals[self.idx] = not done
+        self.idx = (self.idx + 1) % self.size
+        self.full = self.full or self.idx == 0
+        self.steps, self.episodes = self.steps + 1, self.episodes + (1 if done else 0)
+
+    # Returns an index for a valid single sequence chunk uniformly sampled from the memory
+    def _sample_idx(self, L):
+        valid_idx = False
+        while not valid_idx:
+            idx = np.random.randint(0, self.size if self.full else self.idx - L)
+            idxs = np.arange(idx, idx + L) % self.size
+            valid_idx = not self.idx in idxs[1:]  # Make sure data does not cross the memory index
+        return idxs
+
+    def _retrieve_batch(self, idxs, n, L):
+        vec_idxs = idxs.transpose().reshape(-1)  # Unroll indices
+        observations = torch.as_tensor(self.observations[vec_idxs].astype(np.float32))
+        if not self.symbolic_env:
+            preprocess_observation_(observations, self.bit_depth)  # Undo discretisation for visual observations
+        return observations.reshape(L, n, *observations.shape[1:]), self.actions[vec_idxs].reshape(L, n, -1), \
+               self.rewards[vec_idxs].reshape(L, n), self.nonterminals[vec_idxs].reshape(L, n, 1)
+
+    # Returns a batch of sequence chunks uniformly sampled from the memory
+    def sample(self, n, L):
+        batch = self._retrieve_batch(np.asarray([self._sample_idx(L) for _ in range(n)]), n, L)
+        # print(np.asarray([self._sample_idx(L) for _ in range(n)]))
+        # [1578 1579 1580 ... 1625 1626 1627]                                                                                                                                        | 0/100 [00:00<?, ?it/s]
+        # [1049 1050 1051 ... 1096 1097 1098]
+        # [1236 1237 1238 ... 1283 1284 1285]
+        # ...
+        # [2199 2200 2201 ... 2246 2247 2248]
+        # [ 686  687  688 ...  733  734  735]
+        # [1377 1378 1379 ... 1424 1425 1426]]
+        return [torch.as_tensor(item).to(device=self.device) for item in batch]
diff --git a/models-2.py b/models-2.py
new file mode 100644
index 0000000..532e59d
--- /dev/null
+++ b/models-2.py
@@ -0,0 +1,193 @@
+import torch
+
+from torch import nn, Tensor
+from typing import List, Set, Dict, Tuple, Optional, Union
+# from torch.nn import functional as F
+import torch.nn.functional as F
+import torch.distributions as D
+
+from torch.nn.common_types import *
+
+Activation = Union[str, nn.Module]
+
+
+def merge_belief_and_state(belief: Tensor, state: Tensor) -> Tensor:
+    return torch.cat([belief, state], dim=1)
+
+class TransitionModel(nn.Module):
+    def __init__(self) -> None:
+        super(TransitionModel, self).__init__()
+
+
+class ObservationModel(nn.Module):
+    def __init__(self,
+                 belief_size: int,
+                 state_size: int,
+                 embedding_size: int,
+                 activation: Activation = 'relu') -> None:
+        super(ObservationModel, self).__init__()
+
+        if isinstance(activation, str):
+            activation = getattr(F, activation)()
+
+        self.embedding_size = embedding_size
+        self.linear = nn.Linear(belief_size + state_size, embedding_size)
+        self.deconvs = nn.Sequential(
+            # in_channels, out_channels, kernel_size, stride: _size_2_t = 1
+            nn.ConvTranspose2d(embedding_size, 128, 5, 2),
+            activation,
+            nn.ConvTranspose2d(128, 64, 5, 2),
+            activation,
+            nn.ConvTranspose2d(64, 32, 6, 2),
+            activation,
+            nn.ConvTranspose2d(32, 3, 6, 2)
+        )
+
+    def forward(self, belief: Tensor, state: Tensor) -> Tensor:
+        x = self.linear(merge_belief_and_state(belief, state))
+        x = x.view(-1, self.embedding_size, 1, 1)
+        return self.deconvs(x)
+
+
+class RewardModel(nn.Module):
+    def __init__(self,
+                 belief_size: int,
+                 state_size: int,
+                 hidden_size: int,
+                 activation: Activation = 'relu') -> None:
+        super(RewardModel, self).__init__()
+
+        if isinstance(activation, str):
+            activation = getattr(F, activation)()
+
+        self.model = nn.Sequential(
+            nn.Linear(belief_size + state_size, hidden_size),
+            activation,
+            nn.Linear(hidden_size, hidden_size),
+            activation,
+            nn.Linear(hidden_size, 1)
+        )
+
+    def forward(self, belief: Tensor, state: Tensor) -> Tensor:
+        return self.model(merge_belief_and_state(belief, state)).squeeze(dim=1)
+
+
+class CnnImageEncoder(nn.Module):
+    def __init__(self,
+                 embedding_size: int,
+                 activation: Activation = 'relu') -> None:
+        super(CnnImageEncoder, self).__init__()
+
+        if isinstance(activation, str):
+            activation = getattr(F, activation)()
+
+        self.model = nn.Sequential(
+            # in_channels, out_channels, kernel_size, stride
+            nn.Conv2d(3, 32, 4, 2),
+            activation,
+            nn.Conv2d(32, 64, 4, 2),
+            activation,
+            nn.Conv2d(64, 128, 4, 2),
+            activation,
+            nn.Conv2d(128, 256, 4, 2),  # (B, 3, H, W) ->  (B, 256, H/16, W/16)
+            activation,
+            nn.Flatten(),
+            nn.Identity() if embedding_size == 1024 else nn.Linear(1024, embedding_size)
+        )
+
+    def forward(self, observation: Tensor) -> Tensor:
+        return self.model(observation)
+
+
+class LinearCombination(nn.Module):
+    def __init__(self, in1_size, in2_size, out_size):
+        super(LinearCombination, self).__init__()
+        self.in1_linear = nn.Linear(in1_size, out_size)
+        self.in2_linear = nn.Linear(in2_size, bias=False)
+
+    def forward(self, in1, in2):
+        return self.in1_linear(in1) + self.in1_linear(in2)
+
+def activation(x, layer_norm=False):
+    norm = nn.LayerNorm if layer_norm else nn.Identity()
+    return F.elu(norm(x))
+
+def diag_normal(x: Tensor, min_std=0.1, max_std=2.0):
+    mean, std = x.chunk(2, -1)
+    std = max_std * torch.sigmoid(std) + min_std
+    return D.independent.Independent(D.normal.Normal(mean, std), 1)
+
+
+# TODO Use Importance Weighted VAE to improve performance.
+class RSSM(nn.Module):
+    def __init__(self,
+                 embedding_size: int,
+                 action_size: int,
+                 deterministic_size: int,
+                 stochastic_size: int,
+                 hidden_size: int,
+                 rnn_layers: int):
+        super(RSSM, self).__init__()
+        self.rnn = nn.GRU(input_size=hidden_size, hidden_size=deterministic_size, num_layers=rnn_layers)
+
+        self.za_combination = LinearCombination(stochastic_size, action_size, hidden_size)
+
+        self.prior_h = nn.Linear(deterministic_size, hidden_size)
+        self.prior_out = nn.Linear(hidden_size, stochastic_size * 2)
+
+        self.he_combination = LinearCombination(deterministic_size, embedding_size, hidden_size)
+        self.posterior_parameters = nn.Linear(hidden_size, stochastic_size * 2)
+
+    def prior(self, h):
+        return self.prior_out(activation(self.prior_h(h)))
+
+    def forward(self,
+                embedded: Tensor,   # (T, B, embedding_size)
+                action: Tensor,     # (T, B, action_size)
+                reset: Tensor,      # (T, B)
+                z_in: Tensor,       # (T, B, stochastic_size)
+                h_in: Tensor        # (T, B, deterministic_size)
+                ):
+        priors = []
+        posts = []
+        states_h = []
+        samples = []
+
+        for t in range(T):
+            za = activation(self.za_combination(z_in, action))
+            h_out = self.rnn(za, h_in)
+            he = activation(self.he_combination(h_out, embedded))
+            posterior_parameters = self.posterior_parameters(he)
+            posterior_distribution = diag_normal(posterior_parameters)
+            sample = posterior_distribution.rsample().reshape(action.shape[0], -1)
+
+            # record step
+            posts.append(posterior_parameters)
+            samples.append(sample)
+            states_h.append(h_out)
+
+            # update states for next step
+            h_in = h_out
+            z_in = sample
+
+        posts = torch.stack(posts)                              # (T,B,2S)
+        states_h = torch.stack(states_h)                        # (T,B,D)
+        samples = torch.stack(samples)                          # (T,B,S)
+        priors =  self.pri# (T,B,2S)
+
+        return(
+            priors,
+            posts,
+            samples,
+            states_h
+        )
+
+class Dreamer(nn.Module):
+    def __init__(self):
+        super(DreamerV1, self).__init__()
+        self.
+
+
+class Planet(nn.Module):
+    def __init__(self):
+        super(Planet, self).__init__()
\ No newline at end of file
diff --git a/models.py b/models.py
new file mode 100644
index 0000000..c921db6
--- /dev/null
+++ b/models.py
@@ -0,0 +1,346 @@
+from typing import Optional, List
+
+import numpy as np
+import torch
+import torch.distributions
+from torch import jit, nn, Tensor
+from torch.distributions.normal import Normal
+from torch.distributions.transformed_distribution import TransformedDistribution
+from torch.nn import functional as F
+
+
+# Wraps the input tuple for a function to process a time x batch x features sequence in batch x features (assumes one output)
+def bottle(f, x_tuple):
+    x_sizes = tuple(map(lambda x: x.size(), x_tuple))
+    y = f(*map(lambda x: x[0].view(x[1][0] * x[1][1], *x[1][2:]), zip(x_tuple, x_sizes)))
+    y_size = y.size()
+    output = y.view(x_sizes[0][0], x_sizes[0][1], *y_size[1:])
+    return output
+
+
+class TransitionModel(jit.ScriptModule):
+    __constants__ = ['min_std_dev']
+
+    def __init__(self, belief_size, state_size, action_size, hidden_size, embedding_size, activation_function='relu',
+                 min_std_dev=0.1):
+        super().__init__()
+        self.act_fn = getattr(F, activation_function)
+        self.min_std_dev = min_std_dev
+        self.fc_embed_state_action = nn.Linear(state_size + action_size, belief_size)
+        self.rnn = nn.GRUCell(belief_size, belief_size)
+        self.fc_embed_belief_prior = nn.Linear(belief_size, hidden_size)
+        self.fc_state_prior = nn.Linear(hidden_size, 2 * state_size)
+        self.fc_embed_belief_posterior = nn.Linear(belief_size + embedding_size, hidden_size)
+        self.fc_state_posterior = nn.Linear(hidden_size, 2 * state_size)
+        self.modules = [self.fc_embed_state_action, self.fc_embed_belief_prior, self.fc_state_prior,
+                        self.fc_embed_belief_posterior, self.fc_state_posterior]
+
+    # Operates over (previous) state, (previous) actions, (previous) belief, (previous) nonterminals (mask), and (current) observations
+    # Diagram of expected inputs and outputs for T = 5 (-x- signifying beginning of output belief/state that gets sliced off):
+    # t :  0  1  2  3  4  5
+    # o :    -X--X--X--X--X-
+    # a : -X--X--X--X--X-
+    # n : -X--X--X--X--X-
+    # pb: -X-
+    # ps: -X-
+    # b : -x--X--X--X--X--X-
+    # s : -x--X--X--X--X--X-
+    @jit.script_method
+    def forward(self,
+                prev_state: Tensor,
+                actions: Tensor,
+                prev_belief: Tensor,
+                observations: Optional[Tensor] = None,
+                nonterminals: Optional[Tensor] = None) -> List[Tensor]:
+        '''
+        Input: init_belief, init_state:  torch.Size([50, 200]) torch.Size([50, 30])
+        Output: beliefs, prior_states, prior_means, prior_std_devs, posterior_states, posterior_means, posterior_std_devs
+                torch.Size([49, 50, 200]) torch.Size([49, 50, 30]) torch.Size([49, 50, 30]) torch.Size([49, 50, 30]) torch.Size([49, 50, 30]) torch.Size([49, 50, 30]) torch.Size([49, 50, 30])
+        '''
+        # Create lists for hidden states (cannot use single tensor as buffer because autograd won't work with inplace writes)
+        T = actions.size(0) + 1
+        beliefs = [torch.empty(0)] * T
+        prior_states = [torch.empty(0)] * T
+        prior_means = [torch.empty(0)] * T
+        prior_std_devs = [torch.empty(0)] * T
+        posterior_states = [torch.empty(0)] * T
+        posterior_means = [torch.empty(0)] * T
+        posterior_std_devs = [torch.empty(0)] * T
+
+        beliefs[0], prior_states[0], posterior_states[0] = prev_belief, prev_state, prev_state
+        # Loop over time sequence
+        for t in range(T - 1):
+            _state = prior_states[t] if observations is None else posterior_states[
+                t]  # Select appropriate previous state
+            _state = _state if nonterminals is None else _state * nonterminals[
+                t]  # Mask if previous transition was terminal
+            # Compute belief (deterministic hidden state)
+            hidden = self.act_fn(self.fc_embed_state_action(torch.cat([_state, actions[t]], dim=1)))
+            beliefs[t + 1] = self.rnn(hidden, beliefs[t])
+            # Compute state prior by applying transition dynamics
+            hidden = self.act_fn(self.fc_embed_belief_prior(beliefs[t + 1]))
+            prior_means[t + 1], _prior_std_dev = torch.chunk(self.fc_state_prior(hidden), 2, dim=1)
+            prior_std_devs[t + 1] = F.softplus(_prior_std_dev) + self.min_std_dev
+            prior_states[t + 1] = prior_means[t + 1] + prior_std_devs[t + 1] * torch.randn_like(prior_means[t + 1])
+            if observations is not None:
+                # Compute state posterior by applying transition dynamics and using current observation
+                t_ = t - 1  # Use t_ to deal with different time indexing for observations
+                hidden = self.act_fn(
+                    self.fc_embed_belief_posterior(torch.cat([beliefs[t + 1], observations[t_ + 1]], dim=1)))
+                posterior_means[t + 1], _posterior_std_dev = torch.chunk(self.fc_state_posterior(hidden), 2, dim=1)
+                posterior_std_devs[t + 1] = F.softplus(_posterior_std_dev) + self.min_std_dev
+                posterior_states[t + 1] = posterior_means[t + 1] + posterior_std_devs[t + 1] * torch.randn_like(
+                    posterior_means[t + 1])
+        # Return new hidden states
+        hidden = [torch.stack(beliefs[1:], dim=0), torch.stack(prior_states[1:], dim=0),
+                  torch.stack(prior_means[1:], dim=0), torch.stack(prior_std_devs[1:], dim=0)]
+        if observations is not None:
+            hidden += [torch.stack(posterior_states[1:], dim=0), torch.stack(posterior_means[1:], dim=0),
+                       torch.stack(posterior_std_devs[1:], dim=0)]
+        return hidden
+
+
+class SymbolicObservationModel(jit.ScriptModule):
+    def __init__(self, observation_size, belief_size, state_size, embedding_size, activation_function='relu'):
+        super().__init__()
+        self.act_fn = getattr(F, activation_function)
+        self.fc1 = nn.Linear(belief_size + state_size, embedding_size)
+        self.fc2 = nn.Linear(embedding_size, embedding_size)
+        self.fc3 = nn.Linear(embedding_size, observation_size)
+        self.modules = [self.fc1, self.fc2, self.fc3]
+
+    @jit.script_method
+    def forward(self, belief, state):
+        hidden = self.act_fn(self.fc1(torch.cat([belief, state], dim=1)))
+        hidden = self.act_fn(self.fc2(hidden))
+        observation = self.fc3(hidden)
+        return observation
+
+
+class VisualObservationModel(jit.ScriptModule):
+    __constants__ = ['embedding_size']
+
+    def __init__(self, belief_size, state_size, embedding_size, activation_function='relu'):
+        super().__init__()
+        self.act_fn = getattr(F, activation_function)
+        self.embedding_size = embedding_size
+        self.fc1 = nn.Linear(belief_size + state_size, embedding_size)
+        self.conv1 = nn.ConvTranspose2d(embedding_size, 128, 5, stride=2)
+        self.conv2 = nn.ConvTranspose2d(128, 64, 5, stride=2)
+        self.conv3 = nn.ConvTranspose2d(64, 32, 6, stride=2)
+        self.conv4 = nn.ConvTranspose2d(32, 3, 6, stride=2)
+        self.modules = [self.fc1, self.conv1, self.conv2, self.conv3, self.conv4]
+
+    @jit.script_method
+    def forward(self, belief, state):
+        hidden = self.fc1(torch.cat([belief, state], dim=1))  # No nonlinearity here
+        hidden = hidden.view(-1, self.embedding_size, 1, 1)
+        hidden = self.act_fn(self.conv1(hidden))
+        hidden = self.act_fn(self.conv2(hidden))
+        hidden = self.act_fn(self.conv3(hidden))
+        observation = self.conv4(hidden)
+        return observation
+
+
+def ObservationModel(symbolic, observation_size, belief_size, state_size, embedding_size, activation_function='relu'):
+    if symbolic:
+        return SymbolicObservationModel(observation_size, belief_size, state_size, embedding_size, activation_function)
+    else:
+        return VisualObservationModel(belief_size, state_size, embedding_size, activation_function)
+
+
+class RewardModel(jit.ScriptModule):
+    def __init__(self, belief_size, state_size, hidden_size, activation_function='relu'):
+        # [--belief-size: 200, --hidden-size: 200, --state-size: 30]
+        super().__init__()
+        self.act_fn = getattr(F, activation_function)
+        self.fc1 = nn.Linear(belief_size + state_size, hidden_size)
+        self.fc2 = nn.Linear(hidden_size, hidden_size)
+        self.fc3 = nn.Linear(hidden_size, 1)
+        self.modules = [self.fc1, self.fc2, self.fc3]
+
+    @jit.script_method
+    def forward(self, belief, state):
+        x = torch.cat([belief, state], dim=1)
+        hidden = self.act_fn(self.fc1(x))
+        hidden = self.act_fn(self.fc2(hidden))
+        reward = self.fc3(hidden).squeeze(dim=1)
+        return reward
+
+
+class CriticModel(jit.ScriptModule):
+    def __init__(self, belief_size, state_size, hidden_size, activation_function='relu'):
+        super().__init__()
+        self.act_fn = getattr(F, activation_function)
+        self.fc1 = nn.Linear(belief_size + state_size, hidden_size)
+        self.fc2 = nn.Linear(hidden_size, hidden_size)
+        self.fc3 = nn.Linear(hidden_size, hidden_size)
+        self.fc4 = nn.Linear(hidden_size, 1)
+        self.modules = [self.fc1, self.fc2, self.fc3, self.fc4]
+
+    @jit.script_method
+    def forward(self, belief, state):
+        x = torch.cat([belief, state], dim=1)
+        hidden = self.act_fn(self.fc1(x))
+        hidden = self.act_fn(self.fc2(hidden))
+        hidden = self.act_fn(self.fc3(hidden))
+        reward = self.fc4(hidden).squeeze(dim=1)
+        return reward
+
+
+class ActorModel(jit.ScriptModule):
+    def __init__(self, belief_size, state_size, hidden_size, action_size, dist='tanh_normal',
+                 activation_function='elu', min_std=1e-4, init_std=5, mean_scale=5):
+        super().__init__()
+        self.act_fn = getattr(F, activation_function)
+        self.fc1 = nn.Linear(belief_size + state_size, hidden_size)
+        self.fc2 = nn.Linear(hidden_size, hidden_size)
+        self.fc3 = nn.Linear(hidden_size, hidden_size)
+        self.fc4 = nn.Linear(hidden_size, hidden_size)
+        self.fc5 = nn.Linear(hidden_size, 2 * action_size)
+        self.modules = [self.fc1, self.fc2, self.fc3, self.fc4, self.fc5]
+
+        self._dist = dist
+        self._min_std = min_std
+        self._init_std = init_std
+        self._mean_scale = mean_scale
+
+    @jit.script_method
+    def forward(self, belief, state):
+        raw_init_std = torch.log(torch.exp(self._init_std) - 1)
+        x = torch.cat([belief, state], dim=1)
+        hidden = self.act_fn(self.fc1(x))
+        hidden = self.act_fn(self.fc2(hidden))
+        hidden = self.act_fn(self.fc3(hidden))
+        hidden = self.act_fn(self.fc4(hidden))
+        action = self.fc5(hidden).squeeze(dim=1)
+
+        action_mean, action_std_dev = torch.chunk(action, 2, dim=1)
+        action_mean = self._mean_scale * torch.tanh(action_mean / self._mean_scale)
+        action_std = F.softplus(action_std_dev + raw_init_std) + self._min_std
+        return action_mean, action_std
+
+    def get_action(self, belief, state, deterministic=False):
+        action_mean, action_std = self.forward(belief, state)
+        dist = Normal(action_mean, action_std)
+        dist = TransformedDistribution(dist, TanhBijector())
+        dist = torch.distributions.Independent(dist, 1)
+        dist = SampleDist(dist)
+        if deterministic:
+            return dist.mode()
+        else:
+            return dist.rsample()
+
+
+class SymbolicEncoder(jit.ScriptModule):
+    def __init__(self, observation_size, embedding_size, activation_function='relu'):
+        super().__init__()
+        self.act_fn = getattr(F, activation_function)
+        self.fc1 = nn.Linear(observation_size, embedding_size)
+        self.fc2 = nn.Linear(embedding_size, embedding_size)
+        self.fc3 = nn.Linear(embedding_size, embedding_size)
+        self.modules = [self.fc1, self.fc2, self.fc3]
+
+    @jit.script_method
+    def forward(self, observation):
+        hidden = self.act_fn(self.fc1(observation))
+        hidden = self.act_fn(self.fc2(hidden))
+        hidden = self.fc3(hidden)
+        return hidden
+
+
+class VisualEncoder(jit.ScriptModule):
+    __constants__ = ['embedding_size']
+
+    def __init__(self, embedding_size, activation_function='relu'):
+        super().__init__()
+        self.act_fn = getattr(F, activation_function)
+        self.embedding_size = embedding_size
+        self.conv1 = nn.Conv2d(3, 32, 4, stride=2)
+        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)
+        self.conv3 = nn.Conv2d(64, 128, 4, stride=2)
+        self.conv4 = nn.Conv2d(128, 256, 4, stride=2)
+        self.fc = nn.Identity() if embedding_size == 1024 else nn.Linear(1024, embedding_size)
+        self.modules = [self.conv1, self.conv2, self.conv3, self.conv4]
+
+    @jit.script_method
+    def forward(self, observation):
+        hidden = self.act_fn(self.conv1(observation))
+        hidden = self.act_fn(self.conv2(hidden))
+        hidden = self.act_fn(self.conv3(hidden))
+        hidden = self.act_fn(self.conv4(hidden))
+        hidden = hidden.view(-1, 1024)
+        hidden = self.fc(hidden)  # Identity if embedding size is 1024 else linear projection
+        return hidden
+
+
+def Encoder(symbolic, observation_size, embedding_size, activation_function='relu'):
+    if symbolic:
+        return SymbolicEncoder(observation_size, embedding_size, activation_function)
+    else:
+        return VisualEncoder(embedding_size, activation_function)
+
+
+# "atanh", "TanhBijector" and "SampleDist" are from the following repo
+# https://github.com/juliusfrost/dreamer-pytorch
+def atanh(x):
+    return 0.5 * torch.log((1 + x) / (1 - x))
+
+
+class TanhBijector(torch.distributions.Transform):
+    def __init__(self):
+        super().__init__()
+        self.bijective = True
+        self.domain = torch.distributions.constraints.real
+        self.codomain = torch.distributions.constraints.interval(-1.0, 1.0)
+
+    @property
+    def sign(self): return 1.
+
+    def _call(self, x): return torch.tanh(x)
+
+    def _inverse(self, y: torch.Tensor):
+        y = torch.where(
+            (torch.abs(y) <= 1.),
+            torch.clamp(y, -0.99999997, 0.99999997),
+            y)
+        y = atanh(y)
+        return y
+
+    def log_abs_det_jacobian(self, x, y):
+        return 2. * (np.log(2) - x - F.softplus(-2. * x))
+
+
+class SampleDist:
+    def __init__(self, dist, samples=100):
+        self._dist = dist
+        self._samples = samples
+
+    @property
+    def name(self):
+        return 'SampleDist'
+
+    def __getattr__(self, name):
+        return getattr(self._dist, name)
+
+    def mean(self):
+        sample = dist.rsample()
+        return torch.mean(sample, 0)
+
+    def mode(self):
+        dist = self._dist.expand((self._samples, *self._dist.batch_shape))
+        sample = dist.rsample()
+        logprob = dist.log_prob(sample)
+        batch_size = sample.size(1)
+        feature_size = sample.size(2)
+        indices = torch.argmax(logprob, dim=0).reshape(1, batch_size, 1).expand(1, batch_size, feature_size)
+        return torch.gather(sample, 0, indices).squeeze(0)
+
+    def entropy(self):
+        dist = self._dist.expand((self._samples, *self._dist.batch_shape))
+        sample = dist.rsample()
+        logprob = dist.log_prob(sample)
+        return -torch.mean(logprob, 0)
+
+    def sample(self):
+        return self._dist.sample()
diff --git a/planet.py b/planet.py
new file mode 100644
index 0000000..9812f9a
--- /dev/null
+++ b/planet.py
@@ -0,0 +1,335 @@
+import os
+
+import torch
+from torch import optim, nn
+from torch.distributions import Normal, kl_divergence
+from torch.nn import functional as F
+
+import utils
+from env import EnvBatcher
+from memory import ExperienceReplay
+from models import TransitionModel, ObservationModel, RewardModel, Encoder, bottle
+from planner import MPCPlanner
+from base_agent import BaseAgent
+
+
+class Planet(BaseAgent):
+    def __init__(self, params, env):
+        self.env = env
+        self.belief_size = params['belief_size']
+        self.state_size = params['state_size']
+        self.action_size = env.action_size
+        self.hidden_size = params['hidden_size']
+        self.embedding_size = params['embedding_size']
+        self.dense_activation_function = params['dense_activation_function']
+
+        self.symbolic_env = params['symbolic_env']
+        self.observation_size = env.observation_size
+        self.embedding_size = params['embedding_size']
+        self.cnn_activation_function = params['cnn_activation_function']
+
+        self.batch_size = params['batch_size']
+        self.chunk_size = params['chunk_size']
+
+        self.seed_episodes = params['seed_episodes']
+
+        self.overshooting_distance = params['overshooting_distance']
+
+        self.learning_rate_schedule = params['learning_rate_schedule']
+
+        self.worldmodel_LogProbLoss = params['worldmodel_LogProbLoss']
+
+        self.experience_size = params['experience_size']
+        self.bit_depth = params['bit_depth']
+
+
+        # for the MPC planner
+        self.planning_horizon = params['planning_horizon']
+        self.optimisation_iters = params['optimisation_iters']
+        self.candidates = params['candidates']
+        self.top_candidates = params['top_candidates']
+
+        self.model_learning_rate = params['model_learning_rate']
+        self.adam_epsilon = params['adam_epsilon']
+        self.global_kl_beta = params['global_kl_beta']
+        self.overshooting_kl_beta = params['overshooting_kl_beta']
+        self.overshooting_reward_scale = params['overshooting_reward_scale']
+        self.action_noise = params['action_noise']
+        self.grad_clip_norm = params['grad_clip_norm']
+        self.action_repeat = params['action_repeat']
+
+        self.replay_buffer = ExperienceReplay(self.experience_size, self.symbolic_env, env.observation_size,
+                                              env.action_size, self.bit_depth, utils.device)
+
+        # initialize the different models
+        self.initialize_models()
+
+        # optimization priors
+        # Global prior N(0, I)
+        self.global_prior = Normal(torch.zeros(params['batch_size'], params['state_size'], device=utils.device),
+                                   torch.ones(params['batch_size'], params['state_size'], device=utils.device))
+        # Allowed deviation in KL divergence
+        self.free_nats = torch.full((1,), params['free_nats'],
+                                    device=utils.device)  # Allowed deviation in KL divergence
+
+        # initialize the optimizers
+        self.model_optimizer, self.actor_optimizer, self.value_optimizer = None, None, None
+        self.initialize_optimizers()
+
+        # load the models if possible
+        if params['models'] is not '' and os.path.exists(params['models']):
+            model_dicts = torch.load(params['models'])
+            self.transition_model.load_state_dict(model_dicts['transition_model'])
+            self.observation_model.load_state_dict(model_dicts['observation_model'])
+            self.reward_model.load_state_dict(model_dicts['reward_model'])
+            self.encoder.load_state_dict(model_dicts['encoder'])
+            self.model_optimizer.load_state_dict(model_dicts['model_optimizer'])
+
+    def eval(self):
+        self.transition_model.eval()
+        self.observation_model.eval()
+        self.reward_model.eval()
+        self.encoder.eval()
+
+    def train(self):
+        self.transition_model.train()
+        self.observation_model.train()
+        self.reward_model.train()
+        self.encoder.train()
+
+    def randomly_initialize_replay_buffer(self) :
+        total_steps = 0
+        for s in range(1, self.seed_episodes + 1):
+            done = False
+            t = 0
+            observation = self.env.reset()
+            while not done:
+                action = self.env.sample_random_action()
+                next_observation, reward, done = self.env.step(action)
+                self.replay_buffer.append(observation, action, reward, done)
+                observation = next_observation
+                t += 1
+
+            total_steps += t * self.action_repeat
+        return total_steps, s
+
+    def initialize_models(self):
+        self.transition_model = TransitionModel(self.belief_size,
+                                                self.state_size,
+                                                self.action_size,
+                                                self.hidden_size,
+                                                self.embedding_size,
+                                                self.dense_activation_function).to(device=utils.device)
+
+        self.observation_model = ObservationModel(self.symbolic_env,
+                                                  self.observation_size,
+                                                  self.belief_size,
+                                                  self.state_size,
+                                                  self.embedding_size,
+                                                  self.cnn_activation_function).to(device=utils.device)
+
+        self.reward_model = RewardModel(self.belief_size,
+                                        self.state_size,
+                                        self.hidden_size,
+                                        self.dense_activation_function).to(device=utils.device)
+
+        self.encoder = Encoder(self.symbolic_env,
+                               self.observation_size,
+                               self.embedding_size,
+                               self.cnn_activation_function).to(device=utils.device)
+
+        self.planner = MPCPlanner(self.action_size,
+                                  self.planning_horizon,
+                                  self.optimisation_iters,
+                                  self.candidates,
+                                  self.top_candidates,
+                                  self.transition_model,
+                                  self.reward_model)
+
+    def initialize_optimizers(self):
+        self.model_modules = self.transition_model.modules + \
+                             self.observation_model.modules + \
+                             self.reward_model.modules + \
+                             self.encoder.modules
+
+        self.model_params = list(self.transition_model.parameters()) + \
+                            list(self.observation_model.parameters()) + \
+                            list(self.reward_model.parameters()) + \
+                            list(self.encoder.parameters())
+        self.model_optimizer = optim.Adam(self.model_params, lr=self.model_learning_rate, eps=self.adam_epsilon)
+
+    def compute_observation_loss(self, beliefs, posterior_states, observations):
+        if self.worldmodel_LogProbLoss:
+            observation_dist = Normal(bottle(self.observation_model, (beliefs, posterior_states)), 1)
+            observation_loss = -observation_dist.log_prob(observations[1:]).sum(
+                dim=2 if self.symbolic_env else (2, 3, 4)).mean(dim=(0, 1))
+        else:
+            observation_loss = F.mse_loss(bottle(self.observation_model, (beliefs, posterior_states)), observations[1:],
+                                          reduction='none').sum(dim=2 if self.symbolic_env else (2, 3, 4)).mean(
+                dim=(0, 1))
+            # print(observation_loss)
+        return observation_loss
+
+    def compute_reward_loss(self, beliefs, posterior_states, rewards):
+        if self.worldmodel_LogProbLoss:
+            reward_dist = Normal(bottle(self.reward_model, (beliefs, posterior_states)), 1)
+            reward_loss = -reward_dist.log_prob(rewards[:-1]).mean(dim=(0, 1))
+        else:
+            reward_loss = F.mse_loss(bottle(self.reward_model, (beliefs, posterior_states)), rewards[:-1],
+                                     reduction='none').mean(dim=(0, 1))
+        return reward_loss
+
+    def compute_kl_loss(self, posterior_means, posterior_std_devs, prior_means, prior_std_devs):
+        div = kl_divergence(Normal(posterior_means, posterior_std_devs),
+                            Normal(prior_means, prior_std_devs)).sum(dim=2)
+        # Note that normalisation by overshooting distance and weighting by overshooting distance cancel out
+        kl_loss = torch.max(div, self.free_nats).mean(dim=(0, 1))
+        if self.global_kl_beta != 0:
+            kl_loss += self.global_kl_beta * kl_divergence(Normal(posterior_means, posterior_std_devs),
+                                                           self.global_prior).sum(dim=2).mean(dim=(0, 1))
+        return kl_loss
+
+    def compute_overshooting_losses(self, kl_loss, reward_loss, actions, nonterminals, rewards, beliefs, prior_states,
+                                    posterior_means, posterior_std_devs):
+        overshooting_vars = []  # Collect variables for overshooting to process in batch
+        for t in range(1, self.chunk_size - 1):
+            d = min(t + self.overshooting_distance, self.chunk_size - 1)  # Overshooting distance
+            t_, d_ = t - 1, d - 1  # Use t_ and d_ to deal with different time indexing for latent states
+            # Calculate sequence padding so overshooting terms can be calculated in one batch
+            seq_pad = (0, 0, 0, 0, 0, t - d + self.overshooting_distance)
+            # Store
+            # (0) actions,
+            # (1) nonterminals,
+            # (2) rewards,
+            # (3) beliefs,
+            # (4) prior states,
+            # (5) posterior means,
+            # (6) posterior standard deviations and
+            # (7) sequence masks
+            # Posterior standard deviations must be padded with > 0 to prevent infinite KL divergences
+            overshooting_vars.append((F.pad(actions[t:d], seq_pad),
+                                      F.pad(nonterminals[t:d], seq_pad),
+                                      F.pad(rewards[t:d], seq_pad[2:]), beliefs[t_], prior_states[t_],
+                                      F.pad(posterior_means[t_ + 1:d_ + 1].detach(), seq_pad),
+                                      F.pad(posterior_std_devs[t_ + 1:d_ + 1].detach(), seq_pad, value=1),
+                                      F.pad(torch.ones(d - t, self.batch_size, self.state_size,
+                                                       device=utils.device), seq_pad)))
+        overshooting_vars = tuple(zip(*overshooting_vars))
+
+        # Update belief/state using prior from previous belief/state
+        # and previous action (over entire sequence at once)
+        beliefs, prior_states, prior_means, prior_std_devs = self.transition_model(
+            torch.cat(overshooting_vars[4], dim=0),
+            torch.cat(overshooting_vars[0], dim=1),
+            torch.cat(overshooting_vars[3], dim=0),
+            None,
+            torch.cat(overshooting_vars[1], dim=1))
+        seq_mask = torch.cat(overshooting_vars[7], dim=1)
+
+        # Calculate overshooting KL loss with sequence mask
+        # Update KL loss (compensating for extra average over each overshooting/open loop sequence)
+        p = Normal(torch.cat(overshooting_vars[5], dim=1), torch.cat(overshooting_vars[6], dim=1))
+        q = Normal(prior_means, prior_std_devs)
+        kl_loss = (1 / self.overshooting_distance) * \
+                  self.overshooting_kl_beta * \
+                  torch.max((kl_divergence(p, q) * seq_mask).sum(dim=2), self.free_nats).mean(dim=(0, 1)) * (
+                          self.chunk_size - 1)
+        # Calculate overshooting reward prediction loss with sequence mask
+        # Update reward loss (compensating for extra average over each overshooting/open loop sequence)
+        if self.overshooting_reward_scale != 0:
+            reward_overshoothing_weight = (1 / self.overshooting_distance) * self.overshooting_reward_scale
+            reward_loss = reward_overshoothing_weight * F.mse_loss(
+                bottle(self.reward_model, (beliefs, prior_states)) * seq_mask[:, :, 0],
+                torch.cat(overshooting_vars[2], dim=1), reduction='none').mean(dim=(0, 1)) * (
+                                  self.chunk_size - 1)
+        return (kl_loss, reward_loss), (beliefs, prior_states, prior_means, prior_std_devs)
+
+    def train_step(self) -> dict:
+        log = {}
+        ####################
+        # DYNAMICS LEARNING
+        ####################
+
+        # 1) Draw Sequences
+
+        # Draw sequence chunks {(o_t, a_t, r_t+1, terminal_t+1)} ~ D uniformly
+        # at random from the dataset (including terminal flags)
+        # Transitions start at time t = 0
+        observations, actions, rewards, nonterminals = self.replay_buffer.sample(self.batch_size, self.chunk_size)
+
+        # 2) Compute model States
+
+        # Create initial belief and state for time t = 0
+        init_belief = torch.zeros(self.batch_size, self.belief_size, device=utils.device)
+        init_state = torch.zeros(self.batch_size, self.state_size, device=utils.device)
+
+        # Update belief/state using posterior from previous belief/state, previous action and current observation
+        # (over entire sequence at once)
+        beliefs, prior_states, prior_means, prior_std_devs, posterior_states, posterior_means, posterior_std_devs = self.transition_model(
+            init_state, actions[:-1], init_belief, bottle(self.encoder, (observations[1:],)), nonterminals[:-1])
+
+        # 3) Update model weights
+
+        # Calculate observation likelihood, reward likelihood and KL losses (for t = 0 only for latent overshooting);
+        # sum over final dims, average over batch and time
+        # (original implementation, though paper seems to miss 1/T scaling?)
+        observation_loss = self.compute_observation_loss(beliefs, posterior_states, observations)
+        log['observation_loss'] = observation_loss.item()
+
+        reward_loss = self.compute_reward_loss(beliefs, posterior_states, rewards)
+        log['reward_loss'] = reward_loss.item()
+
+        # transition loss
+        kl_loss = self.compute_kl_loss(posterior_means, posterior_std_devs, prior_means, prior_std_devs)
+        log['kl_loss'] = kl_loss.item()
+
+        # Calculate latent overshooting objective for t > 0
+        if self.overshooting_kl_beta != 0:
+            (kl_over, reward_over), state_updates = self.compute_overshooting_losses(actions, nonterminals, rewards, beliefs,
+                                                                               prior_states, posterior_means,
+                                                                               posterior_std_devs)
+            kl_loss += kl_over
+            reward_loss += reward_over
+            beliefs, prior_states, prior_means, prior_std_devs = state_updates
+
+            # log overshoot losses
+            log['kl_loss_overshoot'] = kl_over.item()
+            log['reward_loss_overshoot'] = reward_over.item()
+
+        model_loss = observation_loss + reward_loss + kl_loss
+        log['model_loss'] = model_loss.item()
+
+        # TODO : add a learning rate schedule
+        # Update model parameters
+        self.model_optimizer.zero_grad()
+        model_loss.backward()
+        nn.utils.clip_grad_norm_(self.model_params, self.grad_clip_norm, norm_type=2)
+        self.model_optimizer.step()
+
+        # for dreamer, ugly for the moment
+        self.posterior_states = posterior_states
+        self.beliefs = beliefs
+
+        return log
+
+    def update_belief_and_act(self, env, belief, posterior_state, action, observation, explore=False):
+        # Infer belief over current state q(s_t|ot,a<t) from the history
+        # print("action size: ",action.size()) torch.Size([1, 6])
+        # Action and observation need extra time dimension
+        belief, _, _, _, posterior_state, _, _ = self.transition_model(posterior_state,
+                                                                       action.unsqueeze(dim=0),
+                                                                       belief,
+                                                                       self.encoder(observation).unsqueeze(dim=0))
+        # Remove time dimension from belief/state
+        belief, posterior_state = belief.squeeze(dim=0), posterior_state.squeeze(dim=0)
+        # Get action from planner(q(s_t|ot,a<t), p)
+        action = self.planner.get_action(belief, posterior_state, deterministic=not explore)
+        if explore:
+            # Add gaussian exploration noise on top of the sampled action
+            action = torch.clamp(Normal(action, self.action_noise).rsample(), -1, 1)
+            # action = action + self.action_noise * torch.randn_like(action)  # Add exploration noise  ~ p() to the action
+        # Perform environment step (action repeats handled internally)
+        next_observation, reward, done = env.step(action.cpu() if isinstance(env, EnvBatcher) else action[0].cpu())
+
+        # self.replay_buffer.append(observation, action, reward, done)
+        return belief, posterior_state, action, next_observation, reward, done
diff --git a/planner.py b/planner.py
new file mode 100644
index 0000000..fdbd418
--- /dev/null
+++ b/planner.py
@@ -0,0 +1,54 @@
+import torch
+from torch import jit
+
+
+# Model-predictive control planner with cross-entropy method and learned transition model
+class MPCPlanner(jit.ScriptModule):
+    __constants__ = ['action_size', 'planning_horizon', 'optimisation_iters', 'candidates', 'top_candidates']
+
+    def __init__(self, action_size, planning_horizon, optimisation_iters, candidates, top_candidates, transition_model,
+                 reward_model):
+        super().__init__()
+        self.transition_model, self.reward_model = transition_model, reward_model
+        self.action_size = action_size
+        self.planning_horizon = planning_horizon
+        self.optimisation_iters = optimisation_iters
+        self.candidates, self.top_candidates = candidates, top_candidates
+
+    @jit.script_method
+    def forward(self, belief, state):
+        B, H, Z = belief.size(0), belief.size(1), state.size(1)
+        belief = belief.unsqueeze(dim=1).expand(B, self.candidates, H).reshape(-1, H)
+        state = state.unsqueeze(dim=1).expand(B, self.candidates, Z).reshape(-1, Z)
+
+        # Initialize factorized belief over action sequences q(a_t:t+H) ~ N(0, I)
+        action_mean = torch.zeros(self.planning_horizon, B, 1, self.action_size, device=belief.device)
+        action_std_dev = torch.ones(self.planning_horizon, B, 1, self.action_size, device=belief.device)
+
+        for _ in range(self.optimisation_iters):
+            # print("optimization_iters",_)
+            # Evaluate J action sequences from the current belief (over entire sequence at once, batched over particles)
+            # Sample actions (time x (batch x candidates) x actions)
+            noise = torch.randn(self.planning_horizon, B, self.candidates, self.action_size, device=action_mean.device)
+            actions = (action_mean + action_std_dev * noise).view(self.planning_horizon, B * self.candidates, self.action_size)
+            # Sample next states
+            # [12, 1000, 200] [12, 1000, 30] : 12 horizon steps; 1000 candidates
+            beliefs, states, _, _ = self.transition_model(state, actions, belief)
+            # Calculate expected returns (technically sum of rewards over planning horizon)
+            # output from r-model[12000]->view[12, 1000]->sum[1000]
+            returns = self.reward_model(beliefs.view(-1, H), states.view(-1, Z)).view(self.planning_horizon, -1).sum(dim=0)
+            # Re-fit belief to the K best action sequences
+            _, topk = returns.reshape(B, self.candidates).topk(self.top_candidates, dim=1, largest=True, sorted=False)
+            # Fix indices for unrolled actions
+            topk += self.candidates * torch.arange(0, B, dtype=torch.int64, device=topk.device).unsqueeze(dim=1)
+            best_actions = actions[:, topk.view(-1)].reshape(self.planning_horizon, B, self.top_candidates, self.action_size)
+
+            # Update belief with new means and standard deviations
+            action_mean = best_actions.mean(dim=2, keepdim=True)
+            action_std_dev = best_actions.std(dim=2, unbiased=False, keepdim=True)
+
+        # Return first action mean _t
+        return action_mean[0].squeeze(dim=1)
+
+    def get_action(self, belief, posterior_state, deterministic=False):
+        return self.forward(belief, posterior_state)
diff --git a/requirements.txt b/requirements.txt
deleted file mode 100644
index 7e7d46f..0000000
--- a/requirements.txt
+++ /dev/null
@@ -1,28 +0,0 @@
-numpy
-gym[atari]==0.17.2
-mujoco-py==2.0.2.2
-pybullet==3.2.0
-tensorboard==2.3.0
-tensorboardX==1.8
-matplotlib==2.2.2
-ipython==6.4.0
-moviepy==1.0.0
-pyvirtualdisplay==1.3.2
-torch==1.11.0 --extra-index-url https://download.pytorch.org/whl/cu113
-opencv-python==4.4.0.42
-ipdb==0.13.3
-box2d-py
-hydra
-hydra-core
-junit2html
-pytest-parallel
-pytest-timeout
-pytest-xdist
-atari_py==0.2.6
-torchvision
-plotly
-wandb
-pre-commit
-pylint
-dm-control
-torchtyping
diff --git a/run_exp_cedar.sh b/run_exp_cedar.sh
deleted file mode 100644
index 47cdda7..0000000
--- a/run_exp_cedar.sh
+++ /dev/null
@@ -1,57 +0,0 @@
-#!/bin/bash
-#SBATCH --job-name=big-dreamer
-#SBATCH --account=rrg-bengioy-ad
-#SBATCH --mail-type=ALL
-#SBATCH --mail-user=olivier.ethier@umontreal.ca
-#SBATCH --time=10:00:00
-#SBATCH --gres=gpu:1
-#SBATCH --cpus-per-task=10
-#SBATCH --mem=32Gb
-#SBATCH --output=logs_slurm/slurm-%j-%x.log
-
-# You need to launch this script from the folder big-dreamer
-# and have the environment in the grand-parent folder
-
-# Arguments
-# $1: algorithm: planet, dreamer or dreamerV2
-# $2: environement to run on: HalfCheetah-v2,
-
-date
-echo "Algorithm:" ${1:-planet}
-echo "Env:" ${2:-HalfCheetah-v2}
-
-# Setup environment
-module load python/3.7
-source ../../dreamer_env/bin/activate
-export LD_LIBRARY_PATH=~/.mujoco/mujoco200/bin
-
-# copying code to tmp dir
-rsync -av src $TMPDIR/
-cd $TMPDIR
-
-python src/main.py disable_cuda=False \
-                    algorithm="${1:-planet}" \
-                    env="${2:-HalfCheetah-v2}" \
-                    action_repeat=2 \
-                    episodes=100 \
-                    collect_interval=50 \
-                    hidden_size=32 \
-                    belief_size=32 \
-                    test_interval=10 \
-                    log_video_freq=10
-
-# # Use this for testing the algo:
-
-# python src/main.py disable_cuda=False \
-#                     algorithm="${1:-planet}" \
-#                     env="${1:-HalfCheetah-v2}" \
-#                     action_repeat=2 \
-#                     episodes=1 \
-#                     collect_interval=2 \
-#                     hidden_size=32 \
-#                     belief_size=32 \
-#                     test_interval=3 \
-#                     log_video_freq=3
-
-# copying outputs to original folder (in scratch)
-rsync -av outputs/ ~/scratch/dreamer_oli/big-dreamer/outputs/slurm-$SLURM_JOB_ID/
\ No newline at end of file
diff --git a/run_exp_narval.sh b/run_exp_narval.sh
deleted file mode 100644
index 9da38c0..0000000
--- a/run_exp_narval.sh
+++ /dev/null
@@ -1,57 +0,0 @@
-#!/bin/bash
-#SBATCH --job-name=big-dreamer
-#SBATCH --account=rrg-bengioy-ad
-#SBATCH --mail-type=ALL
-#SBATCH --mail-user=olivier.ethier@umontreal.ca
-#SBATCH --time=10:00:00
-#SBATCH --gres=gpu:1
-#SBATCH --cpus-per-task=10
-#SBATCH --mem=32Gb
-#SBATCH --output=logs_slurm/slurm-%j-%x.log
-
-# You need to launch this script from the folder big-dreamer
-# and have the environment in the grand-parent folder
-
-# Arguments
-# $1: algorithm: planet, dreamer or dreamerV2
-# $2: environement to run on: HalfCheetah-v2,
-
-date
-echo "Algorithm:" ${1:-planet}
-echo "Env:" ${2:-HalfCheetah-v2}
-
-# Setup environment
-module load python/3.7
-source ../../bigdreamer_env/bin/activate
-export LD_LIBRARY_PATH=~/.mujoco/mujoco200/bin
-
-# copying code to tmp dir
-rsync -av src $TMPDIR/
-cd $TMPDIR
-
-python src/main.py disable_cuda=False \
-                    algorithm="${1:-planet}" \
-                    env="${2:-HalfCheetah-v2}" \
-                    action_repeat=2 \
-                    episodes=100 \
-                    collect_interval=50 \
-                    hidden_size=32 \
-                    belief_size=32 \
-                    test_interval=10 \
-                    log_video_freq=10
-
-# # Use this for testing the algo:
-
-# python src/main.py disable_cuda=False \
-#                     algorithm="${1:-planet}" \
-#                     env="${1:-HalfCheetah-v2}" \
-#                     action_repeat=2 \
-#                     episodes=1 \
-#                     collect_interval=2 \
-#                     hidden_size=32 \
-#                     belief_size=32 \
-#                     test_interval=3 \
-#                     log_video_freq=3
-
-# copying outputs to original folder (in scratch)
-rsync -av outputs/ ~/scratch/dreamer_oli/outputs/slurm-$SLURM_JOB_ID/
diff --git a/setup.py b/setup.py
deleted file mode 100644
index 7f908cf..0000000
--- a/setup.py
+++ /dev/null
@@ -1,7 +0,0 @@
-from setuptools import setup
-
-setup(
-    name="src",
-    version="0.1.0",
-    packages=["src"],
-)
diff --git a/src/base_agent.py b/src/base_agent.py
deleted file mode 100644
index 026ca7e..0000000
--- a/src/base_agent.py
+++ /dev/null
@@ -1,34 +0,0 @@
-from typing import Dict
-
-
-class BaseAgent:
-    """
-    Base class for agents.
-    """
-    def train(self) -> Dict[str, float]:
-        """
-        Return a dictionary of logging information.
-        """
-
-        raise NotImplementedError
-
-    def add_to_replay_buffer(self, paths):
-        """
-        Add a batch of paths to the replay buffer.
-        """
-
-        raise NotImplementedError
-
-    def sample(self, batch_size):
-        """
-        Sample a batch of paths (trajectories) from the replay buffer.
-        """
-
-        raise NotImplementedError
-
-    def save(self, path):
-        """
-        Save the agent to the given path.
-        """
-
-        raise NotImplementedError
diff --git a/src/conf/config.yaml b/src/conf/config.yaml
deleted file mode 100644
index 6e9cf26..0000000
--- a/src/conf/config.yaml
+++ /dev/null
@@ -1,81 +0,0 @@
-algorithm: 'dreamer'
-exp_name: 'default'
-seed: 0
-disable_cuda: false
-env: 'Pendulum-v0'
-max_episode_length: 1000
-experience_size: 1000000
-cnn_activation_function: 'ELU'
-dense_activation_function: 'ELU'
-embedding_size: 1024
-hidden_size: 200
-n_layers: 4
-belief_size: 200
-state_size: 30
-action_repeat: 2
-action_noise: 0.3
-episodes: 1000 # deprecated
-seed_episodes: 1 # deprecated
-seed_steps : 5000
-train_steps: 1000000
-collect_interval: 5
-batch_size: 50
-seq_len: 50
-free_nats: 3.0
-bit_depth: 5
-model_learning_rate: 2e-4
-
-adam_epsilon: 1e-5
-weight_decay: 1e-6
-grad_clip_norm: 100.0
-planning_horizon: 15
-discount: 0.995
-disclam: 0.95
-
-
-
-test: False
-test_interval: 25
-test_episodes: 10
-# checkpoint_interval: 50
-checkpoint_interval: 5
-checkpoint_experience: False
-models: ''
-experience_replay: ''
-render: False
-log_freq : 100
-log_video_freq : -1
-wandb_project : null # 'test-project'
-fps: 10
-
-# Dreamer V2
-
-kl_balance:  0.8
-kl_loss_weight: 0.1 # 0.1 for dreamerV2
-
-latent_distribution: "Gaussian" # so far Gaussian, Categorial
-discrete_latent_dimensions: 32
-discrete_latent_classes: 32
-action_distribution: "Gaussian" # so far Gaussian, Categorial
-
-jit: False
-
-MPC:
- optimisation_iters: 10
- candidates: 1000
- top_candidates: 100
-
-ActorCritic:
- actor_learning_rate: 4e-5
- value_learning_rate: 1e-4
- entropy_weight: 1e-5
- slow_critic_update_interval: 100
- polyak_avg: 1.0
- gradient_mixing: -1
-
-use_discount: False
-discount_weight: 5.0
-
-pixel_observation: True
-
-environment_steps_per_update: 10
diff --git a/src/dreamer.py b/src/dreamer.py
deleted file mode 100644
index c252aa2..0000000
--- a/src/dreamer.py
+++ /dev/null
@@ -1,471 +0,0 @@
-from typing import Tuple, Dict, Any
-import copy
-
-import torch
-from torch import optim, nn, Tensor
-from torch.distributions import Normal, kl_divergence, OneHotCategoricalStraightThrough
-import torch.distributions as D
-from torch.distributions.transformed_distribution import TransformedDistribution
-
-from torchtyping import TensorType, patch_typeguard
-from typeguard import typechecked
-
-from models import ActorModel, TanhBijector, SampleDist, DenseModel
-from planet import Planet
-from utils import FreezeParameters, device, cat, prefill, stack, polyak_update
-
-
-patch_typeguard()
-
-
-class Dreamer(Planet):
-    """
-    Dreamer is the evolution of Planet. The model predictive controller is replaced by
-    an actor-critic to select the next action.
-    """
-
-    def __init__(self, params: Dict[str, Any], env):
-        super().__init__(params, env)
-
-        self.actor = ActorModel(
-            self.belief_size,
-            self.state_size,
-            self.hidden_size,
-            self.action_size,
-            self.dense_activation_function,
-            params['action_distribution']
-        ).to(device)
-
-        self.critic = DenseModel(
-            self.belief_size + self.state_size,
-            self.hidden_size,
-            activation=self.dense_activation_function,
-        ).to(device)
-
-        self.critic_target = DenseModel(
-            self.belief_size + self.state_size,
-            self.hidden_size,
-            activation=self.dense_activation_function,
-        ).to(device)
-        self.critic_target = copy.deepcopy(self.critic)
-
-        if params['jit']:
-            self.actor = torch.jit.script(self.actor)
-            self.critic = torch.jit.script(self.critic)
-
-        self.actor_optimizer = optim.Adam(
-            self.actor.parameters(),
-            lr=params['ActorCritic']["actor_learning_rate"],
-            eps=params["adam_epsilon"],
-            weight_decay=params['weight_decay']
-        )
-        self.value_optimizer = optim.Adam(
-            self.critic.parameters(),
-            lr=params['ActorCritic']["value_learning_rate"],
-            eps=params["adam_epsilon"],
-            weight_decay=params['weight_decay']
-        )
-
-        self.entropy_weight = params['ActorCritic']["entropy_weight"]
-        self.gradient_mixing = params['ActorCritic']["gradient_mixing"]
-        self.polyak_avg = params['ActorCritic']["polyak_avg"]
-        self.discount = params["discount"]
-        self.disclam = params["disclam"]
-
-        self.kl_balance = params['kl_balance']
-
-        self.use_discount = params["use_discount"]
-        self.discount_weight = params['discount_weight']
-
-        if self.use_discount:
-            self.discount_model = DenseModel(
-                self.belief_size + self.state_size,
-                self.hidden_size,
-                activation=self.dense_activation_function,
-            ).to(device)
-        else:
-            # just to shut up pylint
-            self.discount_model = nn.Identity()
-
-        self._initialize_optimizers()
-
-    def _get_dist(
-            self,
-            distribution_parameters: Tuple[Tensor, ...],
-            detach: bool=False
-    ) -> torch.distributions.Distribution:
-        if self.latent_distribution == "Gaussian":
-            means, std_devs = distribution_parameters
-            if detach:
-                means, std_devs = means.detach(), std_devs.detach()
-            return Normal(means, std_devs)
-        if self.latent_distribution == "Categorical":
-            logits, = distribution_parameters
-            if detach:
-                logits = logits.detach()
-            return OneHotCategoricalStraightThrough(logits=logits)
-        # if it gets here, there is a problem
-        raise NotImplementedError(f'{self.latent_distribution}  is yet yet implemented')
-
-    def _kl_loss(
-            self,
-            posterior_params: Tuple[Tensor, ...],
-            prior_params: Tuple[Tensor, ...],
-    ) -> Tensor:
-
-        """
-        Compute the KL loss.
-        """
-        prior_dist = self._get_dist(prior_params)
-        posterior_dist = self._get_dist(posterior_params)
-
-        if self.kl_balance == -1:
-            div = kl_divergence(posterior_dist, prior_dist).sum(dim=2)
-
-            # this is the free bits optimization presented in
-            # Improved Variational Inference with Inverse Autoregressive Flow : C.8.
-            # https://arxiv.org/abs/1606.04934
-            kl_loss = torch.max(div, self.free_nats).mean(dim=(0, 1))
-
-        else:
-            prior_dist_detach = self._get_dist(prior_params, detach=True)
-            posterior_dist_detach = self._get_dist(posterior_params, detach=True)
-
-            kl_lhs = kl_divergence(posterior_dist_detach, prior_dist).mean()
-            kl_rhs = kl_divergence(posterior_dist, prior_dist_detach).mean()
-
-            # this is the free bits optimization presented in
-            # Improved Variational Inference with Inverse Autoregressive Flow : C.8.
-            # https://arxiv.org/abs/1606.04934
-            kl_lhs = torch.max(kl_lhs, self.free_nats)
-            kl_rhs = torch.max(kl_rhs, self.free_nats)
-
-            # balance
-            kl_loss = self.kl_balance * kl_lhs + (1 - self.kl_balance) * kl_rhs
-
-        return kl_loss
-
-    def _initialize_optimizers(self) -> None:
-        """
-        Initialize the optimizers.
-        """
-
-        self.model_modules = [
-            self.transition_model,
-            self.observation_model,
-            self.reward_model,
-            self.encoder,
-        ]
-
-        self.model_params = (
-            list(self.transition_model.parameters())
-            + list(self.observation_model.parameters())
-            + list(self.reward_model.parameters())
-            + list(self.encoder.parameters())
-        )
-
-        if self.use_discount:
-            self.model_modules += [self.discount_model]
-            self.model_params += list(self.discount_model.parameters())
-
-        self.model_optimizer = optim.Adam(
-            self.model_params,
-            lr=self.model_learning_rate,
-            eps=self.adam_epsilon,
-            weight_decay=self.weight_decay
-        )
-
-    @typechecked
-    def imagine_ahead(
-            self,
-            prev_state: TensorType['seq_len', 'batch_size', 'state_size'],
-            prev_belief: TensorType['seq_len', 'batch_size', 'belief_size']
-    ) -> Tuple[Tensor, Tensor, Tuple[Tensor, ...], Tensor]:
-        """
-        imagine_ahead is the function to draw the imaginary trajectory using the
-        dynamics model, actor, critic.
-
-        :param prev_state:
-        :param prev_belief:
-        :return: generated trajectory of features includes beliefs, prior_states, prior_means,
-                prior_std_devs
-        """
-        flatten = lambda x: x.view([-1] + list(x.size()[2:]))
-        prev_belief = flatten(prev_belief)
-        prev_state = flatten(prev_state)
-
-        # Create lists for hidden states
-        beliefs = prefill(self.planning_horizon)
-        prior_states = prefill(self.planning_horizon)
-        action_entropy = prefill(self.planning_horizon)
-
-        if self.latent_distribution == "Gaussian":
-            prior_means = prefill(self.planning_horizon)
-            prior_std_devs = prefill(self.planning_horizon)
-        elif self.latent_distribution == "Categorical":
-            prior_logits = prefill(self.planning_horizon)
-
-        beliefs[0] = prev_belief
-        prior_states[0] = prev_state
-        action_entropy[0] = torch.zeros(len(beliefs[0]), device=device)
-
-        # Loop over time sequence
-        for t in range(self.planning_horizon - 1):
-            _state = prior_states[t]
-            actions, action_entropy[t+1] = self.get_action(beliefs[t].detach(), _state.detach())
-
-
-            # Compute belief (deterministic hidden state)
-            hidden = self.transition_model.fc_embed_state_action(cat(_state, actions))
-            beliefs[t + 1] = self.transition_model.rnn(hidden, beliefs[t])
-
-            # Compute state prior by applying transition dynamics
-            prior_states[t + 1], prior_params_ = self.transition_model.belief_prior(beliefs[t + 1])
-            if self.latent_distribution == "Gaussian":
-                prior_means[t + 1], prior_std_devs[t + 1] = prior_params_
-            elif self.latent_distribution == "Categorical":
-                prior_logits[t + 1] = prior_params_
-
-        beliefs = stack(beliefs)
-        prior_states = stack(prior_states)
-        action_entropy = stack(action_entropy)
-        if self.latent_distribution == "Gaussian":
-            prior_params = (stack(prior_means), stack(prior_std_devs))
-        elif self.latent_distribution == "Categorical":
-            prior_params = (stack(prior_logits),)
-
-        return beliefs, prior_states, prior_params, action_entropy
-
-    def _discount_loss(
-            self,
-            beliefs: TensorType['seq_len', 'batch_size', 'belief_size'],
-            posterior_states: TensorType['seq_len', 'batch_size', 'state_size'],
-            nonterminals: TensorType['seq_len', 'batch_size', 1]
-    ) -> Tensor:
-        discount_target = nonterminals.float()
-        discount_logits = self.discount_model(beliefs, posterior_states)
-        discount_dist = D.Independent(D.Bernoulli(logits=discount_logits), 1)
-
-        discount_loss = -torch.mean(discount_dist.log_prob(discount_target))
-
-        return discount_loss
-
-    def train_step(self) -> Dict[str, float]:
-        """
-        Used to train the model.
-        """
-        logs = {}
-
-        ####################
-        # DYNAMICS LEARNING
-        ####################
-        # Draw sequences chunks
-        obs, actions, rewards, nonterminals = self.buffer.sample(self.batch_size, self.seq_len)
-
-        # Create initial belief and state for time t = 0
-        init_belief = torch.zeros(self.batch_size, self.belief_size, device=device)
-        init_state = torch.zeros(self.batch_size, self.state_size, device=device)
-
-        # compute image embeddings
-        embeddings = self.encoder(obs[1:])
-
-        beliefs, _, prior_params, posterior_states, posterior_params = self.transition_model(
-            init_state,
-            actions[:-1],
-            init_belief,
-            embeddings,
-            nonterminals[:-1],
-        )
-        # sum over final dims, average over batch and time
-
-        # compute losses
-        observation_loss = self._observation_loss(beliefs, posterior_states, obs[1:])
-        reward_loss = self._reward_loss(beliefs, posterior_states, rewards[:-1])
-        kl_loss = self._kl_loss(posterior_params, prior_params)
-        model_loss = observation_loss + reward_loss + kl_loss * self.kl_loss_weight
-
-        if self.use_discount:
-            discount_loss = self._discount_loss(beliefs, posterior_states, nonterminals[:-1])
-            model_loss += discount_loss * self.discount_weight
-            logs['discount_loss'] = discount_loss
-
-        # log stuff
-        logs["observation_loss"] = observation_loss.item()
-        logs["reward_loss"] = reward_loss.item()
-        logs["kl_loss"] = kl_loss.item()
-        logs["model_loss"] = model_loss.item()
-
-        # Update model parameters
-        self.model_optimizer.zero_grad()
-        model_loss.backward()
-        nn.utils.clip_grad_norm_(self.model_params, self.grad_clip_norm, norm_type=2)
-        self.model_optimizer.step()
-
-        ####################
-        # BEHAVIOUR LEARNING
-        ####################
-
-        # Imagine trajectories
-        with torch.no_grad():
-            actor_states = posterior_states.detach()
-            actor_beliefs = beliefs.detach()
-
-        with FreezeParameters(self.model_modules):
-            imged_belief, imged_prior_state, _, action_entropy = self.imagine_ahead(
-                actor_states,
-                actor_beliefs
-            )
-
-        # Predict Rewards & Values
-        with FreezeParameters(self.model_modules + [self.critic] + [self.critic_target]):
-            imged_reward = self.reward_model(imged_belief, imged_prior_state)
-            value_pred = self.critic_target(imged_belief, imged_prior_state)
-            if self.use_discount:
-                discount_logits = self.discount_model(imged_belief, imged_prior_state)
-                discount_dist = D.Independent(D.Bernoulli(logits=discount_logits), 1)
-                discount_arr = self.discount * torch.round(discount_dist.base_dist.probs)
-
-        # Compute Values estimates
-        returns = lambda_return(
-            imged_reward,
-            value_pred,
-            bootstrap=value_pred[-1],
-            discount=self.discount,
-            lambda_=self.disclam,
-        )
-        if self.gradient_mixing == -1:
-            objective = returns
-        else:
-            raise NotImplementedError("gradient_mixing not yet implemented ")
-
-        # Update Actor weights
-        policy_entropy = action_entropy.unsqueeze(-1)
-
-        if self.entropy_weight != -1:
-            # print(objective.shape, policy_entropy.shape)
-            objective = objective + self.entropy_weight * policy_entropy
-        if self.use_discount:
-            # discount_arr = torch.cat([torch.ones_like(discount_arr[:, :1]),
-            # discount_arr[:, 1:]], dim=1)
-            discount_arr[:, 0, 0] = 1.0  # at least the first one of each trajectory =1
-            discount = torch.cumprod(discount_arr, 0)
-            objective = discount * objective
-        actor_loss = -objective.mean()
-        # actor_loss = - objective.mean(dim=1).sum()
-
-
-        # actor_loss = -torch.mean(returns)
-        # logs["actor_returns"] = returns.item()
-        logs["actor_loss"] = actor_loss.item()
-        logs["policy_entropy"] = torch.mean(policy_entropy).item()
-
-        self.actor_optimizer.zero_grad()
-        actor_loss.backward()
-        nn.utils.clip_grad_norm_(
-            self.actor.parameters(), self.grad_clip_norm, norm_type=2
-        )
-        self.actor_optimizer.step()
-
-        # detach the input tensor from the transition network
-        with torch.no_grad():
-            value_beliefs = imged_belief.detach()
-            value_prior_states = imged_prior_state.detach()
-            target_return = returns.detach()
-            if self.use_discount:
-                value_discount = discount.detach()
-
-        value_dist = Normal(self.critic(value_beliefs, value_prior_states), 1)
-
-        if self.use_discount:
-            value_loss = -(value_discount * value_dist.log_prob(target_return)).mean()
-        else:
-            value_loss = -value_dist.log_prob(target_return).mean()
-        logs["value_loss"] = value_loss.item()
-
-        # Update model parameters
-        self.value_optimizer.zero_grad()
-        value_loss.backward()
-        nn.utils.clip_grad_norm_(
-            self.critic.parameters(), self.grad_clip_norm, norm_type=2
-        )
-        self.value_optimizer.step()
-
-        return logs
-
-    def eval(self) -> None:
-        """
-        Set the models to evaluation mode.
-        """
-
-        self.transition_model.eval()
-        self.observation_model.eval()
-        self.reward_model.eval()
-        self.encoder.eval()
-        self.actor.eval()
-        self.critic.eval()
-        if self.use_discount:
-            self.discount_model.eval()
-
-    def train(self) -> None:
-        """
-        Set the models to training mode.
-        """
-
-        self.transition_model.train()
-        self.observation_model.train()
-        self.reward_model.train()
-        self.encoder.train()
-        self.actor.train()
-        self.critic.train()
-        if self.use_discount:
-            self.discount_model.train()
-
-    def update_critic(self) -> None:
-        """
-        Update the critic weights using polyack averaging (soft update : see DDPG)
-        """
-        polyak_update(self.critic_target, self.critic, self.polyak_avg)
-
-    def get_action(self, belief: Tensor, state: Tensor, deterministic: bool = False) -> Tensor:
-        """
-        Get action.
-        """
-
-        action_mean, action_std = self.actor(belief, state)
-        dist = D.Normal(action_mean, action_std)
-        dist = TransformedDistribution(dist, TanhBijector())  # clip [-1,1]
-        dist = D.Independent(dist, 1)
-        dist = SampleDist(dist)
-
-        if deterministic:
-            sample = dist.mode()
-        else:
-            sample = dist.rsample()
-        return sample, dist.entropy()
-
-
-def lambda_return(imged_reward, value_pred, bootstrap, discount=0.99, lambda_=0.95):
-    """
-    Compute the lambda-return for a given trajectory.
-
-    Setting lambda=1 gives a discounted Monte Carlo return.
-    Setting lambda=0 gives a fixed 1-step return.
-    """
-
-    next_values = torch.cat([value_pred[1:], bootstrap[None]], 0)
-    discount_tensor = discount * torch.ones_like(imged_reward)  # pcont
-    inputs = imged_reward + discount_tensor * next_values * (1 - lambda_)
-    last = bootstrap
-    indices = reversed(range(len(inputs)))
-    outputs = []
-
-    for index in indices:
-        inp, disc = inputs[index], discount_tensor[index]
-        last = inp + disc * lambda_ * last
-        outputs.append(last)
-
-    outputs = list(reversed(outputs))
-    outputs = torch.stack(outputs, 0)
-    returns = outputs
-
-    return returns
diff --git a/src/dreamerV2.py b/src/dreamerV2.py
deleted file mode 100644
index 5e7951a..0000000
--- a/src/dreamerV2.py
+++ /dev/null
@@ -1,25 +0,0 @@
-# from typing import Tuple
-#
-# import torch
-# from torch import Tensor
-# from torch.distributions import Normal, kl_divergence, OneHotCategoricalStraightThrough
-# # import torch.distributions as D
-# # from torch.distributions.transformed_distribution import TransformedDistribution
-# from utils import stack, cat, prefill
-
-
-from dreamer import Dreamer
-
-
-class DreamerV2(Dreamer):
-    """
-    Dreamer is the evolution of Planet. The model predictive controller is replaced by
-    an actor-critic to select the next action.
-    """
-
-    def __init__(self, params, env):
-        super().__init__(params, env)
-
-        self.kl_balance = params['kl_balance']
-        if self.kl_loss_weight == 1.0:
-            self.kl_loss_weight = 0.1
diff --git a/src/env.py b/src/env.py
deleted file mode 100644
index 5f79e5c..0000000
--- a/src/env.py
+++ /dev/null
@@ -1,394 +0,0 @@
-from typing import Tuple, List
-
-# import cv2
-import numpy as np
-import torch
-
-import gym
-# from dm_control import suite
-# from dm_control.suite.wrappers import pixels
-
-from utils import images_to_observation
-
-GYM_ENVS = [
-    "Pendulum-v0",
-    "MountainCarContinuous-v0",
-    "Ant-v2",
-    "HalfCheetah-v2",
-    "Hopper-v2",
-    "Humanoid-v2",
-    "HumanoidStandup-v2",
-    "InvertedDoublePendulum-v2",
-    "InvertedPendulum-v2",
-    "Reacher-v2",
-    "Swimmer-v2",
-    "Walker2d-v2",
-]
-CONTROL_SUITE_ENVS = [
-    "cartpole-balance",
-    "cartpole-swingup",
-    "reacher-easy",
-    "finger-spin",
-    "cheetah-run",
-    "ball_in_cup-catch",
-    "walker-walk",
-    "reacher-hard",
-    "walker-run",
-    "humanoid-stand",
-    "humanoid-walk",
-    "fish-swim",
-    "acrobot-swingup",
-]
-CONTROL_SUITE_ACTION_REPEATS = {
-    "cartpole": 8,
-    "reacher": 4,
-    "finger": 2,
-    "cheetah": 4,
-    "ball_in_cup": 6,
-    "walker": 2,
-    "humanoid": 2,
-    "fish": 2,
-    "acrobot": 4,
-}
-
-
-class BaseEnv:
-    """
-    Base class for our environments.
-    """
-
-    def __init__(
-            self,
-            env,
-            seed,
-            max_episode_length,
-            action_repeat,
-            bit_depth,
-            pixel_observation
-    ) -> None:
-        """
-        Initialises base environment attributes.
-        """
-        self._env = env
-        self.seed = seed
-        self.max_episode_length = max_episode_length
-        self.action_repeat = action_repeat
-
-        self.pixel_observation = pixel_observation
-
-        self.bit_depth = bit_depth
-        self.t = 0
-
-        """
-        Initialise the observation space.
-        """
-        self.obs_image_depth = 3
-        self.obs_image_height = 64
-        self.obs_image_width = 64
-
-    def step(self, action):
-        """
-        Steps the environment.
-        """
-
-        raise NotImplementedError
-
-    def reset(self):
-        """
-        Resets the environment.
-        """
-
-        raise NotImplementedError
-
-    def render(self):
-        """
-        Renders the environment.
-        """
-
-        raise NotImplementedError
-
-    def close(self):
-        """
-        Closes the environment.
-        """
-
-        raise NotImplementedError
-
-    @property
-    def observation_size(self) -> Tuple[int, int, int]:
-        """
-        Returns the size of the observation.
-        """
-        if self.pixel_observation:
-            return self.obs_image_depth, self.obs_image_height, self.obs_image_width
-        return self._env.observation_space.shape[0]
-
-    @property
-    def action_size(self):
-        """
-        Returns the size of the action.
-        """
-
-        raise NotImplementedError
-
-    def sample_random_action(self):
-        """
-        Samples an action randomly from a uniform distribution over all valid actions.
-        """
-
-        raise NotImplementedError
-
-
-# class ControlSuiteEnv(BaseEnv):
-#     """
-#     Wrapper for the control suite.
-#     """
-#
-#     def __init__(
-#         self, env, seed, max_episode_length, action_repeat, bit_depth
-#     ) -> None:
-#         super().__init__(env, seed, max_episode_length, action_repeat, bit_depth)
-#
-#         domain, task = env.split("-")
-#         self._env = suite.load(
-#             domain_name=domain, task_name=task, task_kwargs={"random": seed}
-#         )
-#         self._env = pixels.Wrapper(self._env)
-#
-#         if action_repeat != CONTROL_SUITE_ACTION_REPEATS[domain]:
-#             print(
-#                 f"WARNING: action repeat {action_repeat} is not recommended for {domain}."
-#             )
-#
-#     def reset(self) -> np.ndarray:
-#         """
-#         Resets the environment.
-#         """
-#
-#         self.t = 0
-#         self._env.reset()
-#
-#         return images_to_observation(
-#             self._env.physics.render(camera_id=0),
-#             self.bit_depth,
-#             (self.obs_image_height, self.obs_image_width)
-#         )
-#
-#     def step(self, action) -> Tuple[np.ndarray, float, bool, dict]:
-#         """
-#         Steps the environment.
-#         """
-#
-#         action = action.detach().numpy()
-#         reward = 0
-#
-#         for _ in range(self.action_repeat):
-#             state = self._env.step(action)
-#             reward += state.reward
-#             self.t += 1  # Increment internal timer
-#             done = state.last() or self.t == self.max_episode_length
-#             if done:
-#                 break
-#
-#         observation = images_to_observation(
-#             self._env.physics.render(camera_id=0),
-#             self.bit_depth,
-#             (self.obs_image_height, self.obs_image_width)
-#         )
-#
-#         return observation, reward, done
-#
-#     def render(self) -> None:
-#         """
-#         Renders the environment.
-#         """
-#
-#         cv2.imshow("screen", self._env.physics.render(camera_id=0)[:, :, ::-1])
-#         cv2.waitKey(1)
-#
-#     def close(self) -> None:
-#         """
-#         Closes the environment.
-#         """
-#
-#         cv2.destroyAllWindows()
-#         self._env.close()
-#
-#     @property
-#     def action_size(self) -> int:
-#         """
-#         Returns the size of the action.
-#         """
-#
-#         return self._env.action_spec().shape[0]
-#
-#     def sample_random_action(self) -> torch.Tensor:
-#         """
-#         Samples an action randomly from a uniform distribution over all valid actions.
-#         """
-#
-#         spec = self._env.action_spec()
-#         return torch.from_numpy(
-#             np.random.uniform(spec.minimum, spec.maximum, spec.shape)
-#         )
-
-class GymEnv(BaseEnv):
-    """
-    Wrapper for the OpenAI Gym environment.
-    """
-
-    def __init__(
-        self, env, seed, max_episode_length, action_repeat, bit_depth, pixel_observation
-    ):
-        super().__init__(env, seed, max_episode_length, action_repeat, bit_depth, pixel_observation)
-
-        self._env = gym.make(env)
-        self._env.seed(seed)
-        self._env.action_space.seed(seed)
-
-    def reset(self) -> np.ndarray:
-        """
-        Resets the environment.
-        """
-
-        self.t = 0
-        state = self._env.reset()
-        if self.pixel_observation:
-            return images_to_observation(
-                self._env.render(mode="rgb_array"),
-                self.bit_depth,
-                (self.obs_image_height, self.obs_image_width)
-            )
-        return torch.tensor(state, dtype=torch.float32).unsqueeze(dim=0)
-
-    def step(self, action) -> Tuple[np.ndarray, float, bool]:
-        """
-        Steps the environment.
-        """
-
-        action = action.detach().numpy()
-        reward = 0
-
-        for _ in range(self.action_repeat):
-            state, reward_k, done, _ = self._env.step(action)
-            reward += reward_k
-            self.t += 1  # Increment internal timer
-            done = done or self.t == self.max_episode_length
-            if done:
-                break
-        if self.pixel_observation:
-            observation = images_to_observation(
-                self._env.render(mode="rgb_array"),
-                self.bit_depth,
-                (self.obs_image_height, self.obs_image_width)
-            )
-        else:
-            observation = torch.tensor(state, dtype=torch.float32).unsqueeze(dim=0)
-
-        return observation, reward, done
-
-    def render(self) -> None:
-        """
-        Renders the environment.
-        """
-
-        self._env.render()
-
-    def close(self) -> None:
-        """
-        Closes the environment.
-        """
-
-        self._env.close()
-
-    @property
-    def action_size(self) -> int:
-        """
-        Returns the size of the action.
-        """
-
-        return self._env.action_space.shape[0]
-
-    def sample_random_action(self) -> torch.Tensor:
-        """
-        Samples an action randomly from a uniform distribution over all valid actions.
-        """
-
-        return torch.from_numpy(self._env.action_space.sample())
-
-
-def Env(params) -> BaseEnv:
-    """
-    Returns an environment wrapper.
-    """
-
-    env = params["env"]
-    seed = params["seed"]
-    max_episode_length = params["max_episode_length"]
-    action_repeat = params["action_repeat"]
-    bit_depth = params["bit_depth"]
-    pixel_observation = params['pixel_observation']
-
-    if env in GYM_ENVS:
-        return GymEnv(env, seed, max_episode_length, action_repeat, bit_depth, pixel_observation)
-
-    # if env in CONTROL_SUITE_ENVS:
-    #     return ControlSuiteEnv(
-    #         env, seed, max_episode_length, action_repeat, bit_depth
-    #     )
-
-    raise ValueError(f"Unknown environment: {env}")
-
-
-class EnvBatcher:
-    """
-    Wrapper for batching environments together.
-    """
-
-    def __init__(self, env_class, env_params, n) -> None:
-        self.n = n
-        self.envs = [env_class(env_params) for _ in range(n)]
-        self.dones = [True] * n
-
-    def reset(self) -> List[np.ndarray]:
-        """
-        Resets the environment.
-        Returns: observation
-        """
-
-        observations = [env.reset() for env in self.envs]
-        self.dones = [False] * self.n
-        return torch.cat(observations)
-
-    def step(self, actions) -> Tuple[List[np.ndarray], List[float], List[bool]]:
-        """
-        Steps the environment.
-        Returns: (observations, rewards, dones)
-        """
-
-        # Done mask to blank out observations and zero rewards for previously
-        # terminated environments
-        done_mask = torch.nonzero(torch.tensor(self.dones))[:, 0]
-        observations, rewards, dones = zip(
-            *[env.step(action) for env, action in zip(self.envs, actions)]
-        )
-        dones = [
-            d or prev_d for d, prev_d in zip(dones, self.dones)
-        ]  # Env should remain terminated if previously terminated
-
-        self.dones = dones
-        observations = torch.cat(observations)
-        rewards = torch.tensor(rewards, dtype=torch.float32)
-        dones = torch.tensor(dones, dtype=torch.uint8)
-        observations[done_mask] = 0
-        rewards[done_mask] = 0
-
-        return observations, rewards, dones
-
-    def close(self) -> None:
-        """
-        Closes the environments.
-        """
-
-        for env in self.envs:
-            env.close()
diff --git a/src/logger.py b/src/logger.py
deleted file mode 100644
index 591d8c9..0000000
--- a/src/logger.py
+++ /dev/null
@@ -1,134 +0,0 @@
-import os
-from tensorboardX import SummaryWriter
-import numpy as np
-import wandb
-
-
-class Logger:
-    """
-    Logger class for logging data to tensorboard and Wandb.
-    """
-
-    def __init__(self, log_dir, n_logged_samples=10, summary_writer=None, params=None):
-        self._log_dir = log_dir
-        self.params = params
-        print("########################")
-        print("logging outputs to ", log_dir)
-        print("########################")
-        self._n_logged_samples = n_logged_samples
-        self._summ_writer = SummaryWriter(log_dir, flush_secs=1, max_queue=1) \
-            if summary_writer is None else summary_writer
-
-        if self.params["wandb_project"] is not None:
-            wandb.init(project=params["wandb_project"], entity="big-dreamer")
-            wandb.config.update(params)
-
-    def log_scalar(self, scalar, name, step_):
-        """
-        Log a scalar value.
-        """
-
-        self._summ_writer.add_scalar(f"{name}", scalar, step_)
-        if self.params["wandb_project"] is not None:
-            wandb.log({f"{name}": scalar}, step=step_)
-
-    def log_scalars(self, scalar_dict, group_name, step, phase):
-        """
-        Will log all scalars in the same plot.
-        """
-
-        self._summ_writer.add_scalars(
-            f"{group_name}_{phase}", scalar_dict, step
-        )
-
-    def log_image(self, image, name, step):
-        """
-        Log an image.
-        """
-
-        assert len(image.shape) == 3  # [C, H, W]
-        self._summ_writer.add_image(f"{name}", image, step)
-
-    def log_video(self, video_frames, name, step, fps=10):
-        """
-        Log a video.
-        """
-
-        assert (
-            len(video_frames.shape) == 5
-        ), "Need [N, T, C, H, W] input tensor for video logging!"
-        self._summ_writer.add_video(f"{name}", video_frames, step, fps=fps)
-
-    def log_paths_as_videos(
-        self, paths, step, max_videos_to_save=2, fps=10, video_title="video"
-    ):
-        """
-        Log a video of the paths.
-        """
-
-        # reshape the rollouts
-        videos = [np.transpose(p["image_obs"], [0, 3, 1, 2]) for p in paths]
-
-        # max rollout length
-        max_videos_to_save = np.min([max_videos_to_save, len(videos)])
-        max_length = videos[0].shape[0]
-        for i in range(max_videos_to_save):
-            if videos[i].shape[0] > max_length:
-                max_length = videos[i].shape[0]
-
-        # pad rollouts to all be same length
-        for i in range(max_videos_to_save):
-            if videos[i].shape[0] < max_length:
-                padding = np.tile(
-                    [videos[i][-1]], (max_length - videos[i].shape[0], 1, 1, 1)
-                )
-                videos[i] = np.concatenate([videos[i], padding], 0)
-
-        # log videos to tensorboard event file
-        videos = np.stack(videos[:max_videos_to_save], 0)
-        self.log_video(videos, video_title, step, fps=fps)
-
-    def log_figures(self, figure, name, step, phase):
-        """
-        figure: matplotlib.pyplot figure handle.
-        """
-
-        assert (
-            figure.shape[0] > 0
-        ), "Figure logging requires input shape [batch x figures]!"
-        self._summ_writer.add_figure(f"{name}_{phase}", figure, step)
-
-    def log_figure(self, figure, name, step, phase):
-        """
-        figure: matplotlib.pyplot figure handle.
-        """
-
-        self._summ_writer.add_figure(f"{name}_{phase}", figure, step)
-
-    def log_graph(self, array, name, step, phase):
-        """
-        figure: matplotlib.pyplot figure handle
-        """
-        def plot_graph(array):
-            raise NotImplementedError()
-        im = plot_graph(array)
-        self._summ_writer.add_image(f"{name}_{phase}", im, step)
-
-    def dump_scalars(self, log_path=None):
-        """
-        Dump all scalars to a json file.
-        """
-
-        log_path = (
-            os.path.join(self._log_dir, "scalar_data.json")
-            if log_path is None
-            else log_path
-        )
-        self._summ_writer.export_scalars_to_json(log_path)
-
-    def flush(self):
-        """
-        Flush the tensorboard event file.
-        """
-
-        self._summ_writer.flush()
diff --git a/src/main.py b/src/main.py
deleted file mode 100644
index c701111..0000000
--- a/src/main.py
+++ /dev/null
@@ -1,291 +0,0 @@
-import random
-import time
-
-import hydra
-import numpy as np
-import torch
-from omegaconf import DictConfig, open_dict
-from tqdm import tqdm
-from torchvision.utils import make_grid
-
-
-from planet import Planet
-from dreamer import Dreamer
-from dreamerV2 import DreamerV2
-from logger import Logger
-from env import Env, EnvBatcher
-from utils import init_gpu, device
-
-
-@hydra.main(config_path="conf", config_name="config")
-def my_app(cfg: DictConfig) -> None:
-    """
-    Main function for running the experiments.
-    """
-    print("Command Dir:", os.getcwd())
-
-    params = vars(cfg)
-    for key, value in cfg.items():
-        params[key] = value
-    print("params: ", params)
-
-    ##################################
-    ### CREATE DIRECTORY FOR LOGGING
-    ##################################
-    data_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), "data")
-
-    if not os.path.exists(data_path):
-        os.makedirs(data_path)
-
-    logdir = os.path.join(
-        data_path,
-        f'project_{cfg.exp_name}_{cfg.env}_{time.strftime("%d-%m-%Y_%H-%M-%S")}',
-    )
-    params["logdir"] = logdir
-    if not os.path.exists(logdir):
-        os.makedirs(logdir)
-
-    with open_dict(cfg):
-        cfg.logdir = logdir
-
-    print("\nLOGGING TO: ", logdir, "\n")
-
-    #############
-    # INIT Structure
-    #############
-    np.random.seed(params["seed"])
-    torch.manual_seed(params["seed"])
-    random.seed(params["seed"])
-
-    init_gpu(use_gpu=not params["disable_cuda"])
-
-    logger = Logger(params["logdir"], params=params)
-
-    #############
-    # ENV
-    #############
-    env = Env(params)
-
-    #############
-    # Model
-    #############
-    if params["algorithm"] == "planet":
-        model = Planet(params, env)
-    elif params["algorithm"] == "dreamer":
-        model = Dreamer(params, env)
-    elif params["algorithm"] == "dreamerV2":
-        model = DreamerV2(params, env)
-    else:
-        raise NotImplementedError(
-            f'algorithm {params["algorithm"]} is not yet implemented.'
-        )
-
-    env_steps, num_episodes = model.randomly_initialize_replay_buffer()
-    print(f"Initialized with {num_episodes} episodes and {env_steps} steps")
-
-    logs ={}
-    observation = env.reset()
-    past_time = time.time()
-    # done = False
-    episode_reward = 0
-    last_episode_reward = 0
-    episode_steps = 0
-    belief = torch.zeros(1, params["belief_size"], device=device)
-    posterior_state = torch.zeros(1, params["state_size"], device=device)
-    action = torch.zeros(1, env.action_size, device=device)
-    ###################
-    # TRAINING LOOP
-    ###################
-    for step in range(env_steps, params['train_steps']):
-        ###################
-        # Weight updates
-        ###################
-        if step % params['environment_steps_per_update'] == 0:
-            weight_update_start_time = time.time()
-            for _ in range(params['collect_interval']):
-                logs = model.train_step()
-                elapsed_time = time.time() - weight_update_start_time
-                logs["weight_update_per_sec"] = params['collect_interval'] / elapsed_time
-
-        if params["algorithm"] in ['dreamer', 'dreamerV2']:
-            if step % params['ActorCritic']['slow_critic_update_interval']:
-                model.update_critic()
-
-        ##########################
-        # Environment interaction
-        ##########################
-        # print("Data collection")
-        with torch.no_grad():
-            # observation = env.reset()
-            # total_reward = 0
-            # belief = torch.zeros(1, params["belief_size"], device=device)
-            # posterior_state = torch.zeros(1, params["state_size"], device=device)
-            # action = torch.zeros(1, env.action_size, device=device)
-
-            # pbar = tqdm(range(params["max_episode_length"] // params["action_repeat"]))
-            # t = 0
-
-            # for t in pbar:
-            (
-                belief,
-                posterior_state,
-                action,
-                next_observation,
-                reward,
-                done,
-            ) = model.update_belief_and_act(
-                env,
-                belief,
-                posterior_state,
-                action,
-                observation.to(device=device),
-                explore=True,
-            )
-
-            # store the new stuff
-            model.buffer.append(observation, action, reward, done)
-
-            episode_reward += reward
-            episode_steps += 1
-            observation = next_observation
-
-            # print(episode_reward)
-
-            if params["render"]:
-                env.render()
-            if done or episode_steps % params["max_episode_length"] == 0:
-                # pbar.close()
-                # env.close()
-                # break
-                observation = env.reset()
-                # done = False
-                last_episode_reward = episode_reward
-                episode_reward = 0
-                episode_steps = 0
-                # todo use inplace stuff instead
-                belief = torch.zeros(1, params["belief_size"], device=device)
-                posterior_state = torch.zeros(1, params["state_size"], device=device)
-                action = torch.zeros(1, env.action_size, device=device)
-
-                num_episodes += 1
-            # env_steps += t * params["action_repeat"]
-            # num_episodes += 1
-            logs["episode_total_reward"] = last_episode_reward
-
-        ##########################
-        # Logging
-        ##########################
-        if step % params["log_freq"] == 0:
-            current_time = time.time()
-            elapsed_time = current_time - past_time
-            logs["env_update_per_sec"] = params['log_freq'] / elapsed_time
-            past_time = current_time
-            print("\nPerform Logging")
-            for key, value in logs.items():
-                print(f"{key} : {value}")
-                logger.log_scalar(value, key, step)
-            print("Done logging...")
-
-            logger.flush()
-
-        ##########################
-        # Test model
-        ##########################
-        if step % params["test_interval"] == 0:
-            print("\nTest model")
-            model.eval()
-
-            # Initialise parallelised test environments
-            test_envs = EnvBatcher(Env, params, params["test_episodes"])
-
-            with torch.no_grad():
-                observation_test = test_envs.reset()
-                total_rewards_test = np.zeros((params["test_episodes"],))
-                video_frames_test = []
-
-                belief_test = torch.zeros(
-                    params["test_episodes"], params["belief_size"], device=device
-                )
-                posterior_state_test = torch.zeros(
-                    params["test_episodes"], params["state_size"], device=device
-                )
-                action_test = torch.zeros(
-                    params["test_episodes"], env.action_size, device=device
-                )
-
-                pbar_test = tqdm(
-                    range(params["max_episode_length"] // params["action_repeat"])
-                )
-                for _ in pbar_test:
-                    (
-                        belief_test,
-                        posterior_state_test,
-                        action_test,
-                        next_observation_test,
-                        reward_test,
-                        done_test,
-                    ) = model.update_belief_and_act(
-                        test_envs,
-                        belief_test,
-                        posterior_state_test,
-                        action_test,
-                        observation_test.to(device=device),
-                    )
-
-                    total_rewards_test += reward_test.numpy()
-
-                    if params['pixel_observation']:
-                        # Collect real vs. predicted frames for video
-                        video_frames_test.append(
-                            make_grid(
-                                torch.cat(
-                                    [
-                                        observation,
-                                        model.observation_model(
-                                            belief_test, posterior_state_test
-                                        ).cpu(),
-                                    ],
-                                    dim=3,
-                                )
-                                + 0.5,
-                                nrow=5,
-                            ).numpy()
-                        )  # Decentre
-                    observation_test = next_observation_test
-
-                    pbar_test.set_description("Testing...")
-                    if done_test.sum().item() == params["test_episodes"]:
-                        pbar_test.close()
-                        test_envs.close()
-                        break
-                # log test scalar
-                test_logs = {
-                    "Eval_min_return": total_rewards_test.min().item(),
-                    "Eval_avg_return": total_rewards_test.mean().item(),
-                    "Eval_max_return": total_rewards_test.max().item(),
-                    "Eval_std_return": total_rewards_test.std().item(),
-                }
-                for key, value in test_logs.items():
-                    print(f"{key} : {value}")
-                    logger.log_scalar(value, key, step)
-                model.eval()
-            test_envs.close()
-
-            # TODO : Save Model
-            if step % params["log_video_freq"] == 0 \
-                    and params["log_video_freq"] != -1 \
-                    and params['pixel_observation']:
-                # log eval videos
-                logger.log_video(
-                    np.expand_dims(np.stack(video_frames_test), axis=0),
-                    name="Eval_rollout",
-                    step=step,
-                )
-    # Close training environment
-    env.close()
-
-if __name__ == "__main__":
-    import os
-
-    print("Command Dir:", os.getcwd())
-    my_app(None)
diff --git a/src/memory.py b/src/memory.py
deleted file mode 100644
index 0d0dd3b..0000000
--- a/src/memory.py
+++ /dev/null
@@ -1,104 +0,0 @@
-import numpy as np
-import torch
-
-from utils import postprocess_observation, preprocess_observation_
-
-
-class ExperienceReplay:
-    """
-    Experience replay memory.
-    """
-
-    def __init__(self, size, action_size, bit_depth, pixel_observation, observation_size, device):
-        self.device = device
-        self.size = size
-        self.pixel_observation = pixel_observation
-        if self.pixel_observation:
-            self.observations = np.empty((size, 3, 64, 64), dtype= np.uint8)
-        else:
-            # print(f"observation_size={observation_size}")
-            self.observations = np.empty((size, observation_size),dtype=np.float32)
-
-        self.actions = np.empty((size, action_size), dtype=np.float32)
-        self.rewards = np.empty((size,), dtype=np.float32)
-        self.nonterminals = np.empty((size, 1), dtype=np.float32)
-        self.idx = 0
-        self.full = False  # Tracks if memory has been filled/all slots are valid
-        self.steps, self.episodes = (
-            0,
-            0,
-        )  # Tracks how much experience has been used in total
-        self.bit_depth = bit_depth
-
-    def append(self, observation, action, reward, done):
-        """
-        Append a new experience to the memory.
-        """
-        if self.pixel_observation:
-            # Decentre and discretise visual observations (to save memory)
-            self.observations[self.idx] = postprocess_observation(
-                observation.numpy(), self.bit_depth
-            )
-        else:
-            self.observations[self.idx] = observation.numpy()
-        self.actions[self.idx] = action.numpy()
-        self.rewards[self.idx] = reward
-        self.nonterminals[self.idx] = not done
-        self.idx = (self.idx + 1) % self.size
-        self.full = self.full or self.idx == 0
-        self.steps, self.episodes = self.steps + 1, self.episodes + (1 if done else 0)
-
-    def _sample_idx(self, L):
-        """
-        Sample a single sequence chunk uniformly from the memory.
-
-        Returns an index for a valid single sequence chunk uniformly sampled from the memory
-        """
-
-        valid_idx = False
-        while not valid_idx:
-            # print("HEREE")
-            # print(f'self.size={self.size}, self.idx={self.idx} L ={L}')
-
-            idx = np.random.randint(0, self.size if self.full else self.idx - L)
-            idxs = np.arange(idx, idx + L) % self.size
-            valid_idx = (
-                not self.idx in idxs[1:]
-            )  # Make sure data does not cross the memory index
-        return idxs
-
-    def _retrieve_batch(self, idxs, n, L):
-        """
-        Retrieve a batch of sequence chunks uniformly sampled from the memory.
-        """
-
-        vec_idxs = idxs.transpose().reshape(-1)  # Unroll indices
-        observations = torch.as_tensor(self.observations[vec_idxs].astype(np.float32))
-        if self.pixel_observation:
-            # Undo discretisation for visual observations
-            preprocess_observation_(observations, self.bit_depth)
-        return (
-            observations.reshape(L, n, *observations.shape[1:]),
-            self.actions[vec_idxs].reshape(L, n, -1),
-            self.rewards[vec_idxs].reshape(L, n),
-            self.nonterminals[vec_idxs].reshape(L, n, 1),
-        )
-
-    def sample(self, n, L):
-        """
-        Sample a batch of n sequence chunks of length L uniformly from the memory.
-        """
-        # print(self.size, n, L)
-
-        batch = self._retrieve_batch(
-            np.asarray([self._sample_idx(L) for _ in range(n)]), n, L
-        )
-        # print(np.asarray([self._sample_idx(L) for _ in range(n)]))
-        # [1578 1579 1580 ... 1625 1626 1627] | 0/100 [00:00<?, ?it/s]
-        # [1049 1050 1051 ... 1096 1097 1098]
-        # [1236 1237 1238 ... 1283 1284 1285]
-        # ...
-        # [2199 2200 2201 ... 2246 2247 2248]
-        # [ 686  687  688 ...  733  734  735]
-        # [1377 1378 1379 ... 1424 1425 1426]]
-        return [torch.as_tensor(item).to(device=self.device) for item in batch]
diff --git a/src/models.py b/src/models.py
deleted file mode 100644
index aeb898b..0000000
--- a/src/models.py
+++ /dev/null
@@ -1,759 +0,0 @@
-from typing import Union, Optional, List, Tuple
-
-import numpy as np
-import torch
-from torch import nn, Tensor
-
-import torch.nn.functional as F
-import torch.distributions as D
-
-
-from torchtyping import TensorType, patch_typeguard
-from typeguard import typechecked
-
-from utils import cat, stack, prefill, build_mlp
-
-
-Activation = Union[str, nn.Module]
-
-patch_typeguard()
-
-
-
-def bottle(f, x_tuple: tuple) -> Tensor:
-    """
-    Wraps the input tuple for a function to process a time x batch x features sequence in
-    batch x features (assumes one output)
-
-    Args:
-        f: function to apply to the input tuple
-        x_tuple (Tuple[belief, posterior_states]): tuple of tensors (belief, posterior_states)
-
-    Returns:
-        Tensor: output of f() of shape: (belief[0].size(), belief[1].size(), *f().size()[1:])
-    """
-
-    x_sizes = [x.size() for x in x_tuple]
-    y = f(*[x[0].view(x[1][0] * x[1][1], *x[1][2:]) for x in zip(x_tuple, x_sizes)])
-    y_size = y.size()
-    output = y.view(x_sizes[0][0], x_sizes[0][1], *y_size[1:])
-
-    return output
-
-
-class GaussianBeliefModel(nn.Module):
-    """
-    Gaussian belief model (for PlaNet and DreamerV1)
-    """
-    def __init__(
-            self,
-            input_size: int,
-            hidden_size: int,
-            state_size: int,
-            activation: str,
-            min_std_dev: float
-    ) -> None:
-        super().__init__()
-        self.min_std_dev = min_std_dev
-        self.model = build_mlp(input_size, hidden_size, 2*state_size, 1, activation)
-
-    def forward(
-            self,
-            belief: Tensor
-    ) -> Tuple[Tensor, Tuple[Tensor, ...]]:
-        """
-        Args:
-            belief (Tensor): belief of shape: (batch_size, belief_size)
-        Returns:
-            Tensor: belief of shape: (batch_size, belief_size)
-        """
-        mean, _std_dev = torch.chunk(self.model(belief), 2, dim=1)
-        std_dev = F.softplus(_std_dev) + self.min_std_dev
-        state = mean + std_dev * torch.randn_like(mean)
-        return state, (mean, std_dev)
-
-
-class CategoricalBeliefModel(nn.Module):
-    """
-    CategoricalBeliefModel
-    """
-    def __init__(
-            self,
-            input_size: int,
-            hidden_size: int,
-            discrete_latent_dimensions: int,
-            discrete_latent_classes: int,
-            activation: str
-    ) -> None:
-        super().__init__()
-        self.discrete_latent_dimensions = discrete_latent_dimensions
-        self.discrete_latent_classes = discrete_latent_classes
-        self.dim = discrete_latent_classes * discrete_latent_dimensions
-        self.model = build_mlp(
-            input_size,
-            hidden_size,
-            self.dim,
-            1,
-            activation
-        )
-        assert discrete_latent_classes
-
-    def forward(self, belief: Tensor) -> Tuple[Tensor, Tuple[Tensor, ...]]:
-        """
-        Args:
-            belief (Tensor): belief of shape: (batch_size, belief_size)
-        Returns:
-            Tensor: belief of shape: (batch_size, belief_size)
-        """
-        logits = self.model(belief)
-        batch_shape = logits.shape[:-1]
-        shape = (*batch_shape, self.discrete_latent_dimensions, self.discrete_latent_classes)
-        logits = logits.reshape(shape)
-        # use straight through gradient
-        # https://arxiv.org/abs/1308.3432
-        dist = D.OneHotCategoricalStraightThrough(logits=logits)
-        state = dist.rsample()
-        state = state.view(*batch_shape, self.dim)
-        return state, (logits,)
-
-
-class TransitionModel(nn.Module):
-    """
-    Transition model for the MDP.
-    """
-
-    def __init__(
-        self,
-        belief_size: int,
-        state_size: int,
-        action_size: int,
-        hidden_size: int,
-        embedding_size: int,
-        activation: Optional[str] = "ELU",
-        min_std_dev: float = 0.1,
-        latent_distribution: Optional[str] = "Gaussian",
-        discrete_latent_dimensions: Optional[int] = 32,
-        discrete_latent_classes: Optional[int] = 32
-    ) -> None:
-        super().__init__()
-
-        # general attibutes
-        if isinstance(activation, str):
-            activation = getattr(nn, activation)
-        self.min_std_dev = min_std_dev
-
-        assert latent_distribution in ["Gaussian", "Categorical"], f"{latent_distribution}"
-        self.latent_distribution = latent_distribution
-
-        # recurrent component
-        self.rnn = nn.GRUCell(belief_size, belief_size)
-
-        # model components
-        self.fc_embed_state_action = build_mlp(
-            state_size+action_size, -1, belief_size, 0, output_activation=activation
-        )
-
-        # Belief models
-        if self.latent_distribution == "Gaussian":
-            self.belief_prior = GaussianBeliefModel(
-                belief_size, hidden_size, state_size, activation, min_std_dev
-            )
-            self.belief_posterior = GaussianBeliefModel(
-                belief_size + embedding_size,
-                hidden_size,
-                state_size,
-                activation,
-                min_std_dev,
-            )
-        elif self.latent_distribution == "Categorical":
-            self.belief_prior = CategoricalBeliefModel(
-                belief_size,
-                hidden_size,
-                discrete_latent_dimensions,
-                discrete_latent_classes,
-                activation
-            )
-            self.belief_posterior = CategoricalBeliefModel(
-                belief_size + embedding_size,
-                hidden_size,
-                discrete_latent_dimensions,
-                discrete_latent_classes,
-                activation
-            )
-
-        self.modules = [
-            self.fc_embed_state_action,
-            self.belief_prior,
-            self.belief_posterior,
-        ]
-
-    @typechecked
-    def forward(
-        self,
-        init_state: TensorType['batch_size', 'state_size'],
-        actions: TensorType['seq_len', 'batch_size', 'action_size'],
-        init_belief: TensorType['batch_size', 'belief_size'],
-        embeddings: Optional[TensorType['seq_len', 'batch_size', 'embedding_size']] = None,
-        nonterminals: Optional[TensorType['seq_len', 'batch_size', 1]] = None,
-    ) -> Tuple[Tensor, Tensor, Tuple[Tensor, ...], Optional[Tensor], Optional[Tuple[Tensor, ...]]]:
-        """
-        L=Chunk size, B=Batch size, Hi=Hidden size, Be=Belief size, S=State Size, A=Action size
-        Input:
-            init_state:     (B, S)
-            actions:        (L, B, A)
-            init_belief:    (B, Be)
-            observations:   (L, B, C, H, W)
-            nonterminals:   (L, B, 1)
-        Output:
-            beliefs:            (L-1, B, Be)
-            prior_states:       (L-1, B, S)
-            prior_params:       (L-1, B, S)
-            posterior_states:   (L-1, B, S)
-            posterior_params:   (L-1, B, S)
-        """
-
-        # Create lists for hidden states
-        # (cannot use single tensor as buffer because autograd won't work with inplace writes)
-        # print("Twerk 1")
-        T = actions.size(0) + 1
-        beliefs = prefill(T)
-        prior_states = prefill(T)
-        posterior_states = prefill(T)
-
-        if self.latent_distribution == "Gaussian":
-            prior_means = prefill(T)
-            prior_std_devs = prefill(T)
-            posterior_means = prefill(T)
-            posterior_std_devs = prefill(T)
-        elif self.latent_distribution == "Categorical":
-            prior_logits = prefill(T)
-            posterior_logits = prefill(T)
-
-        # print("Twerk 2")
-        beliefs[0] = init_belief
-        prior_states[0] = init_state
-        posterior_states[0] = init_state
-
-        # print("Twerk 3")
-        # Loop over time sequence
-        for t in range(T - 1):
-            # Select appropriate previous state
-            _state = prior_states[t] if embeddings is None else posterior_states[t]
-
-            # print("Twerk 4")
-            # print(f'_state.shape={_state.shape}')
-            # print(f'nonterminals[t].shape={nonterminals[t].shape}')
-            # Mask if previous transition was terminal
-            _state = _state if nonterminals is None else _state * nonterminals[t]
-
-            # print("Twerk 5")
-            # Compute belief (deterministic hidden state)
-            hidden = self.fc_embed_state_action(cat(_state, actions[t]))
-            beliefs[t + 1] = self.rnn(hidden, beliefs[t])
-
-            # print("Twerk 6")
-            # Compute state prior by applying transition dynamics
-            prior_states[t + 1], prior_params_ = self.belief_prior(beliefs[t + 1])
-            if self.latent_distribution == "Gaussian":
-                prior_means[t + 1], prior_std_devs[t + 1] = prior_params_
-            elif self.latent_distribution == "Categorical":
-                prior_logits[t + 1] = prior_params_
-
-            if embeddings is not None:
-                # Compute state posterior by applying transition dynamics and using
-                # current observation.
-                t_ = t - 1  # Using t_ to deal with different time indexing for observations
-                posterior_input = cat(beliefs[t + 1], embeddings[t_ + 1])
-                posterior_states[t + 1], post_params_ = self.belief_posterior(posterior_input)
-                if self.latent_distribution == "Gaussian":
-                    posterior_means[t + 1], posterior_std_devs[t + 1] = post_params_
-                elif self.latent_distribution == "Categorical":
-                    posterior_logits[t + 1] = post_params_
-
-        # print("Twerk 7")
-        # print(len(beliefs), [b.shape for b in beliefs])
-        # print(len(prior_states), [b.shape for b in prior_states])
-        # Return new hidden states
-        beliefs = stack(beliefs)
-        prior_states = stack(prior_states)
-
-        prior_params = None
-        if self.latent_distribution == "Gaussian":
-            prior_params = (stack(prior_means), stack(prior_std_devs))
-        elif self.latent_distribution == "Categorical":
-            prior_params = (stack(prior_logits),)
-
-        # print("Twerk 8")
-        # add posterior computations if observations were present
-        if embeddings is not None:
-            posterior_states = stack(posterior_states)
-            # add posterior params
-            posterior_params = None
-            if self.latent_distribution == "Gaussian":
-                posterior_params = (stack(posterior_means), stack(posterior_std_devs))
-            elif self.latent_distribution == "Categorical":
-                posterior_params = (stack(posterior_logits))
-        else:
-            posterior_states, posterior_params = None, None
-
-        return beliefs, prior_states, prior_params, posterior_states, posterior_params
-
-
-class Reshape(nn.Module):
-    """
-    Reshape module for PyTorch.
-    """
-
-    def __init__(self, shape: List):
-        super().__init__()
-        self.shape = shape
-
-    def forward(self, x):
-        """
-        Forward pass.
-        """
-
-        return x.view(self.shape)
-
-
-class ObservationModel(nn.Module):
-    """
-    Observation model.
-    Input : Latent -> Ouput : Recontructed Image
-    """
-
-    def __init__(
-        self,
-        belief_size: int,
-        state_size: int,
-        embedding_size: int,
-        activation: Activation = "relu",
-    ) -> None:
-        super().__init__()
-
-        if isinstance(activation, str):
-            activation = getattr(nn, activation)
-        self.output_shape = (3, 64, 64)
-
-        self.decoder = nn.Sequential(
-            nn.Linear(belief_size + state_size, embedding_size),
-            Reshape([-1, embedding_size, 1, 1]),
-            nn.ConvTranspose2d(embedding_size, 128, 5, 2),
-            activation(),
-            nn.ConvTranspose2d(128, 64, 5, 2),
-            activation(),
-            nn.ConvTranspose2d(64, 32, 6, 2),
-            activation(),
-            nn.ConvTranspose2d(32, 3, 6, 2),
-        )
-
-    def forward(
-            self,
-            belief: Tensor,
-            state: Tensor
-    ) -> Tensor:
-        """
-        Forward pass.
-        """
-        batch_shape = belief.shape[:-1]  # L,B or just B
-        x = torch.cat([belief, state], dim=-1)  # (L, B , S+Be)
-        x = self.decoder(x)
-        x = x.view(*batch_shape, *self.output_shape)
-        return x
-
-
-class DenseModel(nn.Module):
-    """
-    Dense model.
-    """
-
-    def __init__(
-            self,
-            input_size: int,
-            # belief_size: int,
-            # state_size: int,
-            hidden_size: int,
-            output_size: int = 1,
-            activation: str = "ELU",
-            n_layers: int = 4,
-            distribution: str = 'normal'
-    ) -> None:
-        super().__init__()
-        self.model = build_mlp(input_size, hidden_size, output_size, n_layers, activation)
-        # self.model = build_mlp(belief_size + state_size, hidden_size, 1, n_layers, activation)
-        self.distribution = distribution
-
-    # def forward(self, belief: Tensor, state: Tensor) -> Tensor:
-    #     """
-    #     Forward pass.
-    #     """
-    #     x = torch.cat([belief, state], dim=-1)  # (L, B , S+Be)
-    #     x = self.model(x)
-    #     return x
-    def forward(self, *args: Tuple[Tensor]) -> Tensor:
-        """
-        Forward pass.
-        """
-        if len(args) == 2:
-            # print("Twerk")
-            belief, state = args
-            # print("Twerk 2")
-            # print(belief.shape, state.shape)
-            x = torch.cat([belief, state], dim=-1)  # (L, B , S+Be)
-            # print("Twerk 3")
-        else:
-            x, = args
-
-        x = self.model(x)
-        return x
-
-# class RewardModel(nn.Module):
-#     """
-#     Reward model.
-#     """
-#
-#     def __init__(
-#         self,
-#         belief_size: int,
-#         state_size: int,
-#         hidden_size: int,
-#         activation: str = "ELU",
-#         n_layers:int = 4
-#     ) -> None:
-#         super().__init__()
-#         self.model = build_mlp(belief_size+state_size, hidden_size, 1, n_layers, activation)
-#
-#     def forward(self, belief: Tensor, state: Tensor) -> Tensor:
-#         """
-#         Forward pass.
-#         """
-#         x = torch.cat([belief, state], dim=-1)  # (L, B , S+Be)
-#         x = self.model(x)
-#         return x
-
-
-# class CriticModel(nn.Module):
-#     """
-#     Critic model.
-#     """
-#
-#     def __init__(
-#         self,
-#         belief_size: int,
-#         state_size: int,
-#         hidden_size: int,
-#         activation: Optional[str] = "ELU",
-#         n_layers: int = 4
-#     ) -> None:
-#         super().__init__()
-#
-#         self.model = build_mlp(
-#             input_size = belief_size + state_size,
-#             output_size = 1,
-#             n_layers = n_layers,
-#             hidden_size = hidden_size,
-#             activation = activation
-#         )
-#     def forward(self, belief, state):
-#         """
-#         Forward pass.
-#         Input: belief, state
-#         """
-#
-#         return self.model(cat(belief, state)).squeeze(dim=1)
-
-
-class ActorModel(nn.Module):
-    """
-    Actor model.
-    """
-
-    def __init__(
-        self,
-        belief_size: int,
-        state_size: int,
-        hidden_size: int,
-        action_size: int,
-        activation_function: str = "ELU",
-        action_distribution: str = 'Gaussian',
-        min_std: float = 1e-4,
-        init_std: float = 5,
-        mean_scale: float = 5,
-        n_layers: int = 4
-    ) -> None:
-        super().__init__()
-        if action_distribution == 'Gaussian':
-            output_size = 2 * action_size
-        elif action_distribution == 'Categorical':
-            output_size = action_size
-        else:
-            NotImplementedError()
-
-        self.model = build_mlp(
-            input_size=belief_size + state_size,
-            output_size= output_size,
-            n_layers=n_layers,
-            hidden_size=hidden_size,
-            activation=activation_function
-        )
-
-        self._min_std = min_std
-        self._init_std = init_std
-        self._mean_scale = mean_scale
-        self.raw_init_std = torch.log(torch.exp(torch.tensor(self._init_std)) - 1)
-        self.action_distribution = action_distribution
-
-    def forward(self, belief, state):
-        """
-        Forward pass.
-        Input: belief, state
-        """
-        out_model = self.model(cat(belief, state)).squeeze(dim=1)
-
-        if self.action_distribution == 'Gaussian':
-            action_mean, action_std_dev = torch.chunk(out_model, 2, dim=1)
-            action_mean = self._mean_scale * torch.tanh(action_mean / self._mean_scale)
-            action_std = F.softplus(action_std_dev + self.raw_init_std) + self._min_std
-            return action_mean, action_std
-        if self.action_distribution == 'Categorical':
-            action_dist = self.get_action_dist(out_model)
-            action = action_dist.sample()
-            action = action + action_dist.probs - action_dist.probs.detach()
-            return action, action_dist
-
-        raise NotImplementedError()
-
-
-class CnnImageEncoder(nn.Module):
-    """
-    CNN image encoder.
-    """
-
-    def __init__(self, embedding_size: int, activation: Activation = "ELU") -> None:
-        super().__init__()
-
-        if isinstance(activation, str):
-            activation = getattr(nn, activation)
-
-        self.model = nn.Sequential(
-            # in_channels, out_channels, kernel_size, stride
-            nn.Conv2d(3, 32, 4, 2),
-            activation(),
-            nn.Conv2d(32, 64, 4, 2),
-            activation(),
-            nn.Conv2d(64, 128, 4, 2),
-            activation(),
-            nn.Conv2d(128, 256, 4, 2),  # (B, 3, H, W) ->  (B, 256, H/16, W/16)
-            activation(),
-            nn.Flatten(),
-            nn.Identity()
-            if embedding_size == 1024
-            else nn.Linear(1024, embedding_size),
-        )
-
-    def forward(self, observation: Tensor) -> Tensor:
-        """
-        Forward pass.
-        """
-
-        batch_shape = observation.shape[:-3]  # L,B or just B
-        obs_shape = observation.shape[-3:]  # C, W, H
-
-        embedding = self.model(observation.view(-1, *obs_shape))
-        embedding = torch.reshape(embedding, (*batch_shape, -1))
-        return embedding
-
-
-class LinearCombination(nn.Module):
-    """
-    Linear combination of two inputs.
-    """
-
-    def __init__(self, in1_size: int, in2_size: int, out_size: int):
-        super().__init__()
-        self.in1_linear = nn.Linear(in1_size, out_size)
-        self.in2_linear = nn.Linear(in2_size, out_size, bias=False)
-
-    def forward(self, in1, in2):
-        """
-        Forward pass.
-        """
-
-        return self.in1_linear(in1) + self.in1_linear(in2)
-
-
-class DiscountModel(nn.Module):
-    """
-    Discount model as in https://arxiv.org/pdf/2010.02193.pdf
-    """
-    def __init__(
-            self,
-            belief_size: int,
-            embedding_size: int,
-            hidden_size: int,
-            n_layers: int,
-            activation_function: Optional[str] = "relu"
-    ) -> nn.Sequential:
-        super().__init__()
-
-        self.model = build_mlp(
-            input_size=belief_size + embedding_size,
-            output_size=1,
-            n_layers=n_layers,
-            hidden_size=hidden_size,
-            activation=activation_function,
-            output_activation='Identity'
-        )
-
-    def forward(self, belief: Tensor, embedding: Tensor):
-        """
-        Forward pass.
-        """
-        x = cat(belief, embedding)
-        logits = self.model(x)
-        dist = D.Bernoulli(logits=logits)
-        return dist.rsample()
-
-
-
-
-# "atanh", "TanhBijector" and "SampleDist" are from the following repo
-# https://github.com/juliusfrost/dreamer-pytorch
-def atanh(x):
-    """
-    Inverse hyperbolic tangent.
-    """
-
-    return 0.5 * torch.log((1 + x) / (1 - x))
-
-
-class TanhBijector(torch.distributions.Transform):
-    """
-    Bijector for the tanh function.
-    """
-
-    def __init__(self):
-        super().__init__()
-        self.bijective = True
-        self.domain = torch.distributions.constraints.real
-        self.codomain = torch.distributions.constraints.interval(-1.0, 1.0)
-
-    @property
-    def sign(self):
-        """
-        Sign of the bijector.
-        """
-        return 1.0
-
-    def _call(self, x):
-        """
-        Forward pass.
-        Input: x
-        """
-
-        return torch.tanh(x)
-
-    def _inverse(self, y: torch.Tensor):
-        """
-        Inverse pass.
-        Input: y
-        """
-
-        y = torch.where(
-            (torch.abs(y) <= 1.0), torch.clamp(y, -0.99999997, 0.99999997), y
-        )
-        y = atanh(y)
-        return y
-
-    def log_abs_det_jacobian(self, x, y):
-        """
-        Log of the absolute determinant of the Jacobian.
-        """
-
-        return 2.0 * (np.log(2) - x - F.softplus(-2.0 * x))
-
-
-class SampleDist:
-    """
-    Sample from a distribution.
-    """
-
-    def __init__(self, dist, samples=100):
-        self._dist = dist
-        self._samples = samples
-
-    @property
-    def name(self):
-        """
-        Name of the distribution.
-        """
-
-        return "SampleDist"
-
-    def __getattr__(self, name):
-        """
-        Get an attribute.
-        """
-
-        return getattr(self._dist, name)
-
-    def mean(self):
-        """
-        Mean of the distribution.
-        """
-        # TODO: need to be defined. Is dist here supposed to be _dist?
-        sample = self._dist.rsample()
-        return torch.mean(sample, 0)
-
-    def mode(self):
-        """
-        Mode of the distribution.
-        """
-
-        dist = self._dist.expand((self._samples, *self._dist.batch_shape))
-        sample = dist.rsample()
-        logprob = dist.log_prob(sample)
-        batch_size = sample.size(1)
-        feature_size = sample.size(2)
-        indices = (
-            torch.argmax(logprob, dim=0)
-            .reshape(1, batch_size, 1)
-            .expand(1, batch_size, feature_size)
-        )
-        return torch.gather(sample, 0, indices).squeeze(0)
-
-    def entropy(self):
-        """
-        Entropy of the distribution.
-        """
-
-        dist = self._dist.expand((self._samples, *self._dist.batch_shape))
-        sample = dist.rsample()
-        logprob = dist.log_prob(sample)
-        return -torch.mean(logprob, 0)
-
-    def sample(self):
-        """
-        Sample from the distribution.
-        """
-
-        return self._dist.sample()
-
-
-def activation_(x, layer_norm=False):
-    """
-    Activation function.
-    """
-
-    norm = nn.LayerNorm if layer_norm else nn.Identity()
-    return F.elu(norm(x))
-
-
-def diag_normal(x: Tensor, min_std=0.1, max_std=2.0):
-    """
-    Diagonal normal distribution.
-    """
-
-    mean, std = x.chunk(2, -1)
-    std = max_std * torch.sigmoid(std) + min_std
-    return D.independent.Independent(D.normal.Normal(mean, std), 1)
diff --git a/src/planet.py b/src/planet.py
deleted file mode 100644
index 060e81d..0000000
--- a/src/planet.py
+++ /dev/null
@@ -1,410 +0,0 @@
-import os
-
-from typing import Any, Dict, List, Tuple, Union
-from collections import namedtuple
-
-import torch
-from torch import Tensor, optim, nn
-from torch.distributions import Normal, kl_divergence, Independent
-
-from torchtyping import TensorType, patch_typeguard
-from typeguard import typechecked
-
-from env import EnvBatcher
-from memory import ExperienceReplay
-from models import TransitionModel, ObservationModel, DenseModel, CnnImageEncoder
-from utils import device
-from planner import MPCPlanner
-from base_agent import BaseAgent
-
-patch_typeguard()
-
-DiscreteState = namedtuple('DiscreteState', [])
-
-class Planet(BaseAgent):
-    """
-    A planet-based agent.
-    """
-
-    def __init__(self, params: Dict[str, Any], env):
-        self.env = env
-
-        # Initialize base parameters from the config file.
-        self.belief_size = params["belief_size"]
-
-        self.state_size = params["state_size"]
-        self.action_size = env.action_size
-        self.hidden_size = params["hidden_size"]
-        self.embedding_size = params["embedding_size"]
-
-        self.dense_activation_function = params["dense_activation_function"]
-        self.cnn_activation_function = params["cnn_activation_function"]
-
-        self.batch_size = params["batch_size"]
-        self.seq_len = params["seq_len"]
-
-        self.seed_episodes = params["seed_episodes"]
-        self.seed_steps = params["seed_steps"]
-
-        self.experience_size = params["experience_size"]
-        self.bit_depth = params["bit_depth"]
-        self.kl_loss_weight = params['kl_loss_weight']
-        self.latent_distribution = params['latent_distribution']
-        self.discrete_latent_dimensions = params['discrete_latent_dimensions']
-        self.discrete_latent_classes = params['discrete_latent_classes']
-
-        if self.latent_distribution == 'Categorical':
-            self.state_size = self.discrete_latent_dimensions * self.discrete_latent_classes
-
-        self.jit = params['jit']
-
-        self.planning_horizon = params["planning_horizon"]
-
-        self.pixel_observation = params['pixel_observation']
-
-        """
-                Initialize MPC parameters.
-        """
-        self.optimisation_iters = params["MPC"]["optimisation_iters"]
-        self.candidates = params["MPC"]["candidates"]
-        self.top_candidates = params["MPC"]["top_candidates"]
-
-        self.model_learning_rate = params["model_learning_rate"]
-        self.adam_epsilon = params["adam_epsilon"]
-        self.weight_decay = params['weight_decay']
-        self.action_noise = params["action_noise"]
-        self.grad_clip_norm = params["grad_clip_norm"]
-        self.action_repeat = params["action_repeat"]
-
-        self.posterior_states = None
-        self.beliefs = None
-
-        """
-        Initialize the replay buffer and models.
-        """
-
-        self.buffer = ExperienceReplay(
-            self.experience_size,
-            self.env.action_size,
-            self.bit_depth,
-            self.pixel_observation,
-            self.env.observation_size,
-            device,
-        )
-
-        self.initialize_models()
-
-        # Allowed deviation in KL divergence
-        self.free_nats = torch.full((1,), params["free_nats"], device=device)
-
-        self.initialize_optimizers()
-        self.load(params)
-
-    def load(self, params: Dict[str, Any]) -> None:
-        """
-        Load models if they exist.
-        """
-        if params["models"] != "" and os.path.exists(params["models"]):
-            print("Loading models...")
-            model_dicts = torch.load(params["models"])
-            self.transition_model.load_state_dict(model_dicts["transition_model"])
-            self.observation_model.load_state_dict(model_dicts["observation_model"])
-            self.reward_model.load_state_dict(model_dicts["reward_model"])
-            self.encoder.load_state_dict(model_dicts["encoder"])
-            self.model_optimizer.load_state_dict(model_dicts["model_optimizer"])
-
-    def eval(self) -> None:
-        """
-        Set the models to evaluation mode.
-        """
-
-        self.transition_model.eval()
-        self.observation_model.eval()
-        self.reward_model.eval()
-        self.encoder.eval()
-
-    def train(self) -> None:
-        """
-        Set the models to training mode.
-        """
-
-        self.transition_model.train()
-        self.observation_model.train()
-        self.reward_model.train()
-        self.encoder.train()
-
-    def randomly_initialize_replay_buffer(self) -> List[int]:
-        """
-        Initialize the replay buffer with random transitions.
-        """
-
-        total_steps = 0
-        s = 0
-        while total_steps < self.seed_steps:
-        # for s in range(1, self.seed_episodes + 1):
-            done = False
-            t = 0
-            observation = self.env.reset()
-            # print(f'observation.shape={observation.shape}')
-            while not done:
-                action = self.env.sample_random_action()
-                next_observation, reward, done = self.env.step(action)
-                self.buffer.append(observation, action, reward, done)
-                observation = next_observation
-                t += 1
-            s += 1
-
-            total_steps += t * self.action_repeat
-        self.env.close()
-        return total_steps, s
-
-    def initialize_models(self) -> None:
-        """
-        Initialize the different models.
-        """
-        self.transition_model = TransitionModel(
-            self.belief_size,
-            self.state_size,
-            self.action_size,
-            self.hidden_size,
-            self.embedding_size,
-            self.dense_activation_function,
-            latent_distribution=self.latent_distribution,
-            discrete_latent_dimensions=self.discrete_latent_dimensions,
-            discrete_latent_classes=self.discrete_latent_classes
-        ).to(device=device)
-
-        self.reward_model = DenseModel(
-            self.belief_size + self.state_size,
-            self.hidden_size,
-            activation=self.dense_activation_function,
-        ).to(device=device)
-
-        if self.pixel_observation:
-            self.encoder = CnnImageEncoder(
-                    self.embedding_size,
-                    self.cnn_activation_function,
-                ).to(device=device)
-            self.observation_model = ObservationModel(
-                self.belief_size,
-                self.state_size,
-                self.embedding_size,
-                self.cnn_activation_function,
-            ).to(device=device)
-        else:
-            self.encoder = DenseModel(
-                self.env.observation_size,
-                self.hidden_size,
-                self.embedding_size,
-                self.dense_activation_function
-            )
-            self.observation_model = DenseModel(
-                self.belief_size + self.state_size,
-                self.hidden_size,
-                self.env.observation_size,
-                self.dense_activation_function
-            )
-
-        self.planner = MPCPlanner(
-            self.action_size,
-            self.planning_horizon,
-            self.optimisation_iters,
-            self.candidates,
-            self.top_candidates,
-            self.transition_model,
-            self.reward_model,
-        )
-
-        if self.jit:
-            self.transition_model = torch.jit.script(self.transition_model)
-            self.observation_model = torch.jit.script(self.observation_model)
-            self.reward_model = torch.jit.script(self.reward_model)
-            self.encoder = torch.jit.script(self.encoder)
-            self.planner = torch.jit.script(self.planner)
-
-    def initialize_optimizers(self) -> None:
-        """
-        Initialize the optimizers.
-        """
-
-        self.model_modules = [
-            self.transition_model,
-            self.observation_model,
-            self.reward_model,
-            self.encoder,
-        ]
-
-        self.model_params = (
-            list(self.transition_model.parameters())
-            + list(self.observation_model.parameters())
-            + list(self.reward_model.parameters())
-            + list(self.encoder.parameters())
-        )
-
-        self.model_optimizer = optim.Adam(
-            self.model_params,
-            lr=self.model_learning_rate,
-            eps=self.adam_epsilon,
-            weight_decay=self.weight_decay
-        )
-
-    @typechecked
-    def _observation_loss(
-            self,
-            beliefs: TensorType['seq_len', 'batch_size', 'belief_size'],
-            posterior_states: TensorType['seq_len', 'batch_size', 'state_size'],
-            observations: Union[TensorType['seq_len', 'batch_size', 'channels', 'height', 'width'],
-                                TensorType['seq_len', 'batch_size', 'observation_size']]
-    ) -> Tensor:
-        """
-        Compute the observation loss.
-        """
-        means = self.observation_model(beliefs, posterior_states)
-        # print(observations.shape, "TWWWWRWR")
-        if self.pixel_observation:
-            observation_dist = Independent(Normal(means, 1), 3)  # independent for matching dims
-        else:
-            observation_dist = Independent(Normal(means, 1), 1)
-        observation_loss = -observation_dist.log_prob(observations).mean()
-        return observation_loss
-
-    @typechecked
-    def _reward_loss(
-        self,
-            beliefs: TensorType['seq_len', 'batch_size', 'belief_size'],
-            posterior_states: TensorType['seq_len', 'batch_size', 'state_size'],
-            rewards: TensorType['seq_len', 'batch_size']
-    ) -> Tensor:
-        """
-        Compute the reward loss
-        """
-        means = self.reward_model(beliefs, posterior_states)
-        reward_dist = Independent(Normal(means, 1), 1)
-        reward_loss = -reward_dist.log_prob(rewards.unsqueeze(dim=-1)).mean()
-        return reward_loss
-
-    def _kl_loss(
-            self,
-            posterior_params: Tuple[Tensor, ...],
-            prior_params: Tuple[Tensor, ...],
-    ) -> Tensor:
-        """
-        Compute the KL loss.
-        """
-        # extract Gaussian params
-        posterior_means, posterior_std_devs = posterior_params
-        prior_means, prior_std_devs = prior_params
-
-        div = kl_divergence(
-            Normal(posterior_means, posterior_std_devs),
-            Normal(prior_means, prior_std_devs),
-        ).sum(dim=2)
-
-        # this is the free bits optimization presented in
-        # Improved Variational Inference with Inverse Autoregressive Flow : C.8.
-        # https://arxiv.org/abs/1606.04934
-        kl_loss = torch.max(div, self.free_nats).mean(dim=(0, 1))
-
-        return kl_loss
-
-    def train_step(self) -> Dict[str, float]:
-        """
-        Train the model for one step.
-        """
-
-        log = {}
-        ####################
-        # DYNAMICS LEARNING
-        ####################
-
-        # 1) Draw sequences chunks
-        observations, actions, rewards, nonterminals = self.buffer.sample(
-            self.batch_size, self.seq_len
-        )
-
-        # 2) Compute model States
-        # Create initial belief and state for time t = 0
-        init_belief = torch.zeros(self.batch_size, self.belief_size, device=device)
-        init_state = torch.zeros(self.batch_size, self.state_size, device=device)
-
-        # Update belief/state using posterior from previous belief/state,
-        # previous action and current observation (over entire sequence at once)
-        embeddings = self.encoder(observations[1:])
-        (
-            beliefs,
-            _,  # prior_states, just in case
-            prior_params,
-            posterior_states,
-            posterior_params,
-        ) = self.transition_model(
-            init_state,
-            actions[:-1],
-            init_belief,
-            embeddings,
-            nonterminals[:-1],
-        )
-
-        # 3) Update model weights
-
-        # Calculate observation likelihood, reward likelihood and KL losses
-        # sum over final dims, average over batch and time
-        observation_loss = self._observation_loss(beliefs, posterior_states, observations[1:])
-        reward_loss = self._reward_loss(beliefs, posterior_states, rewards[:-1])
-        kl_loss = self._kl_loss(posterior_params, prior_params)
-        model_loss = observation_loss + reward_loss + kl_loss * self.kl_loss_weight
-
-        # log stuff
-        log["observation_loss"] = observation_loss.item()
-        log["reward_loss"] = reward_loss.item()
-        log["kl_loss"] = kl_loss.item()
-        log["model_loss"] = model_loss.item()
-
-        # Update model parameters
-        self.model_optimizer.zero_grad()
-        model_loss.backward()
-        nn.utils.clip_grad_norm_(self.model_params, self.grad_clip_norm, norm_type=2)
-        self.model_optimizer.step()
-
-        return log
-
-    def update_belief_and_act(
-        self, env, belief, posterior_state, action, observation, explore=False
-    ):
-        """
-        Update belief and action given an observation.
-        """
-        embedding = self.encoder(observation).unsqueeze(dim=0)
-        # Infer belief over current state q(s_t|ot,a<t) from the history
-        # Action and observation need extra time dimension
-        belief, _, _, posterior_state, _ = self.transition_model(
-            posterior_state,
-            action.unsqueeze(dim=0),
-            belief,
-            embedding,
-        )
-        # Remove time dimension from belief/state
-        belief = belief.squeeze(dim=0)
-        posterior_state = posterior_state.squeeze(dim=0)
-
-        # Get action from planner(q(s_t|ot,a<t), p)
-        action, _ = self.get_action(belief, posterior_state)
-
-        if explore:
-            # Add gaussian exploration noise on top of the sampled action
-            # print(self.action_noise, action)
-            action = torch.clamp(Normal(action, self.action_noise).rsample(), -1, 1)
-
-        # Perform environment step (action repeats handled internally)
-        next_observation, reward, done = env.step(
-            action.cpu() if isinstance(env, EnvBatcher) else action[0].cpu()
-        )
-
-        # self.replay_buffer.append(observation, action, reward, done)
-        return belief, posterior_state, action, next_observation, reward, done
-
-    def get_action(self, belief, state):
-        """
-        Get action for the given belief and posterior state.
-        """
-
-        return self.planner(belief, state)
diff --git a/src/planner.py b/src/planner.py
deleted file mode 100644
index d85d299..0000000
--- a/src/planner.py
+++ /dev/null
@@ -1,90 +0,0 @@
-import torch
-from torch import nn
-
-
-class MPCPlanner(nn.Module):
-    """
-    Model-predictive control planner with cross-entropy method and learned transition model.
-    """
-
-    def __init__(
-        self,
-        action_size,
-        planning_horizon,
-        optimisation_iters,
-        candidates,
-        top_candidates,
-        transition_model,
-        reward_model,
-    ):
-        super().__init__()
-        self.transition_model = transition_model
-        self.reward_model = reward_model
-        self.action_size = action_size
-        self.planning_horizon = planning_horizon
-        self.optimisation_iters = optimisation_iters
-        self.candidates, self.top_candidates = candidates, top_candidates
-
-    def forward(self, belief, state):
-        """
-        Plan actions for the given belief and state.
-
-        :param belief: (B, H, Z)
-        :param state: (B, Z)
-        :return: actions (B, H, A)
-        """
-
-        B, H, Z = belief.size(0), belief.size(1), state.size(1)
-        belief = belief.unsqueeze(dim=1).expand(B, self.candidates, H).reshape(-1, H)
-        state = state.unsqueeze(dim=1).expand(B, self.candidates, Z).reshape(-1, Z)
-
-        # Initialize factorized belief over action sequences q(a_t:t+H) ~ N(0, I)
-        action_mean = torch.zeros(
-            self.planning_horizon, B, 1, self.action_size, device=belief.device
-        )
-        action_std_dev = torch.ones(
-            self.planning_horizon, B, 1, self.action_size, device=belief.device
-        )
-
-        for _ in range(self.optimisation_iters):
-            # Evaluate J action sequences from the current belief (over entire sequence at once,
-            # batched over particles)
-            # Sample actions (time x (batch x candidates) x actions)
-            noise = torch.randn(
-                self.planning_horizon,
-                B,
-                self.candidates,
-                self.action_size,
-                device=action_mean.device,
-            )
-            actions = (action_mean + action_std_dev * noise).view(
-                self.planning_horizon, B * self.candidates, self.action_size
-            )
-            # Sample next states
-            # [12, 1000, 200] [12, 1000, 30] : 12 horizon steps; 1000 candidates
-            beliefs, states, _, _, _ = self.transition_model(state, actions, belief)
-            # Calculate expected returns (technically sum of rewards over planning horizon)
-            # output from r-model[12000]->view[12, 1000]->sum[1000]
-            returns = (
-                self.reward_model(beliefs.view(-1, H), states.view(-1, Z))
-                .view(self.planning_horizon, -1)
-                .sum(dim=0)
-            )
-            # Re-fit belief to the K best action sequences
-            _, topk = returns.reshape(B, self.candidates).topk(
-                self.top_candidates, dim=1, largest=True, sorted=False
-            )
-            # Fix indices for unrolled actions
-            topk += self.candidates * torch.arange(
-                0, B, dtype=torch.int64, device=topk.device
-            ).unsqueeze(dim=1)
-            best_actions = actions[:, topk.view(-1)].reshape(
-                self.planning_horizon, B, self.top_candidates, self.action_size
-            )
-
-            # Update belief with new means and standard deviations
-            action_mean = best_actions.mean(dim=2, keepdim=True)
-            action_std_dev = best_actions.std(dim=2, unbiased=False, keepdim=True)
-
-        # Return first action mean _t
-        return action_mean[0].squeeze(dim=1)
diff --git a/src/utils.py b/src/utils.py
deleted file mode 100644
index f0ab263..0000000
--- a/src/utils.py
+++ /dev/null
@@ -1,404 +0,0 @@
-import os
-from typing import Iterable, List
-
-import cv2
-import numpy as np
-import plotly
-import torch
-from plotly.graph_objs import Scatter
-from plotly.graph_objs.scatter import Line
-from torch.nn import Module
-from torch import Tensor
-from torch import nn
-
-
-def cat(x: Tensor, y: Tensor) -> Tensor:
-    """
-    Concatenate x and y along the channel dimension
-
-    :param x:
-    :param y:
-    :return:
-    """
-    return torch.cat([x, y], dim=1)
-
-
-def chunk(x: Tensor) -> List[Tensor]:
-    """
-    chunk x in two along the channel dimension
-
-    :param x:
-    :return:
-    """
-    return torch.chunk(x, 2, dim=1)
-
-
-def stack(x: List[Tensor]) -> Tensor:
-    """
-    stack list of tensor while skiping the first element
-
-    :param x:
-    :return:
-    """
-    return torch.stack(x[1:], dim=0)
-
-
-def prefill(t: int) -> List[Tensor]:
-    """
-    stuff
-
-    :param t:
-    :return:
-    """
-    return [torch.empty(0)] * t
-
-
-def polyak(target_param: Tensor, param: Tensor, weight: float):
-    """
-    Polyak averaging for ONE parameter (soft update)
-
-    :param target_param:
-    :param param:
-    :param weight:
-    :return:
-    """
-    target_param.data.copy_(param.data * weight + target_param.data * (1.0 - weight))
-
-
-def polyak_update(target: nn.Module, base: nn.Module, weight: float):
-    """
-    Perform polyack averaging (soft update) for a nn.Module
-
-    :param target:
-    :param base:
-    :param weight:
-    :return:
-    """
-    for target_param, param in zip(target.parameters(), base.parameters()):
-        polyak(target_param, param, weight)
-
-# Plots min, max and mean + standard deviation bars of a population over time
-def lineplot(xs, ys_population, title, path="", xaxis="episode"):
-    """
-    Plots min, max and mean + standard deviation bars of a population over time.
-
-    :param xs: list of x values
-    :param ys_population: list of y values
-    :param title: title of the plot
-    :param path: path to save the plot
-    :param xaxis: x axis label
-    """
-
-    max_colour, mean_colour, std_colour, transparent = (
-        "rgb(0, 132, 180)",
-        "rgb(0, 172, 237)",
-        "rgba(29, 202, 255, 0.2)",
-        "rgba(0, 0, 0, 0)",
-    )
-
-    if isinstance(ys_population[0], (list, tuple)):
-        ys = np.asarray(ys_population, dtype=np.float32)
-        ys_min, ys_max, ys_mean, ys_std, ys_median = (
-            ys.min(1),
-            ys.max(1),
-            ys.mean(1),
-            ys.std(1),
-            np.median(ys, 1),
-        )
-        ys_upper, ys_lower = ys_mean + ys_std, ys_mean - ys_std
-
-        trace_max = Scatter(
-            x=xs, y=ys_max, line=Line(color=max_colour, dash="dash"), name="Max"
-        )
-        trace_upper = Scatter(
-            x=xs,
-            y=ys_upper,
-            line=Line(color=transparent),
-            name="+1 Std. Dev.",
-            showlegend=False,
-        )
-        trace_mean = Scatter(
-            x=xs,
-            y=ys_mean,
-            fill="tonexty",
-            fillcolor=std_colour,
-            line=Line(color=mean_colour),
-            name="Mean",
-        )
-        trace_lower = Scatter(
-            x=xs,
-            y=ys_lower,
-            fill="tonexty",
-            fillcolor=std_colour,
-            line=Line(color=transparent),
-            name="-1 Std. Dev.",
-            showlegend=False,
-        )
-        trace_min = Scatter(
-            x=xs, y=ys_min, line=Line(color=max_colour, dash="dash"), name="Min"
-        )
-        trace_median = Scatter(
-            x=xs, y=ys_median, line=Line(color=max_colour), name="Median"
-        )
-        data = [
-            trace_upper,
-            trace_mean,
-            trace_lower,
-            trace_min,
-            trace_max,
-            trace_median,
-        ]
-    else:
-        data = [Scatter(x=xs, y=ys_population, line=Line(color=mean_colour))]
-    plotly.offline.plot(
-        {
-            "data": data,
-            "layout": dict(title=title, xaxis={"title": xaxis}, yaxis={"title": title}),
-        },
-        filename=os.path.join(path, title + ".html"),
-        auto_open=False,
-    )
-
-
-def write_video(frames, title, path=""):
-    """
-    Writes a video from a list of frames.
-    """
-
-    frames = (
-        np.multiply(np.stack(frames, axis=0).transpose(0, 2, 3, 1), 255)
-        .clip(0, 255)
-        .astype(np.uint8)[:, :, :, ::-1]
-    )  # VideoWrite expects H x W x C in BGR
-    _, H, W, _ = frames.shape
-
-    writer = cv2.VideoWriter(
-        os.path.join(path, f"{title}.mp4"),
-        cv2.VideoWriter_fourcc(*"mp4v"),
-        30.0,
-        (W, H),
-        True,
-    )
-
-    for frame in frames:
-        writer.write(frame)
-    writer.release()
-
-
-class ActivateParameters:
-    """
-    Context manager to locally Activate the gradients.
-    example:
-    ```
-    with ActivateParameters([module]):
-        output_tensor = module(input_tensor)
-    ```
-    :param modules: iterable of modules. used to call .parameters() to freeze gradients.
-    """
-
-    def __init__(self, modules: Iterable[Module]):
-        self.modules = modules
-        self.param_states = [p.requires_grad for p in get_parameters(self.modules)]
-
-    def __enter__(self):
-        for param in get_parameters(self.modules):
-            param.requires_grad = True
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        for i, param in enumerate(get_parameters(self.modules)):
-            param.requires_grad = self.param_states[i]
-
-
-# "get_parameters" and "FreezeParameters" are from the following repo
-# https://github.com/juliusfrost/dreamer-pytorch
-def get_parameters(modules: Iterable[Module]):
-    """
-    Given a list of torch modules, returns a list of their parameters.
-
-    :param modules: iterable of modules
-    :returns: a list of parameters
-    """
-    model_parameters = []
-    for module in modules:
-        model_parameters += list(module.parameters())
-    return model_parameters
-
-
-class FreezeParameters:
-    """
-    Context manager to locally freeze gradients.
-    In some cases with can speed up computation because gradients aren't calculated
-    for these listed modules.
-    example:
-    ```
-    with FreezeParameters([module]):
-        output_tensor = module(input_tensor)
-    ```
-    :param modules: iterable of modules. used to call .parameters() to freeze gradients.
-    """
-
-    def __init__(self, modules: Iterable[Module]):
-        self.modules = modules
-        self.param_states = [p.requires_grad for p in get_parameters(self.modules)]
-
-    def __enter__(self):
-        for param in get_parameters(self.modules):
-            param.requires_grad = False
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        for i, param in enumerate(get_parameters(self.modules)):
-            param.requires_grad = self.param_states[i]
-
-
-## from the homeworks
-device = None
-
-
-def init_gpu(use_gpu=True, gpu_id=0):
-    """
-    Initialize the GPU.
-
-    :param use_gpu: if True, use GPU.
-    :param gpu_id: id of the GPU to use.
-    :return: None
-    """
-
-    global device  # pylint: disable=global-statement
-
-    if torch.cuda.is_available() and use_gpu:
-        device = torch.device("cuda:" + str(gpu_id))
-        print(f"Using GPU id {gpu_id}")
-    else:
-        device = torch.device("cpu")
-        print("GPU not detected. Defaulting to CPU.")
-
-
-def set_device(gpu_id):
-    """
-    Set the device to use.
-    """
-    torch.cuda.set_device(gpu_id)
-
-
-def from_numpy(*args, **kwargs):
-    """
-    Convert a numpy array to a torch tensor.
-    """
-
-    return torch.from_numpy(*args, **kwargs).float().to(device)
-
-
-def to_numpy(tensor):
-    """
-    Convert a torch tensor to a numpy array.
-    """
-
-    return tensor.to("cpu").detach().numpy()
-
-
-def preprocess_observation_(observation, bit_depth) -> None:
-    """
-    Preprocesses an observation inplace.
-    (from float32 Tensor [0, 255] to [-0.5, 0.5])
-
-    Args:
-        observation: the observation to preprocess.
-        bit_depth: the bit depth of the observation
-
-    Returns:
-        None
-
-    """
-
-    # Quantise to given bit depth and centre
-    observation.div_(2 ** (8 - bit_depth)).floor_().div_(2**bit_depth).sub_(0.5)
-    # Dequantise:
-    # (to approx. match likelihood of PDF of continuous images vs. PMF of discrete images)
-    observation.add_(torch.rand_like(observation).div_(2**bit_depth))
-
-
-def postprocess_observation(observation, bit_depth) -> np.ndarray:
-    """
-    Postprocess an observation for storage.
-    (from float32 numpy array [-0.5, 0.5] to uint8 numpy array [0, 255])
-
-    Args:
-        observation: observation to process
-        bit_depth: bit depth to quantise to
-
-    Returns:
-        np.ndarray: postprocessed observation
-    """
-
-    return np.clip(
-        np.floor((observation + 0.5) * 2**bit_depth) * 2 ** (8 - bit_depth),
-        0,
-        2**8 - 1,
-    ).astype(np.uint8)
-
-
-def images_to_observation(images, bit_depth, observation_shape) -> np.ndarray:
-    """
-    Converts a list of images to a single observation.
-
-    Args:
-        images: list of images
-        bit_depth: bit depth of the images
-        observation_shape: shape of the observation
-
-    Returns:
-        observation: observation
-    """
-
-    # Resize and put channel first
-    images = torch.tensor(
-        cv2.resize(images, observation_shape, interpolation=cv2.INTER_LINEAR).transpose(
-            2, 0, 1
-        ),
-        dtype=torch.float32,
-    )
-
-    # Quantise, centre and dequantise inplace
-    preprocess_observation_(images, bit_depth)
-
-    # Add batch dimension
-    return images.unsqueeze(dim=0)
-
-
-def build_mlp(
-        input_size: int,
-        hidden_size: int,
-        output_size: int,
-        n_layers: int,
-        activation: str = 'ELU',
-        output_activation: str = 'Identity',
-) -> nn.Sequential:
-    """
-        Builds a feedforward neural network
-        arguments:
-            input_placeholder: placeholder variable for the state (batch_size, input_size)
-            scope: variable scope of the network
-            n_layers: number of hidden layers
-            size: dimension of each hidden layer
-            activation: activation of each hidden layer
-            input_size: size of the input layer
-            output_size: size of the output layer
-            output_activation: activation of the output layer
-        returns:
-            output_placeholder: the result of a forward pass through the hidden layers
-                                + the output layer
-    """
-    if isinstance(activation, str):
-        activation = getattr(nn, activation)
-    if isinstance(output_activation, str):
-        output_activation = getattr(nn, output_activation)
-
-    layers = []
-    in_size = input_size
-    for _ in range(n_layers):
-        layers.append(nn.Linear(in_size, hidden_size))
-        layers.append(activation())
-        in_size = hidden_size
-    layers.append(nn.Linear(in_size, output_size))
-    layers.append(output_activation())
-    return nn.Sequential(*layers)
diff --git a/test/__init__.py b/test/__init__.py
deleted file mode 100644
index e69de29..0000000
diff --git a/test/test_utils.py b/test/test_utils.py
deleted file mode 100644
index 6b89d68..0000000
--- a/test/test_utils.py
+++ /dev/null
@@ -1,24 +0,0 @@
-import unittest
-from unittest import TestCase
-
-
-class TestFreezeParameters(TestCase):
-    def test_freeze_parameters(self):
-        from src.utils import FreezeParameters
-        import torchvision.models as models
-
-        m = models.resnet18()
-
-        for p in m.parameters():
-            self.assertTrue(p.requires_grad)
-
-        with FreezeParameters([m]):
-            for p in m.parameters():
-                self.assertFalse(p.requires_grad)
-
-        for p in m.parameters():
-            self.assertTrue(p.requires_grad)
-
-
-if __name__ == "__main__":
-    unittest.main()
diff --git a/utils.py b/utils.py
new file mode 100644
index 0000000..b4280c9
--- /dev/null
+++ b/utils.py
@@ -0,0 +1,246 @@
+import inspect
+import os
+from functools import wraps
+from typing import Iterable
+
+import cv2
+import numpy as np
+import plotly
+import torch
+from plotly.graph_objs import Scatter
+from plotly.graph_objs.scatter import Line
+from torch.nn import Module
+
+
+# Plots min, max and mean + standard deviation bars of a population over time
+def lineplot(xs, ys_population, title, path='', xaxis='episode'):
+    max_colour, mean_colour, std_colour, transparent = 'rgb(0, 132, 180)', 'rgb(0, 172, 237)', 'rgba(29, 202, 255, 0.2)', 'rgba(0, 0, 0, 0)'
+
+    if isinstance(ys_population[0], list) or isinstance(ys_population[0], tuple):
+        ys = np.asarray(ys_population, dtype=np.float32)
+        ys_min, ys_max, ys_mean, ys_std, ys_median = ys.min(1), ys.max(1), ys.mean(1), ys.std(1), np.median(ys, 1)
+        ys_upper, ys_lower = ys_mean + ys_std, ys_mean - ys_std
+
+        trace_max = Scatter(x=xs, y=ys_max, line=Line(color=max_colour, dash='dash'), name='Max')
+        trace_upper = Scatter(x=xs, y=ys_upper, line=Line(color=transparent), name='+1 Std. Dev.', showlegend=False)
+        trace_mean = Scatter(x=xs, y=ys_mean, fill='tonexty', fillcolor=std_colour, line=Line(color=mean_colour),
+                             name='Mean')
+        trace_lower = Scatter(x=xs, y=ys_lower, fill='tonexty', fillcolor=std_colour, line=Line(color=transparent),
+                              name='-1 Std. Dev.', showlegend=False)
+        trace_min = Scatter(x=xs, y=ys_min, line=Line(color=max_colour, dash='dash'), name='Min')
+        trace_median = Scatter(x=xs, y=ys_median, line=Line(color=max_colour), name='Median')
+        data = [trace_upper, trace_mean, trace_lower, trace_min, trace_max, trace_median]
+    else:
+        data = [Scatter(x=xs, y=ys_population, line=Line(color=mean_colour))]
+    plotly.offline.plot({
+        'data': data,
+        'layout': dict(title=title, xaxis={'title': xaxis}, yaxis={'title': title})
+    }, filename=os.path.join(path, title + '.html'), auto_open=False)
+
+
+def write_video(frames, title, path=''):
+    frames = np.multiply(np.stack(frames, axis=0).transpose(0, 2, 3, 1), 255).clip(0, 255).astype(np.uint8)[:, :, :,
+             ::-1]  # VideoWrite expects H x W x C in BGR
+    _, H, W, _ = frames.shape
+    writer = cv2.VideoWriter(os.path.join(path, '%s.mp4' % title), cv2.VideoWriter_fourcc(*'mp4v'), 30., (W, H), True)
+    for frame in frames:
+        writer.write(frame)
+    writer.release()
+
+
+class ActivateParameters:
+    def __init__(self, modules: Iterable[Module]):
+        """
+        Context manager to locally Activate the gradients.
+        example:
+        ```
+        with ActivateParameters([module]):
+            output_tensor = module(input_tensor)
+        ```
+        :param modules: iterable of modules. used to call .parameters() to freeze gradients.
+        """
+        self.modules = modules
+        self.param_states = [p.requires_grad for p in get_parameters(self.modules)]
+
+    def __enter__(self):
+        for param in get_parameters(self.modules):
+            # print(param.requires_grad)
+            param.requires_grad = True
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        for i, param in enumerate(get_parameters(self.modules)):
+            param.requires_grad = self.param_states[i]
+
+
+# "get_parameters" and "FreezeParameters" are from the following repo
+# https://github.com/juliusfrost/dreamer-pytorch
+def get_parameters(modules: Iterable[Module]):
+    """
+    Given a list of torch modules, returns a list of their parameters.
+    :param modules: iterable of modules
+    :returns: a list of parameters
+    """
+    model_parameters = []
+    for module in modules:
+        model_parameters += list(module.parameters())
+    return model_parameters
+
+
+class FreezeParameters:
+    def __init__(self, modules: Iterable[Module]):
+        """
+        Context manager to locally freeze gradients.
+        In some cases with can speed up computation because gradients aren't calculated for these listed modules.
+        example:
+        ```
+        with FreezeParameters([module]):
+            output_tensor = module(input_tensor)
+        ```
+        :param modules: iterable of modules. used to call .parameters() to freeze gradients.
+        """
+        self.modules = modules
+        self.param_states = [p.requires_grad for p in get_parameters(self.modules)]
+
+    def __enter__(self):
+        for param in get_parameters(self.modules):
+            param.requires_grad = False
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        for i, param in enumerate(get_parameters(self.modules)):
+            param.requires_grad = self.param_states[i]
+
+
+## from the homeworks
+
+def member_initialize(wrapped__init__):
+    """Decorator to initialize members of a class with the named arguments. (i.e. so D.R.Y. principle is maintained
+    for class initialization).
+
+    Modified from http://stackoverflow.com/questions/1389180/python-automatically-initialize-instance-variables
+    :param wrapped__init__:
+    :returns:
+    :rtype:
+
+    """
+
+    names, varargs, keywords, defaults = inspect.getargspec(wrapped__init__)
+
+    @wraps(wrapped__init__)
+    def wrapper(self, *args, **kargs):
+        for name, arg in list(zip(names[1:], args)) + list(kargs.items()):
+            setattr(self, name, arg)
+
+        if defaults is not None:
+            for i in range(len(defaults)):
+                index = -(i + 1)
+                if not hasattr(self, names[index]):
+                    setattr(self, names[index], defaults[index])
+
+        wrapped__init__(self, *args, **kargs)
+
+    return wrapper
+
+
+def hidden_member_initialize(wrapped__init__):
+    """Decorator to initialize members of a class with the named arguments. (i.e. so D.R.Y. principle is maintained
+    for class initialization).
+
+    Modified from http://stackoverflow.com/questions/1389180/python-automatically-initialize-instance-variables
+    :param wrapped__init__:
+    :returns:
+    :rtype:
+
+    """
+
+    names, varargs, keywords, defaults = inspect.getargspec(wrapped__init__)
+
+    @wraps(wrapped__init__)
+    def wrapper(self, *args, **kargs):
+        for name, arg in list(zip(names[1:], args)) + list(kargs.items()):
+            setattr(self, '_' + name, arg)
+
+        if defaults is not None:
+            for i in range(len(defaults)):
+                index = -(i + 1)
+                if not hasattr(self, '_' + names[index]):
+                    setattr(self, '_' + names[index], defaults[index])
+
+        wrapped__init__(self, *args, **kargs)
+
+    return wrapper
+
+
+def tensor_member_initialize(wrapped__init__):
+    """Decorator to initialize members of a class with the named arguments. (i.e. so D.R.Y. principle is maintained
+    for class initialization).
+
+    Modified from http://stackoverflow.com/questions/1389180/python-automatically-initialize-instance-variables
+    :param wrapped__init__:
+    :returns:
+    :rtype:
+
+    """
+    import tensorflow as tf
+
+    names, varargs, keywords, defaults = inspect.getargspec(wrapped__init__)
+
+    @wraps(wrapped__init__)
+    def wrapper(self, *args, **kargs):
+        for name, arg in list(zip(names[1:], args)) + list(kargs.items()):
+            setattr(self, name, tf.compat.v1.convert_to_tensor(arg))
+
+        if defaults is not None:
+            for i in range(len(defaults)):
+                index = -(i + 1)
+                if not hasattr(self, names[index]):
+                    setattr(self, names[index], tf.compat.v1.convert_to_tensor(defaults[index]))
+        wrapped__init__(self, *args, **kargs)
+
+    return wrapper
+
+
+class classproperty(object):
+    def __init__(self, f):
+        """Decorator to enable access to properties of both classes and instances of classes
+
+        :param f:
+        :returns:
+        :rtype:
+
+        """
+
+        self.f = f
+
+    def __get__(self, obj, owner):
+        return self.f(owner)
+
+
+device = None
+
+
+def init_gpu(use_gpu=True, gpu_id=0):
+    global device
+    if torch.cuda.is_available() and use_gpu:
+        device = torch.device("cuda:" + str(gpu_id))
+        print("Using GPU id {}".format(gpu_id))
+    else:
+        device = torch.device("cpu")
+        print("GPU not detected. Defaulting to CPU.")
+
+
+def set_device(gpu_id):
+    torch.cuda.set_device(gpu_id)
+
+
+def from_numpy(*args, **kwargs):
+    return torch.from_numpy(*args, **kwargs).float().to(device)
+
+
+def to_numpy(tensor):
+    return tensor.to('cpu').detach().numpy()
+
+
+class Flatten(torch.nn.Module):
+    def forward(self, x):
+        batch_size = x.shape[0]
+        return x.view(batch_size, -1)
\ No newline at end of file

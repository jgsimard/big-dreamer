algorithm: 'planet'
exp_name: 'default'
seed: 0
disable_cuda: false
env: 'Pendulum-v0'
max_episode_length: 1000
experience_size: 1000000   # Original implementation has an unlimited buffer size, but 1 million is the max experience collected anyway
cnn_activation_function: 'ReLU'
dense_activation_function: 'ELU'
embedding_size: 1024  # Note that the default encoder for visual observations outputs a 1024D vector; for other embedding sizes an additional fully-connected layer is used
hidden_size: 200
belief_size: 200
state_size: 30
action_repeat: 2
action_noise: 0.3
episodes: 1000
seed_episodes: 5
collect_interval: 100
batch_size: 50
# chunk_size: 50
chunk_size: 18
worldmodel_LogProbLoss: False
global_kl_beta: 0.0  # Global KL weight (0 to disable)')
free_nats: 3.0
bit_depth: 5
model_learning_rate: 1e-3
actor_learning_rate: 8e-5
value_learning_rate: 8e-5
learning_rate_schedule: 0
adam_epsilon: 1e-7
# Note that original has a linear learning rate decay, but it seems unlikely that this makes a significant difference
grad_clip_norm: 100.0
planning_horizon: 15
discount: 0.99
disclam: 0.95
optimisation_iters: 10
candidates: 1000
top_candidates: 100
test: False
test_interval: 25
test_episodes: 10
# checkpoint_interval: 50
checkpoint_interval: 5
checkpoint_experience: False
models: ''
experience_replay: ''
render: False
log_freq : 1
log_video_freq : -1
wandb_project : null # 'test-project'
fps: 10
